Current device: cuda
Batch 0/3907, reward: 12.430, loss: -710.7338, took: 0.7810s
Batch 100/3907, reward: 6.563, loss: -8.7809, took: 47.8670s
Batch 200/3907, reward: 4.073, loss: -0.0122, took: 48.5779s
Batch 300/3907, reward: 4.004, loss: -0.5875, took: 49.1861s
Batch 400/3907, reward: 3.899, loss: -1.1968, took: 47.2883s
Batch 500/3907, reward: 3.792, loss: 0.1739, took: 48.7867s
Batch 600/3907, reward: 3.716, loss: 0.0483, took: 49.2874s
Batch 700/3907, reward: 3.678, loss: -0.3463, took: 48.3537s
Batch 800/3907, reward: 3.657, loss: -0.4789, took: 47.7666s
Batch 900/3907, reward: 3.637, loss: 0.7428, took: 48.2012s
Batch 1000/3907, reward: 3.628, loss: 0.4463, took: 48.5856s
Batch 1100/3907, reward: 3.619, loss: 0.2866, took: 48.2249s
Batch 1200/3907, reward: 3.617, loss: -0.6639, took: 46.3639s
Batch 1300/3907, reward: 3.611, loss: -0.3941, took: 48.5419s
Batch 1400/3907, reward: 3.609, loss: 0.1757, took: 48.3652s
Batch 1500/3907, reward: 3.610, loss: 0.5389, took: 48.4868s
Batch 1600/3907, reward: 3.608, loss: 0.8303, took: 48.3196s
Batch 1700/3907, reward: 3.603, loss: 0.7946, took: 48.0379s
Batch 1800/3907, reward: 3.601, loss: 0.4400, took: 48.4616s
Batch 1900/3907, reward: 3.594, loss: -0.2320, took: 48.7685s
Batch 2000/3907, reward: 3.595, loss: 0.2821, took: 48.4723s
Batch 2100/3907, reward: 3.590, loss: 0.4897, took: 48.5215s
Batch 2200/3907, reward: 3.590, loss: 0.1693, took: 45.6426s
Batch 2300/3907, reward: 3.588, loss: -0.0274, took: 48.4852s
Batch 2400/3907, reward: 3.587, loss: 0.2779, took: 47.9467s
Batch 2500/3907, reward: 3.596, loss: -0.1362, took: 48.4380s
Batch 2600/3907, reward: 3.586, loss: 0.0508, took: 48.8771s
Batch 2700/3907, reward: 3.589, loss: 0.0844, took: 47.4532s
Batch 2800/3907, reward: 3.586, loss: 0.1919, took: 47.3275s
Batch 2900/3907, reward: 3.587, loss: 0.2624, took: 47.8854s
Batch 3000/3907, reward: 3.588, loss: 0.1836, took: 48.4748s
Batch 3100/3907, reward: 3.588, loss: 0.2324, took: 47.9660s
Batch 3200/3907, reward: 3.590, loss: 0.4007, took: 48.7555s
Batch 3300/3907, reward: 3.580, loss: -0.1810, took: 46.2901s
Batch 3400/3907, reward: 3.581, loss: -0.0421, took: 48.8873s
Batch 3500/3907, reward: 3.582, loss: 0.0465, took: 47.7990s
Batch 3600/3907, reward: 3.581, loss: 0.1021, took: 48.0502s
Batch 3700/3907, reward: 3.583, loss: 0.1690, took: 45.0205s
Batch 3800/3907, reward: 3.575, loss: -0.2776, took: 47.3564s
Batch 3900/3907, reward: 3.570, loss: -0.0171, took: 49.1536s
Mean epoch loss/reward: -0.3344, 3.7156, 3.5736, took: 1888.6750s (46.8764s / 100 batches)
Batch 0/3907, reward: 3.568, loss: 1.6697, took: 0.6259s
Batch 100/3907, reward: 3.564, loss: 0.0591, took: 48.6714s
Batch 200/3907, reward: 3.565, loss: 0.3106, took: 48.4002s
Batch 300/3907, reward: 3.560, loss: 0.3777, took: 48.0960s
Batch 400/3907, reward: 3.560, loss: -0.0482, took: 47.2817s
Batch 500/3907, reward: 3.557, loss: 0.1133, took: 48.0197s
Batch 600/3907, reward: 3.558, loss: 0.2462, took: 47.8521s
Batch 700/3907, reward: 3.555, loss: 0.3500, took: 47.2916s
Batch 800/3907, reward: 3.553, loss: -0.0757, took: 47.5744s
Batch 900/3907, reward: 3.553, loss: -0.0162, took: 48.3633s
Batch 1000/3907, reward: 3.548, loss: 0.1024, took: 47.7691s
Batch 1100/3907, reward: 3.552, loss: 0.2386, took: 49.0855s
Batch 1200/3907, reward: 3.549, loss: 0.0912, took: 45.6991s
Batch 1300/3907, reward: 3.553, loss: -0.0736, took: 47.7843s
Batch 1400/3907, reward: 3.550, loss: 0.0058, took: 48.3035s
Batch 1500/3907, reward: 3.549, loss: -0.0054, took: 47.5653s
Batch 1600/3907, reward: 3.549, loss: 0.1946, took: 48.2226s
Batch 1700/3907, reward: 3.544, loss: 0.0290, took: 48.6336s
Batch 1800/3907, reward: 3.546, loss: -0.0171, took: 47.6120s
Batch 1900/3907, reward: 3.546, loss: 0.0132, took: 48.9410s
Batch 2000/3907, reward: 3.547, loss: 0.0320, took: 48.8179s
Batch 2100/3907, reward: 3.547, loss: -0.1095, took: 47.9566s
Batch 2200/3907, reward: 3.544, loss: 0.0148, took: 46.7001s
Batch 2300/3907, reward: 3.545, loss: 0.1930, took: 47.9035s
Batch 2400/3907, reward: 3.545, loss: -0.0236, took: 48.7759s
Batch 2500/3907, reward: 3.544, loss: 0.0122, took: 48.4499s
Batch 2600/3907, reward: 3.541, loss: 0.0487, took: 46.7295s
Batch 2700/3907, reward: 3.544, loss: 0.1338, took: 48.0709s
Batch 2800/3907, reward: 3.540, loss: 0.0560, took: 48.8042s
Batch 2900/3907, reward: 3.542, loss: 0.0434, took: 48.1520s
Batch 3000/3907, reward: 3.543, loss: -0.0082, took: 49.3764s
Batch 3100/3907, reward: 3.541, loss: 0.0501, took: 48.8850s
Batch 3200/3907, reward: 3.540, loss: 0.0846, took: 47.3663s
Batch 3300/3907, reward: 3.541, loss: 0.0088, took: 46.0965s
Batch 3400/3907, reward: 3.539, loss: -0.0231, took: 48.6551s
Batch 3500/3907, reward: 3.540, loss: -0.0204, took: 48.3556s
Batch 3600/3907, reward: 3.536, loss: -0.0905, took: 47.3845s
Batch 3700/3907, reward: 3.536, loss: 0.0895, took: 45.8809s
Batch 3800/3907, reward: 3.535, loss: 0.0070, took: 49.4676s
Batch 3900/3907, reward: 3.533, loss: -0.0692, took: 48.5553s
Mean epoch loss/reward: 0.0597, 3.5470, 3.5523, took: 1885.6411s (46.8044s / 100 batches)
Batch 0/3907, reward: 3.538, loss: -0.3167, took: 0.6730s
Batch 100/3907, reward: 3.532, loss: 0.0301, took: 47.6025s
Batch 200/3907, reward: 3.528, loss: -0.0018, took: 48.0065s
Batch 300/3907, reward: 3.530, loss: -0.0539, took: 49.5355s
Batch 400/3907, reward: 3.529, loss: -0.0488, took: 48.3317s
Batch 500/3907, reward: 3.525, loss: 0.0654, took: 48.3620s
Batch 600/3907, reward: 3.517, loss: -0.1724, took: 48.1382s
Batch 700/3907, reward: 3.510, loss: -0.0650, took: 47.9533s
Batch 800/3907, reward: 3.496, loss: -0.0703, took: 49.1057s
Batch 900/3907, reward: 3.479, loss: -0.0188, took: 48.6819s
Batch 1000/3907, reward: 3.457, loss: 0.0433, took: 48.8049s
Batch 1100/3907, reward: 3.429, loss: -0.0513, took: 48.1652s
Batch 1200/3907, reward: 3.397, loss: 0.0064, took: 45.2097s
Batch 1300/3907, reward: 3.377, loss: 0.0512, took: 48.1584s
Batch 1400/3907, reward: 3.365, loss: -0.0476, took: 48.2520s
Batch 1500/3907, reward: 3.362, loss: -0.0876, took: 48.1613s
Batch 1600/3907, reward: 3.339, loss: 0.0469, took: 49.2429s
Batch 1700/3907, reward: 3.315, loss: -0.0172, took: 48.0896s
Batch 1800/3907, reward: 3.261, loss: -0.0416, took: 48.0444s
Batch 1900/3907, reward: 3.243, loss: -0.0191, took: 48.1675s
Batch 2000/3907, reward: 3.234, loss: -0.0972, took: 48.7853s
Batch 2100/3907, reward: 3.220, loss: -0.0549, took: 48.2571s
Batch 2200/3907, reward: 3.214, loss: -0.0586, took: 48.4890s
Batch 2300/3907, reward: 3.209, loss: 0.0141, took: 45.0757s
Batch 2400/3907, reward: 3.205, loss: -0.0191, took: 47.1189s
Batch 2500/3907, reward: 3.195, loss: -0.0145, took: 48.3622s
Batch 2600/3907, reward: 3.190, loss: -0.0216, took: 48.6626s
Batch 2700/3907, reward: 3.188, loss: -0.0090, took: 47.5427s
Batch 2800/3907, reward: 3.180, loss: -0.0258, took: 48.0190s
Batch 2900/3907, reward: 3.180, loss: -0.0514, took: 48.0592s
Batch 3000/3907, reward: 3.175, loss: -0.0587, took: 48.7943s
Batch 3100/3907, reward: 3.175, loss: -0.0216, took: 48.3294s
Batch 3200/3907, reward: 3.170, loss: 0.0093, took: 48.2580s
Batch 3300/3907, reward: 3.167, loss: -0.0560, took: 46.2279s
Batch 3400/3907, reward: 3.167, loss: -0.0931, took: 49.1922s
Batch 3500/3907, reward: 3.164, loss: -0.0368, took: 47.6414s
Batch 3600/3907, reward: 3.156, loss: -0.0198, took: 47.5456s
Batch 3700/3907, reward: 3.160, loss: -0.0507, took: 45.7544s
Batch 3800/3907, reward: 3.150, loss: -0.0370, took: 47.4143s
Batch 3900/3907, reward: 3.154, loss: -0.0402, took: 49.0531s
Mean epoch loss/reward: -0.0307, 3.3035, 3.1315, took: 1887.1747s (46.8317s / 100 batches)
Batch 0/3907, reward: 3.159, loss: -0.3366, took: 0.5824s
Batch 100/3907, reward: 3.150, loss: -0.0555, took: 47.7867s
Batch 200/3907, reward: 3.145, loss: -0.0407, took: 49.6195s
Batch 300/3907, reward: 3.143, loss: -0.0480, took: 47.4547s
Batch 400/3907, reward: 3.142, loss: -0.0725, took: 48.8985s
Batch 500/3907, reward: 3.145, loss: -0.0509, took: 48.9111s
Batch 600/3907, reward: 3.140, loss: -0.0083, took: 48.6859s
Batch 700/3907, reward: 3.141, loss: -0.0940, took: 48.2018s
Batch 800/3907, reward: 3.134, loss: -0.0362, took: 48.6628s
Batch 900/3907, reward: 3.131, loss: -0.0401, took: 48.2588s
Batch 1000/3907, reward: 3.130, loss: -0.0526, took: 48.0695s
Batch 1100/3907, reward: 3.132, loss: -0.0367, took: 47.6978s
Batch 1200/3907, reward: 3.127, loss: -0.0353, took: 45.4411s
Batch 1300/3907, reward: 3.126, loss: -0.0384, took: 48.1568s
Batch 1400/3907, reward: 3.128, loss: -0.0428, took: 47.3845s
Batch 1500/3907, reward: 3.123, loss: -0.0472, took: 48.2480s
Batch 1600/3907, reward: 3.121, loss: -0.0592, took: 48.7171s
Batch 1700/3907, reward: 3.119, loss: -0.0542, took: 48.3714s
Batch 1800/3907, reward: 3.123, loss: -0.0391, took: 47.5881s
Batch 1900/3907, reward: 3.120, loss: -0.0327, took: 48.7198s
Batch 2000/3907, reward: 3.117, loss: -0.0565, took: 48.3478s
Batch 2100/3907, reward: 3.120, loss: 0.0060, took: 48.5673s
Batch 2200/3907, reward: 3.117, loss: -0.0458, took: 48.4037s
Batch 2300/3907, reward: 3.116, loss: -0.0450, took: 44.6228s
Batch 2400/3907, reward: 3.114, loss: -0.0367, took: 47.9531s
Batch 2500/3907, reward: 3.112, loss: -0.0511, took: 47.8204s
Batch 2600/3907, reward: 3.113, loss: -0.0507, took: 48.0077s
Batch 2700/3907, reward: 3.111, loss: -0.0457, took: 47.0592s
Batch 2800/3907, reward: 3.109, loss: -0.0219, took: 47.1906s
Batch 2900/3907, reward: 3.110, loss: -0.0327, took: 48.9661s
Batch 3000/3907, reward: 3.110, loss: -0.0338, took: 48.7479s
Batch 3100/3907, reward: 3.106, loss: -0.0562, took: 49.5075s
Batch 3200/3907, reward: 3.107, loss: -0.0388, took: 48.4929s
Batch 3300/3907, reward: 3.106, loss: -0.0178, took: 45.4602s
Batch 3400/3907, reward: 3.104, loss: -0.0545, took: 48.4400s
Batch 3500/3907, reward: 3.102, loss: -0.0302, took: 48.5849s
Batch 3600/3907, reward: 3.105, loss: -0.0178, took: 47.5829s
Batch 3700/3907, reward: 3.107, loss: -0.0282, took: 44.5389s
Batch 3800/3907, reward: 3.103, loss: -0.0401, took: 48.0061s
Batch 3900/3907, reward: 3.102, loss: -0.0224, took: 47.7490s
Mean epoch loss/reward: -0.0413, 3.1207, 3.0874, took: 1883.4047s (46.7376s / 100 batches)
Batch 0/3907, reward: 3.104, loss: 0.0197, took: 0.7533s
Batch 100/3907, reward: 3.102, loss: -0.0345, took: 49.7894s
Batch 200/3907, reward: 3.103, loss: -0.0356, took: 47.6267s
Batch 300/3907, reward: 3.104, loss: -0.0077, took: 48.3784s
Batch 400/3907, reward: 3.101, loss: -0.0341, took: 47.8114s
Batch 500/3907, reward: 3.098, loss: -0.0406, took: 48.6738s
Batch 600/3907, reward: 3.098, loss: -0.0343, took: 47.9100s
Batch 700/3907, reward: 3.098, loss: -0.0484, took: 48.2102s
Batch 800/3907, reward: 3.097, loss: -0.0241, took: 48.9786s
Batch 900/3907, reward: 3.096, loss: -0.0350, took: 48.6564s
Batch 1000/3907, reward: 3.091, loss: -0.0353, took: 47.0614s
Batch 1100/3907, reward: 3.098, loss: -0.0486, took: 48.5634s
Batch 1200/3907, reward: 3.091, loss: -0.0342, took: 46.4832s
Batch 1300/3907, reward: 3.091, loss: -0.0536, took: 49.8865s
Batch 1400/3907, reward: 3.093, loss: -0.0250, took: 48.4797s
Batch 1500/3907, reward: 3.088, loss: -0.0323, took: 48.1030s
Batch 1600/3907, reward: 3.093, loss: -0.0330, took: 47.6782s
Batch 1700/3907, reward: 3.090, loss: -0.0332, took: 48.2181s
Batch 1800/3907, reward: 3.090, loss: -0.0384, took: 48.3370s
Batch 1900/3907, reward: 3.088, loss: -0.0359, took: 48.8593s
Batch 2000/3907, reward: 3.090, loss: -0.0243, took: 49.4600s
Batch 2100/3907, reward: 3.089, loss: -0.0376, took: 47.8664s
Batch 2200/3907, reward: 3.087, loss: -0.0400, took: 47.8909s
Batch 2300/3907, reward: 3.086, loss: -0.0342, took: 45.6846s
Batch 2400/3907, reward: 3.085, loss: -0.0403, took: 47.9417s
Batch 2500/3907, reward: 3.082, loss: -0.0407, took: 48.5618s
Batch 2600/3907, reward: 3.080, loss: -0.0370, took: 47.4029s
Batch 2700/3907, reward: 3.081, loss: -0.0315, took: 47.7950s
Batch 2800/3907, reward: 3.081, loss: -0.0432, took: 47.3298s
Batch 2900/3907, reward: 3.081, loss: -0.0378, took: 48.2270s
Batch 3000/3907, reward: 3.078, loss: -0.0417, took: 48.1893s
Batch 3100/3907, reward: 3.078, loss: -0.0243, took: 49.2699s
Batch 3200/3907, reward: 3.080, loss: -0.0469, took: 48.7280s
Batch 3300/3907, reward: 3.077, loss: -0.0520, took: 44.8726s
Batch 3400/3907, reward: 3.076, loss: -0.0316, took: 49.1241s
Batch 3500/3907, reward: 3.076, loss: -0.0356, took: 47.0879s
Batch 3600/3907, reward: 3.075, loss: -0.0456, took: 47.7098s
Batch 3700/3907, reward: 3.076, loss: -0.0442, took: 44.4531s
Batch 3800/3907, reward: 3.075, loss: -0.0329, took: 49.3480s
Batch 3900/3907, reward: 3.102, loss: -0.0339, took: 48.2215s
Mean epoch loss/reward: -0.0356, 3.0884, 3.1340, took: 1887.4934s (46.8406s / 100 batches)
Batch 0/3907, reward: 3.140, loss: 0.6528, took: 0.6361s
Batch 100/3907, reward: 3.266, loss: -0.0527, took: 47.5360s
Batch 200/3907, reward: 3.254, loss: -0.1374, took: 47.8485s
Batch 300/3907, reward: 3.239, loss: -0.0973, took: 48.3394s
Batch 400/3907, reward: 3.207, loss: -0.0926, took: 47.4908s
Batch 500/3907, reward: 3.199, loss: -0.0919, took: 48.2379s
Batch 600/3907, reward: 3.193, loss: -0.1115, took: 48.1731s
Batch 700/3907, reward: 3.190, loss: -0.0822, took: 48.3750s
Batch 800/3907, reward: 3.187, loss: -0.1016, took: 47.5838s
Batch 900/3907, reward: 3.182, loss: -0.0704, took: 48.2038s
Batch 1000/3907, reward: 3.182, loss: -0.0559, took: 48.6951s
Batch 1100/3907, reward: 3.180, loss: -0.0793, took: 47.9890s
Batch 1200/3907, reward: 3.178, loss: -0.0949, took: 47.0646s
Batch 1300/3907, reward: 3.175, loss: -0.0839, took: 47.6118s
Batch 1400/3907, reward: 3.175, loss: -0.0856, took: 47.5624s
Batch 1500/3907, reward: 3.170, loss: -0.0808, took: 48.0068s
Batch 1600/3907, reward: 3.172, loss: -0.0771, took: 47.3334s
Batch 1700/3907, reward: 3.170, loss: -0.0700, took: 47.1690s
Batch 1800/3907, reward: 3.164, loss: -0.0924, took: 48.2493s
Batch 1900/3907, reward: 3.165, loss: -0.0766, took: 47.3996s
Batch 2000/3907, reward: 3.165, loss: -0.0827, took: 48.2596s
Batch 2100/3907, reward: 3.162, loss: -0.0778, took: 46.9701s
Batch 2200/3907, reward: 3.159, loss: -0.0694, took: 48.1383s
Batch 2300/3907, reward: 3.158, loss: -0.0764, took: 45.1466s
Batch 2400/3907, reward: 3.156, loss: -0.0729, took: 48.5238s
Batch 2500/3907, reward: 3.153, loss: -0.0796, took: 46.6935s
Batch 2600/3907, reward: 3.153, loss: -0.0719, took: 48.2576s
Batch 2700/3907, reward: 3.151, loss: -0.0647, took: 48.0655s
Batch 2800/3907, reward: 3.151, loss: -0.0668, took: 47.8271s
Batch 2900/3907, reward: 3.148, loss: -0.0666, took: 48.7434s
Batch 3000/3907, reward: 3.147, loss: -0.0500, took: 48.3092s
Batch 3100/3907, reward: 3.145, loss: -0.0736, took: 48.1468s
Batch 3200/3907, reward: 3.143, loss: -0.0698, took: 48.4944s
Batch 3300/3907, reward: 3.144, loss: -0.0604, took: 45.1022s
Batch 3400/3907, reward: 3.144, loss: -0.0695, took: 47.7036s
Batch 3500/3907, reward: 3.137, loss: -0.0553, took: 48.4636s
Batch 3600/3907, reward: 3.142, loss: -0.0667, took: 48.4429s
Batch 3700/3907, reward: 3.141, loss: -0.0713, took: 44.5500s
Batch 3800/3907, reward: 3.140, loss: -0.0555, took: 47.6180s
Batch 3900/3907, reward: 3.139, loss: -0.0604, took: 46.5642s
Mean epoch loss/reward: -0.0767, 3.1699, 3.1173, took: 1874.5756s (46.4881s / 100 batches)
Batch 0/3907, reward: 3.137, loss: 0.0430, took: 0.6581s
Batch 100/3907, reward: 3.138, loss: -0.0458, took: 49.0240s
Batch 200/3907, reward: 3.136, loss: -0.0596, took: 48.3728s
Batch 300/3907, reward: 3.137, loss: -0.0495, took: 47.9963s
Batch 400/3907, reward: 3.139, loss: -0.0570, took: 47.1281s
Batch 500/3907, reward: 3.135, loss: -0.0704, took: 47.1664s
Batch 600/3907, reward: 3.136, loss: -0.0594, took: 48.2121s
Batch 700/3907, reward: 3.132, loss: -0.0592, took: 48.6233s
Batch 800/3907, reward: 3.133, loss: -0.0566, took: 49.9824s
Batch 900/3907, reward: 3.133, loss: -0.0611, took: 48.0833s
Batch 1000/3907, reward: 3.132, loss: -0.0565, took: 49.6094s
Batch 1100/3907, reward: 3.133, loss: -0.0490, took: 47.6813s
Batch 1200/3907, reward: 3.133, loss: -0.0535, took: 44.3670s
Batch 1300/3907, reward: 3.129, loss: -0.0462, took: 47.8771s
Batch 1400/3907, reward: 3.129, loss: -0.0397, took: 48.6778s
Batch 1500/3907, reward: 3.130, loss: -0.0585, took: 48.8178s
Batch 1600/3907, reward: 3.130, loss: -0.0462, took: 48.2019s
Batch 1700/3907, reward: 3.127, loss: -0.0493, took: 48.7952s
Batch 1800/3907, reward: 3.128, loss: -0.0570, took: 48.2222s
Batch 1900/3907, reward: 3.126, loss: -0.0528, took: 46.8112s
Batch 2000/3907, reward: 3.129, loss: -0.0468, took: 49.2176s
Batch 2100/3907, reward: 3.127, loss: -0.0484, took: 46.8975s
Batch 2200/3907, reward: 3.130, loss: -0.0452, took: 46.1762s
Batch 2300/3907, reward: 3.127, loss: -0.0452, took: 47.7815s
Batch 2400/3907, reward: 3.126, loss: -0.0479, took: 47.8442s
Batch 2500/3907, reward: 3.125, loss: -0.0326, took: 49.0679s
Batch 2600/3907, reward: 3.127, loss: -0.0350, took: 48.0250s
Batch 2700/3907, reward: 3.127, loss: -0.0522, took: 48.1373s
Batch 2800/3907, reward: 3.124, loss: -0.0501, took: 48.0876s
Batch 2900/3907, reward: 3.124, loss: -0.0340, took: 48.6732s
Batch 3000/3907, reward: 3.125, loss: -0.0415, took: 47.3012s
Batch 3100/3907, reward: 3.122, loss: -0.0442, took: 49.4047s
Batch 3200/3907, reward: 3.123, loss: -0.0465, took: 47.5083s
Batch 3300/3907, reward: 3.126, loss: -0.0438, took: 44.0861s
Batch 3400/3907, reward: 3.122, loss: -0.0424, took: 47.5975s
Batch 3500/3907, reward: 3.122, loss: -0.0418, took: 49.6501s
Batch 3600/3907, reward: 3.121, loss: -0.0343, took: 47.7391s
Batch 3700/3907, reward: 3.119, loss: -0.0432, took: 45.9993s
Batch 3800/3907, reward: 3.121, loss: -0.0456, took: 47.4187s
Batch 3900/3907, reward: 3.120, loss: -0.0483, took: 49.1654s
Mean epoch loss/reward: -0.0487, 3.1282, 3.1020, took: 1884.0127s (46.7522s / 100 batches)
Batch 0/3907, reward: 3.112, loss: -0.1713, took: 0.7462s
Batch 100/3907, reward: 3.120, loss: -0.0427, took: 48.2840s
Batch 200/3907, reward: 3.120, loss: -0.0378, took: 47.4261s
Batch 300/3907, reward: 3.119, loss: -0.0509, took: 46.4381s
Batch 400/3907, reward: 3.117, loss: -0.0447, took: 48.6156s
Batch 500/3907, reward: 3.123, loss: -0.0405, took: 48.7609s
Batch 600/3907, reward: 3.121, loss: -0.0472, took: 47.5226s
Batch 700/3907, reward: 3.118, loss: -0.0490, took: 48.2947s
Batch 800/3907, reward: 3.118, loss: -0.0517, took: 47.6463s
Batch 900/3907, reward: 3.115, loss: -0.0381, took: 48.0381s
Batch 1000/3907, reward: 3.116, loss: -0.0400, took: 48.6282s
Batch 1100/3907, reward: 3.128, loss: -0.0527, took: 49.0205s
Batch 1200/3907, reward: 3.133, loss: -0.0406, took: 45.6923s
Batch 1300/3907, reward: 3.121, loss: -0.0391, took: 49.3677s
Batch 1400/3907, reward: 3.121, loss: -0.0409, took: 48.4117s
Batch 1500/3907, reward: 3.120, loss: -0.0445, took: 47.4926s
Batch 1600/3907, reward: 3.116, loss: -0.0401, took: 48.3525s
Batch 1700/3907, reward: 3.119, loss: -0.0385, took: 48.6626s
Batch 1800/3907, reward: 3.118, loss: -0.0302, took: 48.1325s
Batch 1900/3907, reward: 3.117, loss: -0.0368, took: 47.8776s
Batch 2000/3907, reward: 3.117, loss: -0.0390, took: 47.8350s
Batch 2100/3907, reward: 3.115, loss: -0.0393, took: 47.8537s
Batch 2200/3907, reward: 3.115, loss: -0.0372, took: 47.0529s
Batch 2300/3907, reward: 3.115, loss: -0.0378, took: 46.3481s
Batch 2400/3907, reward: 3.116, loss: -0.0390, took: 47.8616s
Batch 2500/3907, reward: 3.113, loss: -0.0371, took: 49.1132s
Batch 2600/3907, reward: 3.140, loss: -0.0354, took: 48.9877s
Batch 2700/3907, reward: 3.119, loss: -0.0423, took: 48.9182s
Batch 2800/3907, reward: 3.117, loss: -0.0358, took: 48.2537s
Batch 2900/3907, reward: 3.115, loss: -0.0465, took: 47.9810s
Batch 3000/3907, reward: 3.114, loss: -0.0468, took: 48.9439s
Batch 3100/3907, reward: 3.113, loss: -0.0397, took: 47.8982s
Batch 3200/3907, reward: 3.110, loss: -0.0331, took: 47.8557s
Batch 3300/3907, reward: 3.106, loss: -0.0395, took: 43.4786s
Batch 3400/3907, reward: 3.103, loss: -0.0401, took: 48.6178s
Batch 3500/3907, reward: 3.108, loss: -0.0418, took: 48.1986s
Batch 3600/3907, reward: 3.102, loss: -0.0509, took: 47.8056s
Batch 3700/3907, reward: 3.081, loss: -0.0449, took: 45.7512s
Batch 3800/3907, reward: 3.076, loss: -0.0475, took: 47.9485s
Batch 3900/3907, reward: 3.072, loss: -0.0351, took: 47.3213s
Mean epoch loss/reward: -0.0415, 3.1139, 3.0530, took: 1881.6316s (46.6859s / 100 batches)
Batch 0/3907, reward: 3.068, loss: -0.0660, took: 0.6031s
Batch 100/3907, reward: 3.074, loss: -0.0416, took: 48.3111s
Batch 200/3907, reward: 3.071, loss: -0.0371, took: 48.5598s
Batch 300/3907, reward: 3.086, loss: -0.0407, took: 46.6720s
Batch 400/3907, reward: 3.118, loss: -0.0382, took: 47.2526s
Batch 500/3907, reward: 3.128, loss: -0.0311, took: 47.3832s
Batch 600/3907, reward: 3.102, loss: -0.0387, took: 48.0152s
Batch 700/3907, reward: 3.105, loss: -0.0369, took: 48.3293s
Batch 800/3907, reward: 3.097, loss: -0.0374, took: 48.5052s
Batch 900/3907, reward: 3.093, loss: -0.0364, took: 49.6523s
Batch 1000/3907, reward: 3.081, loss: -0.0344, took: 47.3961s
Batch 1100/3907, reward: 3.072, loss: -0.0366, took: 46.2731s
Batch 1200/3907, reward: 3.063, loss: -0.0439, took: 47.4773s
Batch 1300/3907, reward: 3.065, loss: -0.0358, took: 48.7781s
Batch 1400/3907, reward: 3.073, loss: -0.0305, took: 48.0994s
Batch 1500/3907, reward: 3.066, loss: -0.0358, took: 48.2244s
Batch 1600/3907, reward: 3.060, loss: -0.0354, took: 47.9081s
Batch 1700/3907, reward: 3.060, loss: -0.0306, took: 47.8452s
Batch 1800/3907, reward: 3.059, loss: -0.0378, took: 46.6726s
Batch 1900/3907, reward: 3.072, loss: -0.0341, took: 48.4864s
Batch 2000/3907, reward: 3.063, loss: -0.0352, took: 48.0735s
Batch 2100/3907, reward: 3.059, loss: -0.0391, took: 47.7254s
Batch 2200/3907, reward: 3.058, loss: -0.0296, took: 46.7539s
Batch 2300/3907, reward: 3.061, loss: -0.0378, took: 46.0398s
Batch 2400/3907, reward: 3.058, loss: -0.0339, took: 48.4112s
Batch 2500/3907, reward: 3.056, loss: -0.0354, took: 47.4499s
Batch 2600/3907, reward: 3.056, loss: -0.0327, took: 47.2563s
Batch 2700/3907, reward: 3.060, loss: -0.0367, took: 46.5770s
Batch 2800/3907, reward: 3.062, loss: -0.0270, took: 47.3468s
Batch 2900/3907, reward: 3.053, loss: -0.0347, took: 47.7436s
Batch 3000/3907, reward: 3.055, loss: -0.0330, took: 48.9018s
Batch 3100/3907, reward: 3.055, loss: -0.0366, took: 49.5199s
Batch 3200/3907, reward: 3.062, loss: -0.0329, took: 49.2068s
Batch 3300/3907, reward: 3.057, loss: -0.0413, took: 46.0765s
Batch 3400/3907, reward: 3.053, loss: -0.0353, took: 47.6500s
Batch 3500/3907, reward: 3.053, loss: -0.0319, took: 48.0850s
Batch 3600/3907, reward: 3.053, loss: -0.0309, took: 47.7531s
Batch 3700/3907, reward: 3.054, loss: -0.0264, took: 44.5433s
Batch 3800/3907, reward: 3.059, loss: -0.0420, took: 47.2139s
Batch 3900/3907, reward: 3.100, loss: -0.0311, took: 48.1970s
Mean epoch loss/reward: -0.0353, 3.0701, 3.0708, took: 1875.4162s (46.5242s / 100 batches)
Batch 0/3907, reward: 3.094, loss: -0.0168, took: 0.5664s
Batch 100/3907, reward: 3.089, loss: -0.0325, took: 47.7022s
Batch 200/3907, reward: 3.103, loss: -0.0362, took: 49.2994s
Batch 300/3907, reward: 3.066, loss: -0.0358, took: 46.9108s
Batch 400/3907, reward: 3.058, loss: -0.0310, took: 48.5474s
Batch 500/3907, reward: 3.062, loss: -0.0315, took: 49.7792s
Batch 600/3907, reward: 3.055, loss: -0.0337, took: 48.6806s
Batch 700/3907, reward: 3.053, loss: -0.0305, took: 47.6674s
Batch 800/3907, reward: 3.067, loss: -0.0306, took: 47.2452s
Batch 900/3907, reward: 3.061, loss: -0.0324, took: 47.5902s
Batch 1000/3907, reward: 3.092, loss: -0.0318, took: 47.7728s
Batch 1100/3907, reward: 3.065, loss: -0.0322, took: 45.3750s
Batch 1200/3907, reward: 3.062, loss: -0.0323, took: 46.9915s
Batch 1300/3907, reward: 3.057, loss: -0.0284, took: 47.6894s
Batch 1400/3907, reward: 3.057, loss: -0.0304, took: 46.7719s
Batch 1500/3907, reward: 3.057, loss: -0.0276, took: 46.5858s
Batch 1600/3907, reward: 3.054, loss: -0.0337, took: 47.7756s
Batch 1700/3907, reward: 3.051, loss: -0.0338, took: 47.3692s
Batch 1800/3907, reward: 3.099, loss: -0.0394, took: 48.1571s
Batch 1900/3907, reward: 3.069, loss: -0.0336, took: 47.3073s
Batch 2000/3907, reward: 3.056, loss: -0.0308, took: 47.4877s
Batch 2100/3907, reward: 3.059, loss: -0.0324, took: 48.9002s
Batch 2200/3907, reward: 3.053, loss: -0.0312, took: 47.1577s
Batch 2300/3907, reward: 3.051, loss: -0.0298, took: 46.4851s
Batch 2400/3907, reward: 3.056, loss: -0.0332, took: 48.2528s
Batch 2500/3907, reward: 3.052, loss: -0.0354, took: 48.1914s
Batch 2600/3907, reward: 3.054, loss: -0.0339, took: 49.0895s
Batch 2700/3907, reward: 3.051, loss: -0.0288, took: 48.6148s
Batch 2800/3907, reward: 3.056, loss: -0.0308, took: 47.8581s
Batch 2900/3907, reward: 3.095, loss: -0.0239, took: 47.9556s
Batch 3000/3907, reward: 3.104, loss: -0.0288, took: 48.7061s
Batch 3100/3907, reward: 3.091, loss: -0.0307, took: 48.2065s
Batch 3200/3907, reward: 3.122, loss: -0.0456, took: 48.1096s
Batch 3300/3907, reward: 3.117, loss: -0.0329, took: 44.6810s
Batch 3400/3907, reward: 3.114, loss: -0.0343, took: 48.0881s
Batch 3500/3907, reward: 3.109, loss: -0.0339, took: 47.3359s
Batch 3600/3907, reward: 3.108, loss: -0.0367, took: 49.5145s
Batch 3700/3907, reward: 3.111, loss: -0.0322, took: 45.4034s
Batch 3800/3907, reward: 3.090, loss: -0.0283, took: 47.5623s
Batch 3900/3907, reward: 3.081, loss: -0.0261, took: 47.7130s
Mean epoch loss/reward: -0.0323, 3.0745, 3.0602, took: 1874.7131s (46.5274s / 100 batches)
Batch 0/3907, reward: 3.071, loss: -0.0870, took: 0.6099s
Batch 100/3907, reward: 3.068, loss: -0.0293, took: 48.4019s
Batch 200/3907, reward: 3.068, loss: -0.0296, took: 47.6171s
Batch 300/3907, reward: 3.056, loss: -0.0293, took: 46.8861s
Batch 400/3907, reward: 3.080, loss: -0.0325, took: 48.1068s
Batch 500/3907, reward: 3.066, loss: -0.0303, took: 47.6001s
Batch 600/3907, reward: 3.054, loss: -0.0297, took: 48.0515s
Batch 700/3907, reward: 3.052, loss: -0.0243, took: 48.9750s
Batch 800/3907, reward: 3.053, loss: -0.0330, took: 46.9403s
Batch 900/3907, reward: 3.055, loss: -0.0319, took: 46.8021s
Batch 1000/3907, reward: 3.052, loss: -0.0256, took: 48.4160s
Batch 1100/3907, reward: 3.051, loss: -0.0277, took: 45.7494s
Batch 1200/3907, reward: 3.073, loss: -0.0296, took: 47.4347s
Batch 1300/3907, reward: 3.076, loss: -0.0325, took: 48.9409s
Batch 1400/3907, reward: 3.071, loss: -0.0310, took: 48.2060s
Batch 1500/3907, reward: 3.070, loss: -0.0301, took: 49.3003s
Batch 1600/3907, reward: 3.081, loss: -0.0278, took: 47.3284s
Batch 1700/3907, reward: 3.071, loss: -0.0220, took: 47.4506s
Batch 1800/3907, reward: 3.083, loss: -0.0270, took: 46.7724s
Batch 1900/3907, reward: 3.070, loss: -0.0313, took: 48.6744s
Batch 2000/3907, reward: 3.061, loss: -0.0266, took: 47.0377s
Batch 2100/3907, reward: 3.057, loss: -0.0248, took: 47.6184s
Batch 2200/3907, reward: 3.065, loss: -0.0246, took: 45.6829s
Batch 2300/3907, reward: 3.061, loss: -0.0330, took: 47.5508s
Batch 2400/3907, reward: 3.056, loss: -0.0269, took: 47.4663s
Batch 2500/3907, reward: 3.051, loss: -0.0241, took: 48.7150s
Batch 2600/3907, reward: 3.049, loss: -0.0309, took: 48.4765s
Batch 2700/3907, reward: 3.049, loss: -0.0292, took: 48.4507s
Batch 2800/3907, reward: 3.052, loss: -0.0296, took: 47.8789s
Batch 2900/3907, reward: 3.064, loss: -0.0246, took: 48.7051s
Batch 3000/3907, reward: 3.069, loss: -0.0278, took: 47.9403s
Batch 3100/3907, reward: 3.084, loss: -0.0304, took: 47.1880s
Batch 3200/3907, reward: 3.095, loss: -0.0266, took: 48.5522s
Batch 3300/3907, reward: 3.079, loss: -0.0290, took: 44.1607s
Batch 3400/3907, reward: 3.064, loss: -0.0248, took: 47.8292s
Batch 3500/3907, reward: 3.057, loss: -0.0233, took: 47.5234s
Batch 3600/3907, reward: 3.058, loss: -0.0262, took: 47.1977s
Batch 3700/3907, reward: 3.051, loss: -0.0271, took: 44.2203s
Batch 3800/3907, reward: 3.051, loss: -0.0250, took: 47.1398s
Batch 3900/3907, reward: 3.051, loss: -0.0309, took: 48.2495s
Mean epoch loss/reward: -0.0282, 3.0634, 3.0429, took: 1869.5543s (46.3962s / 100 batches)
Batch 0/3907, reward: 3.037, loss: 0.0126, took: 0.5882s
Batch 100/3907, reward: 3.051, loss: -0.0249, took: 49.0459s
Batch 200/3907, reward: 3.054, loss: -0.0260, took: 47.1135s
Batch 300/3907, reward: 3.049, loss: -0.0234, took: 47.1892s
Batch 400/3907, reward: 3.049, loss: -0.0279, took: 48.9383s
Batch 500/3907, reward: 3.046, loss: -0.0261, took: 47.1766s
Batch 600/3907, reward: 3.052, loss: -0.0296, took: 47.1286s
Batch 700/3907, reward: 3.050, loss: -0.0257, took: 48.2004s
Batch 800/3907, reward: 3.047, loss: -0.0270, took: 47.9033s
Batch 900/3907, reward: 3.048, loss: -0.0271, took: 47.8361s
Batch 1000/3907, reward: 3.049, loss: -0.0246, took: 47.8505s
Batch 1100/3907, reward: 3.048, loss: -0.0290, took: 44.8629s
Batch 1200/3907, reward: 3.046, loss: -0.0260, took: 48.2585s
Batch 1300/3907, reward: 3.058, loss: -0.0287, took: 49.3762s
Batch 1400/3907, reward: 3.066, loss: -0.0302, took: 47.5675s
Batch 1500/3907, reward: 3.111, loss: -0.0301, took: 46.3560s
Batch 1600/3907, reward: 3.074, loss: -0.0334, took: 47.8683s
Batch 1700/3907, reward: 3.065, loss: -0.0270, took: 47.4860s
Batch 1800/3907, reward: 3.084, loss: -0.0237, took: 47.9426s
Batch 1900/3907, reward: 3.090, loss: -0.0272, took: 47.6515s
Batch 2000/3907, reward: 3.074, loss: -0.0271, took: 48.7073s
Batch 2100/3907, reward: 3.070, loss: -0.0282, took: 47.6853s
Batch 2200/3907, reward: 3.061, loss: -0.0305, took: 47.5654s
Batch 2300/3907, reward: 3.056, loss: -0.0299, took: 43.7737s
Batch 2400/3907, reward: 3.060, loss: -0.0266, took: 49.4040s
Batch 2500/3907, reward: 3.062, loss: -0.0291, took: 47.3434s
Batch 2600/3907, reward: 3.058, loss: -0.0286, took: 47.6846s
Batch 2700/3907, reward: 3.054, loss: -0.0253, took: 48.3790s
Batch 2800/3907, reward: 3.054, loss: -0.0294, took: 47.5214s
Batch 2900/3907, reward: 3.052, loss: -0.0284, took: 46.8334s
Batch 3000/3907, reward: 3.047, loss: -0.0238, took: 47.5622s
Batch 3100/3907, reward: 3.048, loss: -0.0235, took: 47.3781s
Batch 3200/3907, reward: 3.049, loss: -0.0261, took: 47.7317s
Batch 3300/3907, reward: 3.047, loss: -0.0257, took: 44.9473s
Batch 3400/3907, reward: 3.053, loss: -0.0275, took: 48.0714s
Batch 3500/3907, reward: 3.045, loss: -0.0248, took: 47.8837s
Batch 3600/3907, reward: 3.044, loss: -0.0259, took: 48.6676s
Batch 3700/3907, reward: 3.050, loss: -0.0234, took: 44.6976s
Batch 3800/3907, reward: 3.066, loss: -0.0260, took: 47.0208s
Batch 3900/3907, reward: 3.082, loss: -0.0282, took: 46.5159s
Mean epoch loss/reward: -0.0271, 3.0583, 3.1025, took: 1865.0576s (46.2928s / 100 batches)
Batch 0/3907, reward: 3.110, loss: 0.0288, took: 0.6366s
Batch 100/3907, reward: 3.108, loss: -0.0299, took: 48.6747s
Batch 200/3907, reward: 3.072, loss: -0.0254, took: 47.5540s
Batch 300/3907, reward: 3.063, loss: -0.0237, took: 47.4290s
Batch 400/3907, reward: 3.078, loss: -0.0285, took: 46.3020s
Batch 500/3907, reward: 3.079, loss: -0.0216, took: 47.7749s
Batch 600/3907, reward: 3.093, loss: -0.0309, took: 49.4759s
Batch 700/3907, reward: 3.077, loss: -0.0248, took: 47.8341s
Batch 800/3907, reward: 3.065, loss: -0.0281, took: 47.5678s
Batch 900/3907, reward: 3.065, loss: -0.0309, took: 47.3110s
Batch 1000/3907, reward: 3.053, loss: -0.0233, took: 47.6319s
Batch 1100/3907, reward: 3.048, loss: -0.0238, took: 45.6915s
Batch 1200/3907, reward: 3.046, loss: -0.0216, took: 47.6738s
Batch 1300/3907, reward: 3.048, loss: -0.0315, took: 48.6028s
Batch 1400/3907, reward: 3.050, loss: -0.0251, took: 49.1179s
Batch 1500/3907, reward: 3.048, loss: -0.0245, took: 47.6584s
Batch 1600/3907, reward: 3.045, loss: -0.0230, took: 46.8964s
Batch 1700/3907, reward: 3.044, loss: -0.0215, took: 47.9934s
Batch 1800/3907, reward: 3.065, loss: -0.0260, took: 48.1623s
Batch 1900/3907, reward: 3.059, loss: -0.0238, took: 46.9189s
Batch 2000/3907, reward: 3.070, loss: -0.0271, took: 47.5888s
Batch 2100/3907, reward: 3.058, loss: -0.0241, took: 47.7850s
Batch 2200/3907, reward: 3.082, loss: -0.0246, took: 47.5170s
Batch 2300/3907, reward: 3.071, loss: -0.0259, took: 45.8742s
Batch 2400/3907, reward: 3.066, loss: -0.0238, took: 47.9169s
Batch 2500/3907, reward: 3.057, loss: -0.0271, took: 47.4190s
Batch 2600/3907, reward: 3.072, loss: -0.0217, took: 46.6200s
Batch 2700/3907, reward: 3.061, loss: -0.0278, took: 48.4054s
Batch 2800/3907, reward: 3.052, loss: -0.0205, took: 47.3162s
Batch 2900/3907, reward: 3.064, loss: -0.0280, took: 46.7849s
Batch 3000/3907, reward: 3.059, loss: -0.0255, took: 46.6248s
Batch 3100/3907, reward: 3.051, loss: -0.0230, took: 48.2617s
Batch 3200/3907, reward: 3.052, loss: -0.0216, took: 47.4471s
Batch 3300/3907, reward: 3.051, loss: -0.0273, took: 44.3910s
Batch 3400/3907, reward: 3.051, loss: -0.0216, took: 49.0283s
Batch 3500/3907, reward: 3.063, loss: -0.0294, took: 48.1816s
Batch 3600/3907, reward: 3.050, loss: -0.0265, took: 49.2245s
Batch 3700/3907, reward: 3.049, loss: -0.0268, took: 46.3835s
Batch 3800/3907, reward: 3.046, loss: -0.0203, took: 48.0227s
Batch 3900/3907, reward: 3.046, loss: -0.0229, took: 47.7368s
Mean epoch loss/reward: -0.0252, 3.0609, 3.0436, took: 1869.4480s (46.3859s / 100 batches)
Batch 0/3907, reward: 3.048, loss: -0.0653, took: 0.8576s
Batch 100/3907, reward: 3.047, loss: -0.0214, took: 47.5402s
Batch 200/3907, reward: 3.047, loss: -0.0245, took: 47.7140s
Batch 300/3907, reward: 3.050, loss: -0.0272, took: 47.2403s
Batch 400/3907, reward: 3.046, loss: -0.0196, took: 46.6995s
Batch 500/3907, reward: 3.047, loss: -0.0234, took: 48.3914s
Batch 600/3907, reward: 3.046, loss: -0.0210, took: 47.2568s
Batch 700/3907, reward: 3.045, loss: -0.0271, took: 48.3847s
Batch 800/3907, reward: 3.045, loss: -0.0227, took: 48.5723s
Batch 900/3907, reward: 3.044, loss: -0.0257, took: 48.0828s
Batch 1000/3907, reward: 3.044, loss: -0.0241, took: 47.6618s
Batch 1100/3907, reward: 3.045, loss: -0.0193, took: 45.5005s
Batch 1200/3907, reward: 3.048, loss: -0.0229, took: 46.6136s
Batch 1300/3907, reward: 3.043, loss: -0.0219, took: 48.2239s
Batch 1400/3907, reward: 3.047, loss: -0.0227, took: 46.7146s
Batch 1500/3907, reward: 3.044, loss: -0.0258, took: 47.3287s
Batch 1600/3907, reward: 3.043, loss: -0.0226, took: 47.8076s
Batch 1700/3907, reward: 3.043, loss: -0.0257, took: 46.8399s
Batch 1800/3907, reward: 3.050, loss: -0.0281, took: 48.2809s
Batch 1900/3907, reward: 3.043, loss: -0.0221, took: 47.3979s
Batch 2000/3907, reward: 3.044, loss: -0.0222, took: 47.3624s
Batch 2100/3907, reward: 3.043, loss: -0.0237, took: 48.2738s
Batch 2200/3907, reward: 3.044, loss: -0.0204, took: 47.8286s
Batch 2300/3907, reward: 3.043, loss: -0.0211, took: 45.0934s
Batch 2400/3907, reward: 3.045, loss: -0.0223, took: 48.2571s
Batch 2500/3907, reward: 3.044, loss: -0.0243, took: 48.0840s
Batch 2600/3907, reward: 3.044, loss: -0.0236, took: 48.8132s
Batch 2700/3907, reward: 3.050, loss: -0.0234, took: 48.5395s
Batch 2800/3907, reward: 3.081, loss: -0.0264, took: 48.1264s
Batch 2900/3907, reward: 3.057, loss: -0.0243, took: 47.8875s
Batch 3000/3907, reward: 3.060, loss: -0.0284, took: 48.0232s
Batch 3100/3907, reward: 3.061, loss: -0.0244, took: 47.7940s
Batch 3200/3907, reward: 3.047, loss: -0.0172, took: 47.5691s
Batch 3300/3907, reward: 3.042, loss: -0.0217, took: 45.1570s
Batch 3400/3907, reward: 3.044, loss: -0.0198, took: 46.9297s
Batch 3500/3907, reward: 3.041, loss: -0.0233, took: 47.6354s
Batch 3600/3907, reward: 3.044, loss: -0.0196, took: 46.3796s
Batch 3700/3907, reward: 3.043, loss: -0.0204, took: 47.4300s
Batch 3800/3907, reward: 3.042, loss: -0.0198, took: 48.3325s
Batch 3900/3907, reward: 3.043, loss: -0.0236, took: 47.4698s
Mean epoch loss/reward: -0.0230, 3.0469, 3.0410, took: 1868.0558s (46.3524s / 100 batches)
Batch 0/3907, reward: 3.045, loss: -0.0482, took: 0.6409s
Batch 100/3907, reward: 3.048, loss: -0.0213, took: 47.0823s
Batch 200/3907, reward: 3.044, loss: -0.0209, took: 47.4967s
Batch 300/3907, reward: 3.044, loss: -0.0205, took: 49.0827s
Batch 400/3907, reward: 3.046, loss: -0.0220, took: 48.1775s
Batch 500/3907, reward: 3.046, loss: -0.0241, took: 47.7289s
Batch 600/3907, reward: 3.043, loss: -0.0248, took: 49.3129s
Batch 700/3907, reward: 3.049, loss: -0.0202, took: 48.4383s
Batch 800/3907, reward: 3.041, loss: -0.0223, took: 48.4725s
Batch 900/3907, reward: 3.042, loss: -0.0197, took: 48.7822s
Batch 1000/3907, reward: 3.044, loss: -0.0274, took: 48.3406s
Batch 1100/3907, reward: 3.083, loss: -0.0271, took: 46.6712s
Batch 1200/3907, reward: 3.095, loss: -0.0281, took: 47.2809s
Batch 1300/3907, reward: 3.103, loss: -0.0231, took: 46.8944s
Batch 1400/3907, reward: 3.110, loss: -0.0249, took: 48.8433s
Batch 1500/3907, reward: 3.102, loss: -0.0260, took: 47.2692s
Batch 1600/3907, reward: 3.087, loss: -0.0241, took: 48.0824s
Batch 1700/3907, reward: 3.101, loss: -0.0232, took: 47.9869s
Batch 1800/3907, reward: 3.066, loss: -0.0276, took: 46.4119s
Batch 1900/3907, reward: 3.076, loss: -0.0252, took: 48.4428s
Batch 2000/3907, reward: 3.111, loss: -0.0224, took: 47.0168s
Batch 2100/3907, reward: 3.083, loss: -0.0210, took: 48.1969s
Batch 2200/3907, reward: 3.057, loss: -0.0263, took: 46.8636s
Batch 2300/3907, reward: 3.077, loss: -0.0233, took: 44.4189s
Batch 2400/3907, reward: 3.059, loss: -0.0212, took: 47.5767s
Batch 2500/3907, reward: 3.053, loss: -0.0251, took: 48.2595s
Batch 2600/3907, reward: 3.053, loss: -0.0221, took: 47.5069s
Batch 2700/3907, reward: 3.076, loss: -0.0208, took: 48.6859s
Batch 2800/3907, reward: 3.068, loss: -0.0185, took: 47.8829s
Batch 2900/3907, reward: 3.068, loss: -0.0230, took: 47.5129s
Batch 3000/3907, reward: 3.063, loss: -0.0219, took: 48.8155s
Batch 3100/3907, reward: 3.072, loss: -0.0232, took: 48.2468s
Batch 3200/3907, reward: 3.068, loss: -0.0204, took: 47.0590s
Batch 3300/3907, reward: 3.068, loss: -0.0219, took: 45.9610s
Batch 3400/3907, reward: 3.100, loss: -0.0205, took: 47.2132s
Batch 3500/3907, reward: 3.089, loss: -0.0222, took: 48.1006s
Batch 3600/3907, reward: 3.076, loss: -0.0192, took: 49.0464s
Batch 3700/3907, reward: 3.081, loss: -0.0279, took: 44.4295s
Batch 3800/3907, reward: 3.094, loss: -0.0213, took: 47.3500s
Batch 3900/3907, reward: 3.087, loss: -0.0258, took: 48.7849s
Mean epoch loss/reward: -0.0231, 3.0712, 3.0741, took: 1874.1550s (46.5092s / 100 batches)
Batch 0/3907, reward: 3.090, loss: -0.0730, took: 0.6323s
Batch 100/3907, reward: 3.081, loss: -0.0216, took: 48.0829s
Batch 200/3907, reward: 3.075, loss: -0.0206, took: 48.4366s
Batch 300/3907, reward: 3.064, loss: -0.0188, took: 47.3962s
Batch 400/3907, reward: 3.062, loss: -0.0167, took: 48.6941s
Batch 500/3907, reward: 3.061, loss: -0.0210, took: 47.7834s
Batch 600/3907, reward: 3.054, loss: -0.0254, took: 46.4510s
Batch 700/3907, reward: 3.060, loss: -0.0174, took: 47.4515s
Batch 800/3907, reward: 3.057, loss: -0.0195, took: 47.3496s
Batch 900/3907, reward: 3.072, loss: -0.0229, took: 50.3707s
Batch 1000/3907, reward: 3.061, loss: -0.0204, took: 47.3151s
Batch 1100/3907, reward: 3.070, loss: -0.0156, took: 44.5178s
Batch 1200/3907, reward: 3.063, loss: -0.0170, took: 48.6756s
Batch 1300/3907, reward: 3.068, loss: -0.0233, took: 47.9265s
Batch 1400/3907, reward: 3.056, loss: -0.0197, took: 48.2645s
Batch 1500/3907, reward: 3.056, loss: -0.0185, took: 47.4865s
Batch 1600/3907, reward: 3.052, loss: -0.0210, took: 48.0078s
Batch 1700/3907, reward: 3.050, loss: -0.0216, took: 48.4019s
Batch 1800/3907, reward: 3.049, loss: -0.0223, took: 46.3363s
Batch 1900/3907, reward: 3.046, loss: -0.0176, took: 48.1188s
Batch 2000/3907, reward: 3.045, loss: -0.0205, took: 47.4934s
Batch 2100/3907, reward: 3.056, loss: -0.0198, took: 48.0104s
Batch 2200/3907, reward: 3.059, loss: -0.0206, took: 47.6304s
Batch 2300/3907, reward: 3.079, loss: -0.0256, took: 45.1388s
Batch 2400/3907, reward: 3.073, loss: -0.0239, took: 48.6195s
Batch 2500/3907, reward: 3.090, loss: -0.0232, took: 47.7967s
Batch 2600/3907, reward: 3.057, loss: -0.0209, took: 47.1573s
Batch 2700/3907, reward: 3.054, loss: -0.0200, took: 48.3579s
Batch 2800/3907, reward: 3.054, loss: -0.0218, took: 47.9559s
Batch 2900/3907, reward: 3.056, loss: -0.0157, took: 47.6414s
Batch 3000/3907, reward: 3.069, loss: -0.0228, took: 49.3395s
Batch 3100/3907, reward: 3.075, loss: -0.0169, took: 47.4333s
Batch 3200/3907, reward: 3.067, loss: -0.0207, took: 46.9186s
Batch 3300/3907, reward: 3.061, loss: -0.0202, took: 44.5814s
Batch 3400/3907, reward: 3.057, loss: -0.0216, took: 48.0896s
Batch 3500/3907, reward: 3.054, loss: -0.0205, took: 46.1348s
Batch 3600/3907, reward: 3.047, loss: -0.0197, took: 47.2678s
Batch 3700/3907, reward: 3.047, loss: -0.0205, took: 44.6489s
Batch 3800/3907, reward: 3.051, loss: -0.0207, took: 47.6164s
Batch 3900/3907, reward: 3.061, loss: -0.0207, took: 48.0159s
Mean epoch loss/reward: -0.0205, 3.0608, 3.0562, took: 1867.5696s (46.3387s / 100 batches)
Batch 0/3907, reward: 3.063, loss: 0.0919, took: 0.7015s
Batch 100/3907, reward: 3.067, loss: -0.0233, took: 48.3369s
Batch 200/3907, reward: 3.057, loss: -0.0237, took: 47.6931s
Batch 300/3907, reward: 3.052, loss: -0.0198, took: 48.1300s
Batch 400/3907, reward: 3.061, loss: -0.0209, took: 48.6382s
Batch 500/3907, reward: 3.080, loss: -0.0194, took: 48.8173s
Batch 600/3907, reward: 3.065, loss: -0.0239, took: 47.4773s
Batch 700/3907, reward: 3.067, loss: -0.0223, took: 48.1026s
Batch 800/3907, reward: 3.076, loss: -0.0199, took: 47.3128s
Batch 900/3907, reward: 3.072, loss: -0.0200, took: 47.6137s
Batch 1000/3907, reward: 3.063, loss: -0.0199, took: 48.7158s
Batch 1100/3907, reward: 3.076, loss: -0.0240, took: 43.4915s
Batch 1200/3907, reward: 3.075, loss: -0.0187, took: 49.0900s
Batch 1300/3907, reward: 3.093, loss: -0.0264, took: 47.5648s
Batch 1400/3907, reward: 3.094, loss: -0.0195, took: 48.4767s
Batch 1500/3907, reward: 3.081, loss: -0.0204, took: 47.7022s
Batch 1600/3907, reward: 3.082, loss: -0.0218, took: 47.2452s
Batch 1700/3907, reward: 3.071, loss: -0.0247, took: 47.2053s
Batch 1800/3907, reward: 3.069, loss: -0.0191, took: 47.2106s
Batch 1900/3907, reward: 3.069, loss: -0.0187, took: 47.2920s
Batch 2000/3907, reward: 3.058, loss: -0.0211, took: 48.7853s
Batch 2100/3907, reward: 3.048, loss: -0.0165, took: 48.1449s
Batch 2200/3907, reward: 3.048, loss: -0.0215, took: 46.8724s
Batch 2300/3907, reward: 3.053, loss: -0.0201, took: 45.2475s
Batch 2400/3907, reward: 3.056, loss: -0.0240, took: 47.9310s
Batch 2500/3907, reward: 3.056, loss: -0.0206, took: 48.1989s
Batch 2600/3907, reward: 3.054, loss: -0.0196, took: 48.7002s
Batch 2700/3907, reward: 3.048, loss: -0.0234, took: 47.8549s
Batch 2800/3907, reward: 3.049, loss: -0.0182, took: 47.6059s
Batch 2900/3907, reward: 3.051, loss: -0.0202, took: 48.1015s
Batch 3000/3907, reward: 3.052, loss: -0.0154, took: 46.9559s
Batch 3100/3907, reward: 3.045, loss: -0.0241, took: 48.3798s
Batch 3200/3907, reward: 3.049, loss: -0.0187, took: 47.1178s
Batch 3300/3907, reward: 3.049, loss: -0.0190, took: 44.2466s
Batch 3400/3907, reward: 3.046, loss: -0.0200, took: 48.0820s
Batch 3500/3907, reward: 3.046, loss: -0.0185, took: 47.5890s
Batch 3600/3907, reward: 3.052, loss: -0.0221, took: 46.3584s
Batch 3700/3907, reward: 3.061, loss: -0.0188, took: 43.1544s
Batch 3800/3907, reward: 3.056, loss: -0.0218, took: 46.0327s
Batch 3900/3907, reward: 3.049, loss: -0.0216, took: 48.0181s
Mean epoch loss/reward: -0.0208, 3.0614, 3.0411, took: 1864.5768s (46.2549s / 100 batches)
Batch 0/3907, reward: 3.050, loss: -0.0357, took: 0.6239s
Batch 100/3907, reward: 3.054, loss: -0.0199, took: 47.8740s
Batch 200/3907, reward: 3.059, loss: -0.0184, took: 46.8894s
Batch 300/3907, reward: 3.078, loss: -0.0173, took: 47.6323s
Batch 400/3907, reward: 3.070, loss: -0.0204, took: 46.9203s
Batch 500/3907, reward: 3.055, loss: -0.0194, took: 48.8117s
Batch 600/3907, reward: 3.065, loss: -0.0212, took: 47.9142s
Batch 700/3907, reward: 3.062, loss: -0.0202, took: 48.5974s
Batch 800/3907, reward: 3.055, loss: -0.0197, took: 47.9612s
Batch 900/3907, reward: 3.052, loss: -0.0205, took: 48.5319s
Batch 1000/3907, reward: 3.065, loss: -0.0179, took: 48.0574s
Batch 1100/3907, reward: 3.060, loss: -0.0207, took: 45.9686s
Batch 1200/3907, reward: 3.054, loss: -0.0192, took: 45.6968s
Batch 1300/3907, reward: 3.066, loss: -0.0182, took: 47.6540s
Batch 1400/3907, reward: 3.065, loss: -0.0212, took: 48.6407s
Batch 1500/3907, reward: 3.066, loss: -0.0259, took: 47.9134s
Batch 1600/3907, reward: 3.088, loss: -0.0205, took: 48.1300s
Batch 1700/3907, reward: 3.067, loss: -0.0205, took: 46.8690s
Batch 1800/3907, reward: 3.069, loss: -0.0221, took: 47.2486s
Batch 1900/3907, reward: 3.054, loss: -0.0190, took: 48.2465s
Batch 2000/3907, reward: 3.073, loss: -0.0188, took: 46.6160s
Batch 2100/3907, reward: 3.072, loss: -0.0249, took: 48.4360s
Batch 2200/3907, reward: 3.061, loss: -0.0207, took: 46.9771s
Batch 2300/3907, reward: 3.050, loss: -0.0189, took: 45.6608s
Batch 2400/3907, reward: 3.072, loss: -0.0264, took: 47.2711s
Batch 2500/3907, reward: 3.066, loss: -0.0193, took: 47.6189s
Batch 2600/3907, reward: 3.058, loss: -0.0188, took: 48.8709s
Batch 2700/3907, reward: 3.052, loss: -0.0195, took: 48.8438s
Batch 2800/3907, reward: 3.052, loss: -0.0227, took: 47.0722s
Batch 2900/3907, reward: 3.054, loss: -0.0205, took: 46.6692s
Batch 3000/3907, reward: 3.051, loss: -0.0194, took: 48.4662s
Batch 3100/3907, reward: 3.070, loss: -0.0214, took: 47.4883s
Batch 3200/3907, reward: 3.065, loss: -0.0232, took: 48.1550s
Batch 3300/3907, reward: 3.060, loss: -0.0210, took: 44.6254s
Batch 3400/3907, reward: 3.046, loss: -0.0210, took: 47.6579s
Batch 3500/3907, reward: 3.054, loss: -0.0218, took: 48.4337s
Batch 3600/3907, reward: 3.066, loss: -0.0211, took: 48.3442s
Batch 3700/3907, reward: 3.052, loss: -0.0188, took: 45.6409s
Batch 3800/3907, reward: 3.078, loss: -0.0214, took: 38.3181s
Batch 3900/3907, reward: 3.080, loss: -0.0196, took: 38.4102s
Mean epoch loss/reward: -0.0205, 3.0626, 3.0718, took: 1849.3405s (45.8939s / 100 batches)
Batch 0/3907, reward: 3.074, loss: 0.0111, took: 0.6156s
Batch 100/3907, reward: 3.074, loss: -0.0199, took: 38.4959s
Batch 200/3907, reward: 3.059, loss: -0.0193, took: 44.2588s
Batch 300/3907, reward: 3.069, loss: -0.0182, took: 47.5303s
Batch 400/3907, reward: 3.073, loss: -0.0225, took: 46.9062s
Batch 500/3907, reward: 3.077, loss: -0.0188, took: 47.0782s
Batch 600/3907, reward: 3.069, loss: -0.0178, took: 46.5910s
Batch 700/3907, reward: 3.061, loss: -0.0164, took: 48.1110s
Batch 800/3907, reward: 3.057, loss: -0.0203, took: 47.9157s
Batch 900/3907, reward: 3.052, loss: -0.0207, took: 46.7647s
Batch 1000/3907, reward: 3.053, loss: -0.0189, took: 47.3953s
Batch 1100/3907, reward: 3.050, loss: -0.0217, took: 47.2986s
Batch 1200/3907, reward: 3.056, loss: -0.0231, took: 45.0921s
Batch 1300/3907, reward: 3.068, loss: -0.0227, took: 47.2430s
Batch 1400/3907, reward: 3.066, loss: -0.0199, took: 47.1009s
Batch 1500/3907, reward: 3.067, loss: -0.0165, took: 38.4230s
Batch 1600/3907, reward: 3.064, loss: -0.0203, took: 40.0083s
Batch 1700/3907, reward: 3.069, loss: -0.0196, took: 38.2303s
Batch 1800/3907, reward: 3.062, loss: -0.0187, took: 38.9561s
Batch 1900/3907, reward: 3.061, loss: -0.0180, took: 39.2277s
Batch 2000/3907, reward: 3.062, loss: -0.0210, took: 37.5576s
Batch 2100/3907, reward: 3.056, loss: -0.0168, took: 37.9303s
Batch 2200/3907, reward: 3.056, loss: -0.0171, took: 36.4431s
Batch 2300/3907, reward: 3.054, loss: -0.0193, took: 29.3122s
Batch 2400/3907, reward: 3.052, loss: -0.0186, took: 30.0790s
Batch 2500/3907, reward: 3.053, loss: -0.0183, took: 30.5636s
Batch 2600/3907, reward: 3.053, loss: -0.0215, took: 29.9306s
Batch 2700/3907, reward: 3.063, loss: -0.0170, took: 32.1634s
Batch 2800/3907, reward: 3.067, loss: -0.0232, took: 37.4142s
Batch 2900/3907, reward: 3.064, loss: -0.0171, took: 39.2462s
Batch 3000/3907, reward: 3.061, loss: -0.0191, took: 38.0169s
Batch 3100/3907, reward: 3.060, loss: -0.0226, took: 37.3187s
Batch 3200/3907, reward: 3.070, loss: -0.0203, took: 37.0778s
Batch 3300/3907, reward: 3.065, loss: -0.0197, took: 34.1677s
Batch 3400/3907, reward: 3.057, loss: -0.0177, took: 38.2643s
Batch 3500/3907, reward: 3.067, loss: -0.0173, took: 37.5456s
Batch 3600/3907, reward: 3.085, loss: -0.0207, took: 35.8769s
Batch 3700/3907, reward: 3.085, loss: -0.0171, took: 38.7995s
Batch 3800/3907, reward: 3.064, loss: -0.0178, took: 37.7073s
Batch 3900/3907, reward: 3.064, loss: -0.0208, took: 37.7099s
Mean epoch loss/reward: -0.0194, 3.0631, 3.0498, took: 1570.0765s (38.9092s / 100 batches)
Batch 0/3907, reward: 3.064, loss: 0.0164, took: 0.4254s
Batch 100/3907, reward: 3.059, loss: -0.0204, took: 30.0571s
Batch 200/3907, reward: 3.060, loss: -0.0198, took: 30.2847s
Batch 300/3907, reward: 3.072, loss: -0.0197, took: 29.7502s
Batch 400/3907, reward: 3.057, loss: -0.0197, took: 30.3181s
Batch 500/3907, reward: 3.057, loss: -0.0217, took: 29.4210s
Batch 600/3907, reward: 3.064, loss: -0.0195, took: 29.9748s
Batch 700/3907, reward: 3.060, loss: -0.0170, took: 29.0048s
Batch 800/3907, reward: 3.052, loss: -0.0196, took: 29.3954s
Batch 900/3907, reward: 3.045, loss: -0.0186, took: 28.9083s
Batch 1000/3907, reward: 3.047, loss: -0.0178, took: 30.5590s
Batch 1100/3907, reward: 3.048, loss: -0.0203, took: 26.5923s
Batch 1200/3907, reward: 3.054, loss: -0.0206, took: 20.1173s
Batch 1300/3907, reward: 3.049, loss: -0.0185, took: 20.7227s
Batch 1400/3907, reward: 3.049, loss: -0.0185, took: 20.4290s
Batch 1500/3907, reward: 3.050, loss: -0.0195, took: 22.3496s
Batch 1600/3907, reward: 3.048, loss: -0.0171, took: 22.7245s
Batch 1700/3907, reward: 3.064, loss: -0.0235, took: 21.3656s
Batch 1800/3907, reward: 3.081, loss: -0.0188, took: 25.6521s
Batch 1900/3907, reward: 3.080, loss: -0.0161, took: 29.4512s
Batch 2000/3907, reward: 3.067, loss: -0.0176, took: 31.5820s
Batch 2100/3907, reward: 3.064, loss: -0.0169, took: 28.8550s
Batch 2200/3907, reward: 3.079, loss: -0.0222, took: 28.5399s
Batch 2300/3907, reward: 3.082, loss: -0.0209, took: 29.7079s
Batch 2400/3907, reward: 3.073, loss: -0.0202, took: 29.5873s
Batch 2500/3907, reward: 3.065, loss: -0.0186, took: 29.9834s
Batch 2600/3907, reward: 3.068, loss: -0.0211, took: 29.4540s
Batch 2700/3907, reward: 3.076, loss: -0.0234, took: 30.0398s
Batch 2800/3907, reward: 3.069, loss: -0.0216, took: 31.1076s
Batch 2900/3907, reward: 3.062, loss: -0.0203, took: 30.6797s
Batch 3000/3907, reward: 3.056, loss: -0.0190, took: 25.6952s
Batch 3100/3907, reward: 3.061, loss: -0.0266, took: 21.3607s
Batch 3200/3907, reward: 3.073, loss: -0.0235, took: 21.1039s
Batch 3300/3907, reward: 3.063, loss: -0.0201, took: 18.0201s
Batch 3400/3907, reward: 3.065, loss: -0.0197, took: 18.6994s
Batch 3500/3907, reward: 3.063, loss: -0.0173, took: 16.7723s
Batch 3600/3907, reward: 3.062, loss: -0.0182, took: 24.5426s
Batch 3700/3907, reward: 3.058, loss: -0.0220, took: 21.4993s
Batch 3800/3907, reward: 3.074, loss: -0.0224, took: 23.1319s
Batch 3900/3907, reward: 3.060, loss: -0.0209, took: 25.6974s
Mean epoch loss/reward: -0.0200, 3.0625, 3.0494, took: 1035.9949s (25.5891s / 100 batches)
Average tour length for uniform: 7.321032004799415
Average tour length for shifted: 3.049031386999087
Average tour length for adversary: 2.8763565079830844
