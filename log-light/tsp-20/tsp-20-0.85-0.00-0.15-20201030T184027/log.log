Current device: cuda
Batch 0/3907, reward: 11.865, loss: -690.7647, took: 0.3587s
Batch 100/3907, reward: 9.936, loss: -12.3003, took: 12.0638s
Batch 200/3907, reward: 7.554, loss: -1.1030, took: 11.8763s
Batch 300/3907, reward: 7.386, loss: -1.2041, took: 11.9125s
Batch 400/3907, reward: 7.326, loss: -1.5454, took: 11.7925s
Batch 500/3907, reward: 7.297, loss: -0.2326, took: 11.8618s
Batch 600/3907, reward: 7.292, loss: -0.1263, took: 11.7735s
Batch 700/3907, reward: 7.258, loss: 0.0479, took: 11.9770s
Batch 800/3907, reward: 7.267, loss: 0.3272, took: 11.8790s
Batch 900/3907, reward: 7.227, loss: 0.2130, took: 12.0625s
Batch 1000/3907, reward: 7.148, loss: -0.5421, took: 12.1022s
Batch 1100/3907, reward: 6.450, loss: 0.0978, took: 11.9684s
Batch 1200/3907, reward: 5.896, loss: -0.3030, took: 12.0594s
Batch 1300/3907, reward: 5.667, loss: -0.5234, took: 11.9188s
Batch 1400/3907, reward: 5.385, loss: -0.4322, took: 11.9862s
Batch 1500/3907, reward: 5.236, loss: -0.2863, took: 12.1691s
Batch 1600/3907, reward: 5.158, loss: -0.2073, took: 12.1158s
Batch 1700/3907, reward: 5.095, loss: -0.2122, took: 12.0923s
Batch 1800/3907, reward: 5.046, loss: 0.0176, took: 12.0989s
Batch 1900/3907, reward: 5.009, loss: -0.3329, took: 12.0449s
Batch 2000/3907, reward: 4.962, loss: -0.3311, took: 12.3763s
Batch 2100/3907, reward: 4.924, loss: -0.2222, took: 12.0922s
Batch 2200/3907, reward: 4.889, loss: -0.4142, took: 12.1956s
Batch 2300/3907, reward: 4.843, loss: -0.1738, took: 12.2368s
Batch 2400/3907, reward: 4.797, loss: -0.2127, took: 12.1050s
Batch 2500/3907, reward: 4.754, loss: -0.1119, took: 12.2852s
Batch 2600/3907, reward: 4.721, loss: -0.0825, took: 12.3283s
Batch 2700/3907, reward: 4.688, loss: -0.2047, took: 12.3528s
Batch 2800/3907, reward: 4.655, loss: -0.3286, took: 12.2457s
Batch 2900/3907, reward: 4.635, loss: -0.2804, took: 12.2979s
Batch 3000/3907, reward: 4.610, loss: -0.0200, took: 12.0341s
Batch 3100/3907, reward: 4.586, loss: -0.2526, took: 12.3880s
Batch 3200/3907, reward: 4.560, loss: -0.0718, took: 12.1562s
Batch 3300/3907, reward: 4.548, loss: -0.2046, took: 12.0422s
Batch 3400/3907, reward: 4.527, loss: -0.2223, took: 12.2118s
Batch 3500/3907, reward: 4.515, loss: -0.1568, took: 12.0340s
Batch 3600/3907, reward: 4.511, loss: -0.0067, took: 11.9992s
Batch 3700/3907, reward: 4.499, loss: -0.1324, took: 12.0640s
Batch 3800/3907, reward: 4.495, loss: 0.0806, took: 12.0584s
Batch 3900/3907, reward: 4.483, loss: -0.0124, took: 11.9742s
Mean epoch loss/reward: -0.7401, 5.5855, 4.3892, took: 481.2107s (11.7898s / 100 batches)
Batch 0/3907, reward: 4.468, loss: -0.5809, took: 0.3132s
Batch 100/3907, reward: 4.473, loss: -0.2976, took: 12.4942s
Batch 200/3907, reward: 4.467, loss: -0.2983, took: 12.3339s
Batch 300/3907, reward: 4.458, loss: -0.2288, took: 12.2845s
Batch 400/3907, reward: 4.449, loss: -0.1994, took: 12.4521s
Batch 500/3907, reward: 4.441, loss: -0.0863, took: 12.6005s
Batch 600/3907, reward: 4.439, loss: 0.0272, took: 17.9397s
Batch 700/3907, reward: 4.450, loss: -0.0758, took: 20.5583s
Batch 800/3907, reward: 4.436, loss: -0.1248, took: 20.8568s
Batch 900/3907, reward: 4.424, loss: 0.0260, took: 20.3213s
Batch 1000/3907, reward: 4.419, loss: -0.0919, took: 20.5051s
Batch 1100/3907, reward: 4.416, loss: -0.1109, took: 20.2956s
Batch 1200/3907, reward: 4.414, loss: -0.1573, took: 20.2461s
Batch 1300/3907, reward: 4.415, loss: -0.0970, took: 20.4256s
Batch 1400/3907, reward: 4.409, loss: -0.1397, took: 20.6418s
Batch 1500/3907, reward: 4.403, loss: -0.1080, took: 20.6851s
Batch 1600/3907, reward: 4.401, loss: -0.0906, took: 20.6750s
Batch 1700/3907, reward: 4.397, loss: -0.1240, took: 20.4239s
Batch 1800/3907, reward: 4.396, loss: -0.0399, took: 20.5137s
Batch 1900/3907, reward: 4.398, loss: -0.0847, took: 20.4808s
Batch 2000/3907, reward: 4.405, loss: -0.1215, took: 20.5169s
Batch 2100/3907, reward: 4.397, loss: -0.0876, took: 20.5390s
Batch 2200/3907, reward: 4.387, loss: -0.1680, took: 20.8470s
Batch 2300/3907, reward: 4.383, loss: -0.1331, took: 20.7976s
Batch 2400/3907, reward: 4.384, loss: -0.0896, took: 20.4114s
Batch 2500/3907, reward: 4.382, loss: -0.0827, took: 20.3729s
Batch 2600/3907, reward: 4.379, loss: -0.1282, took: 20.2265s
Batch 2700/3907, reward: 4.382, loss: -0.0897, took: 20.8625s
Batch 2800/3907, reward: 4.384, loss: -0.0896, took: 20.4595s
Batch 2900/3907, reward: 4.381, loss: -0.0873, took: 20.8391s
Batch 3000/3907, reward: 4.373, loss: -0.0039, took: 26.9603s
Batch 3100/3907, reward: 4.372, loss: -0.0899, took: 29.2158s
Batch 3200/3907, reward: 4.369, loss: -0.0695, took: 29.2032s
Batch 3300/3907, reward: 4.368, loss: -0.0634, took: 29.8511s
Batch 3400/3907, reward: 4.372, loss: -0.0452, took: 29.2866s
Batch 3500/3907, reward: 4.375, loss: -0.0913, took: 29.3359s
Batch 3600/3907, reward: 4.366, loss: 0.0402, took: 29.4668s
Batch 3700/3907, reward: 4.374, loss: -0.0587, took: 36.2725s
Batch 3800/3907, reward: 4.364, loss: -0.0747, took: 38.7602s
Batch 3900/3907, reward: 4.359, loss: -0.1401, took: 38.9699s
Mean epoch loss/reward: -0.1020, 4.4015, 4.3808, took: 881.8285s (21.7560s / 100 batches)
Batch 0/3907, reward: 4.416, loss: -0.8042, took: 0.5296s
Batch 100/3907, reward: 4.361, loss: -0.0887, took: 39.1433s
Batch 200/3907, reward: 4.358, loss: -0.0007, took: 38.6496s
Batch 300/3907, reward: 4.358, loss: -0.0248, took: 38.7222s
Batch 400/3907, reward: 4.356, loss: -0.0286, took: 38.6041s
Batch 500/3907, reward: 4.359, loss: -0.0531, took: 38.6654s
Batch 600/3907, reward: 4.361, loss: -0.0760, took: 36.3905s
Batch 700/3907, reward: 4.351, loss: -0.0502, took: 38.6117s
Batch 800/3907, reward: 4.367, loss: -0.0163, took: 38.6806s
Batch 900/3907, reward: 4.357, loss: -0.0235, took: 38.7643s
Batch 1000/3907, reward: 4.353, loss: -0.0733, took: 38.5204s
Batch 1100/3907, reward: 4.348, loss: -0.0737, took: 38.7249s
Batch 1200/3907, reward: 4.352, loss: -0.0194, took: 38.6185s
Batch 1300/3907, reward: 4.348, loss: -0.0111, took: 38.6066s
Batch 1400/3907, reward: 4.358, loss: -0.0148, took: 38.8313s
Batch 1500/3907, reward: 4.355, loss: -0.0207, took: 38.9298s
Batch 1600/3907, reward: 4.356, loss: -0.0665, took: 38.8762s
Batch 1700/3907, reward: 4.351, loss: -0.0562, took: 38.7738s
Batch 1800/3907, reward: 4.349, loss: -0.0108, took: 38.6637s
Batch 1900/3907, reward: 4.347, loss: -0.0017, took: 38.8391s
Batch 2000/3907, reward: 4.346, loss: -0.0593, took: 38.8180s
Batch 2100/3907, reward: 4.349, loss: -0.0307, took: 38.5943s
Batch 2200/3907, reward: 4.345, loss: -0.0793, took: 38.6664s
Batch 2300/3907, reward: 4.342, loss: -0.0715, took: 38.6622s
Batch 2400/3907, reward: 4.345, loss: -0.0353, took: 38.4981s
Batch 2500/3907, reward: 4.344, loss: -0.0113, took: 38.6082s
Batch 2600/3907, reward: 4.343, loss: -0.0665, took: 38.5537s
Batch 2700/3907, reward: 4.338, loss: -0.0263, took: 38.5436s
Batch 2800/3907, reward: 4.343, loss: -0.0272, took: 38.6346s
Batch 2900/3907, reward: 4.345, loss: -0.0103, took: 38.0127s
Batch 3000/3907, reward: 4.351, loss: -0.0185, took: 36.4100s
Batch 3100/3907, reward: 4.341, loss: 0.0079, took: 38.6340s
Batch 3200/3907, reward: 4.344, loss: 0.0019, took: 38.7575s
Batch 3300/3907, reward: 4.345, loss: 0.0381, took: 38.7090s
Batch 3400/3907, reward: 4.342, loss: -0.0448, took: 38.8517s
Batch 3500/3907, reward: 4.341, loss: -0.0663, took: 38.6184s
Batch 3600/3907, reward: 4.334, loss: -0.0164, took: 38.1282s
Batch 3700/3907, reward: 4.356, loss: 0.0012, took: 36.6768s
Batch 3800/3907, reward: 4.347, loss: 0.0296, took: 38.8171s
Batch 3900/3907, reward: 4.339, loss: -0.0337, took: 38.8832s
Mean epoch loss/reward: -0.0319, 4.3493, 4.3061, took: 1513.7263s (37.5556s / 100 batches)
Batch 0/3907, reward: 4.324, loss: 0.5716, took: 0.5284s
Batch 100/3907, reward: 4.336, loss: -0.0083, took: 38.8111s
Batch 200/3907, reward: 4.336, loss: 0.0041, took: 38.7170s
Batch 300/3907, reward: 4.336, loss: -0.0442, took: 38.7388s
Batch 400/3907, reward: 4.337, loss: -0.0311, took: 38.7823s
Batch 500/3907, reward: 4.349, loss: 0.0544, took: 38.4918s
Batch 600/3907, reward: 4.342, loss: -0.0259, took: 35.9415s
Batch 700/3907, reward: 4.345, loss: 0.0332, took: 38.6757s
Batch 800/3907, reward: 4.342, loss: -0.0280, took: 38.6388s
Batch 900/3907, reward: 4.344, loss: -0.0527, took: 38.6461s
Batch 1000/3907, reward: 4.334, loss: 0.0123, took: 38.7889s
Batch 1100/3907, reward: 4.336, loss: -0.0339, took: 38.7200s
Batch 1200/3907, reward: 4.332, loss: -0.0367, took: 38.6224s
Batch 1300/3907, reward: 4.330, loss: -0.0059, took: 38.7374s
Batch 1400/3907, reward: 4.335, loss: 0.0223, took: 38.6859s
Batch 1500/3907, reward: 4.326, loss: -0.0080, took: 38.8493s
Batch 1600/3907, reward: 4.335, loss: -0.0328, took: 38.6687s
Batch 1700/3907, reward: 4.327, loss: -0.0483, took: 38.7407s
Batch 1800/3907, reward: 4.337, loss: -0.0081, took: 38.7072s
Batch 1900/3907, reward: 4.328, loss: -0.0049, took: 38.6387s
Batch 2000/3907, reward: 4.330, loss: -0.0221, took: 38.6404s
Batch 2100/3907, reward: 4.330, loss: -0.0063, took: 38.7086s
Batch 2200/3907, reward: 4.327, loss: -0.0375, took: 38.4559s
Batch 2300/3907, reward: 4.331, loss: 0.0085, took: 38.5467s
Batch 2400/3907, reward: 4.327, loss: -0.0322, took: 38.5426s
Batch 2500/3907, reward: 4.330, loss: 0.0075, took: 38.5074s
Batch 2600/3907, reward: 4.332, loss: 0.0431, took: 38.7024s
Batch 2700/3907, reward: 4.329, loss: -0.0197, took: 38.6674s
Batch 2800/3907, reward: 4.332, loss: -0.0473, took: 38.7391s
Batch 2900/3907, reward: 4.334, loss: 0.0035, took: 37.9833s
Batch 3000/3907, reward: 4.325, loss: -0.0498, took: 36.6031s
Batch 3100/3907, reward: 4.330, loss: -0.0106, took: 38.5456s
Batch 3200/3907, reward: 4.329, loss: -0.0455, took: 38.4908s
Batch 3300/3907, reward: 4.330, loss: -0.0001, took: 38.5566s
Batch 3400/3907, reward: 4.333, loss: -0.0070, took: 38.6062s
Batch 3500/3907, reward: 4.337, loss: 0.0021, took: 38.5922s
Batch 3600/3907, reward: 4.335, loss: 0.0269, took: 38.0317s
Batch 3700/3907, reward: 4.322, loss: -0.0000, took: 36.5036s
Batch 3800/3907, reward: 4.332, loss: 0.0271, took: 38.5695s
Batch 3900/3907, reward: 4.327, loss: -0.0165, took: 38.5166s
Mean epoch loss/reward: -0.0105, 4.3330, 4.3080, took: 1511.2518s (37.4910s / 100 batches)
Batch 0/3907, reward: 4.388, loss: 0.1261, took: 0.5698s
Batch 100/3907, reward: 4.328, loss: 0.0340, took: 38.7751s
Batch 200/3907, reward: 4.323, loss: -0.0141, took: 38.6184s
Batch 300/3907, reward: 4.321, loss: 0.0441, took: 38.6455s
Batch 400/3907, reward: 4.318, loss: -0.0303, took: 38.5566s
Batch 500/3907, reward: 4.323, loss: 0.0512, took: 38.3005s
Batch 600/3907, reward: 4.323, loss: 0.0052, took: 36.0692s
Batch 700/3907, reward: 4.338, loss: -0.0004, took: 38.6108s
Batch 800/3907, reward: 4.332, loss: -0.0057, took: 38.4532s
Batch 900/3907, reward: 4.327, loss: 0.0093, took: 38.6230s
Batch 1000/3907, reward: 4.341, loss: -0.0566, took: 38.5261s
Batch 1100/3907, reward: 4.329, loss: -0.0306, took: 38.4812s
Batch 1200/3907, reward: 4.322, loss: 0.0200, took: 38.4827s
Batch 1300/3907, reward: 4.329, loss: -0.0237, took: 38.4743s
Batch 1400/3907, reward: 4.325, loss: -0.0017, took: 38.4368s
Batch 1500/3907, reward: 4.325, loss: -0.0205, took: 38.2483s
Batch 1600/3907, reward: 4.328, loss: 0.0383, took: 38.2793s
Batch 1700/3907, reward: 4.324, loss: -0.0049, took: 38.5620s
Batch 1800/3907, reward: 4.318, loss: -0.0308, took: 38.3170s
Batch 1900/3907, reward: 4.339, loss: -0.0413, took: 38.4531s
Batch 2000/3907, reward: 4.318, loss: 0.0108, took: 38.5692s
Batch 2100/3907, reward: 4.319, loss: 0.0407, took: 38.3722s
Batch 2200/3907, reward: 4.321, loss: 0.0194, took: 38.3919s
Batch 2300/3907, reward: 4.319, loss: -0.0211, took: 38.5475s
Batch 2400/3907, reward: 4.318, loss: -0.0013, took: 38.4503s
Batch 2500/3907, reward: 4.321, loss: 0.0292, took: 38.5180s
Batch 2600/3907, reward: 4.324, loss: 0.0100, took: 38.1758s
Batch 2700/3907, reward: 4.319, loss: -0.0079, took: 38.1341s
Batch 2800/3907, reward: 4.324, loss: -0.0033, took: 38.3267s
Batch 2900/3907, reward: 4.316, loss: -0.0140, took: 37.0196s
Batch 3000/3907, reward: 4.319, loss: 0.0024, took: 36.0694s
Batch 3100/3907, reward: 4.315, loss: 0.0206, took: 38.1255s
Batch 3200/3907, reward: 4.317, loss: 0.0034, took: 38.2273s
Batch 3300/3907, reward: 4.319, loss: 0.0172, took: 38.0994s
Batch 3400/3907, reward: 4.322, loss: -0.0170, took: 38.1508s
Batch 3500/3907, reward: 4.311, loss: 0.0418, took: 38.0128s
Batch 3600/3907, reward: 4.318, loss: -0.0064, took: 37.5070s
Batch 3700/3907, reward: 4.325, loss: -0.0195, took: 35.7371s
Batch 3800/3907, reward: 4.322, loss: 0.0121, took: 38.2439s
Batch 3900/3907, reward: 4.313, loss: 0.0242, took: 38.0706s
Mean epoch loss/reward: 0.0021, 4.3228, 4.2945, took: 1499.7821s (37.2058s / 100 batches)
Batch 0/3907, reward: 4.314, loss: -0.3550, took: 0.5116s
Batch 100/3907, reward: 4.316, loss: 0.0038, took: 38.2991s
Batch 200/3907, reward: 4.321, loss: 0.0362, took: 38.0784s
Batch 300/3907, reward: 4.320, loss: -0.0097, took: 38.0920s
Batch 400/3907, reward: 4.323, loss: -0.0132, took: 38.3083s
Batch 500/3907, reward: 4.312, loss: 0.0593, took: 38.0865s
Batch 600/3907, reward: 4.317, loss: 0.0027, took: 35.3381s
Batch 700/3907, reward: 4.318, loss: 0.0014, took: 38.1804s
Batch 800/3907, reward: 4.314, loss: 0.0057, took: 38.2151s
Batch 900/3907, reward: 4.314, loss: -0.0114, took: 38.1819s
Batch 1000/3907, reward: 4.313, loss: -0.0038, took: 38.2047s
Batch 1100/3907, reward: 4.311, loss: -0.0091, took: 38.1598s
Batch 1200/3907, reward: 4.314, loss: -0.0190, took: 38.2623s
Batch 1300/3907, reward: 4.314, loss: 0.0147, took: 38.1214s
Batch 1400/3907, reward: 4.316, loss: 0.0282, took: 38.2540s
Batch 1500/3907, reward: 4.327, loss: -0.0135, took: 38.3303s
Batch 1600/3907, reward: 4.310, loss: -0.0194, took: 38.2509s
Batch 1700/3907, reward: 4.319, loss: -0.0010, took: 38.3974s
Batch 1800/3907, reward: 4.315, loss: -0.0149, took: 38.3328s
Batch 1900/3907, reward: 4.320, loss: -0.0174, took: 38.2176s
Batch 2000/3907, reward: 4.315, loss: 0.0140, took: 38.1310s
Batch 2100/3907, reward: 4.315, loss: 0.0097, took: 38.1550s
Batch 2200/3907, reward: 4.323, loss: -0.0026, took: 38.0751s
Batch 2300/3907, reward: 4.320, loss: 0.0146, took: 38.0205s
Batch 2400/3907, reward: 4.317, loss: 0.0310, took: 38.1192s
Batch 2500/3907, reward: 4.317, loss: 0.0295, took: 38.1022s
Batch 2600/3907, reward: 4.314, loss: 0.0041, took: 38.1470s
Batch 2700/3907, reward: 4.321, loss: 0.0066, took: 38.0295s
Batch 2800/3907, reward: 4.310, loss: 0.0385, took: 38.0826s
Batch 2900/3907, reward: 4.314, loss: 0.0200, took: 37.5729s
Batch 3000/3907, reward: 4.309, loss: 0.0172, took: 36.0128s
Batch 3100/3907, reward: 4.314, loss: -0.0012, took: 38.3644s
Batch 3200/3907, reward: 4.314, loss: 0.0011, took: 38.3724s
Batch 3300/3907, reward: 4.306, loss: 0.0065, took: 38.2465s
Batch 3400/3907, reward: 4.316, loss: -0.0006, took: 38.3074s
Batch 3500/3907, reward: 4.313, loss: 0.0224, took: 38.4585s
Batch 3600/3907, reward: 4.308, loss: -0.0017, took: 37.8604s
Batch 3700/3907, reward: 4.309, loss: 0.0086, took: 36.2013s
Batch 3800/3907, reward: 4.314, loss: 0.0033, took: 38.1561s
Batch 3900/3907, reward: 4.324, loss: 0.0082, took: 38.3197s
Mean epoch loss/reward: 0.0067, 4.3155, 4.3040, took: 1494.0596s (37.0639s / 100 batches)
Batch 0/3907, reward: 4.335, loss: 0.2242, took: 0.5643s
Batch 100/3907, reward: 4.312, loss: -0.0007, took: 38.3129s
Batch 200/3907, reward: 4.308, loss: 0.0160, took: 38.1941s
Batch 300/3907, reward: 4.320, loss: 0.0122, took: 38.0983s
Batch 400/3907, reward: 4.315, loss: 0.0100, took: 38.1329s
Batch 500/3907, reward: 4.310, loss: 0.0170, took: 38.1823s
Batch 600/3907, reward: 4.312, loss: -0.0013, took: 35.1539s
Batch 700/3907, reward: 4.311, loss: 0.0283, took: 37.9872s
Batch 800/3907, reward: 4.302, loss: 0.0190, took: 38.0273s
Batch 900/3907, reward: 4.310, loss: 0.0024, took: 38.0710s
Batch 1000/3907, reward: 4.326, loss: 0.0014, took: 38.2850s
Batch 1100/3907, reward: 4.310, loss: -0.0031, took: 38.2346s
Batch 1200/3907, reward: 4.307, loss: 0.0069, took: 38.2777s
Batch 1300/3907, reward: 4.311, loss: -0.0018, took: 38.2797s
Batch 1400/3907, reward: 4.315, loss: 0.0136, took: 38.3023s
Batch 1500/3907, reward: 4.325, loss: 0.0220, took: 38.1163s
Batch 1600/3907, reward: 4.310, loss: -0.0046, took: 38.1968s
Batch 1700/3907, reward: 4.309, loss: 0.0078, took: 38.2575s
Batch 1800/3907, reward: 4.309, loss: -0.0038, took: 38.1939s
Batch 1900/3907, reward: 4.306, loss: 0.0048, took: 38.0648s
Batch 2000/3907, reward: 4.308, loss: 0.0036, took: 38.0918s
Batch 2100/3907, reward: 4.315, loss: 0.0340, took: 37.9524s
Batch 2200/3907, reward: 4.309, loss: -0.0000, took: 37.9951s
Batch 2300/3907, reward: 4.305, loss: 0.0263, took: 38.0150s
Batch 2400/3907, reward: 4.311, loss: -0.0173, took: 38.0915s
Batch 2500/3907, reward: 4.304, loss: -0.0072, took: 38.1798s
Batch 2600/3907, reward: 4.306, loss: 0.0198, took: 38.0355s
Batch 2700/3907, reward: 4.311, loss: -0.0011, took: 38.2771s
Batch 2800/3907, reward: 4.305, loss: 0.0507, took: 38.2200s
Batch 2900/3907, reward: 4.312, loss: 0.0057, took: 37.6079s
Batch 3000/3907, reward: 4.302, loss: 0.0221, took: 35.6198s
Batch 3100/3907, reward: 4.301, loss: 0.0262, took: 38.2868s
Batch 3200/3907, reward: 4.303, loss: 0.0077, took: 38.1536s
Batch 3300/3907, reward: 4.312, loss: 0.0375, took: 38.0487s
Batch 3400/3907, reward: 4.306, loss: 0.0082, took: 38.2534s
Batch 3500/3907, reward: 4.309, loss: 0.0164, took: 38.1545s
Batch 3600/3907, reward: 4.309, loss: -0.0137, took: 37.3230s
Batch 3700/3907, reward: 4.306, loss: 0.0208, took: 36.2238s
Batch 3800/3907, reward: 4.305, loss: 0.0139, took: 37.9837s
Batch 3900/3907, reward: 4.304, loss: 0.0187, took: 38.0103s
Mean epoch loss/reward: 0.0108, 4.3094, 4.3032, took: 1490.7631s (36.9864s / 100 batches)
Batch 0/3907, reward: 4.319, loss: 0.1172, took: 0.5189s
Batch 100/3907, reward: 4.311, loss: 0.0053, took: 38.4219s
Batch 200/3907, reward: 4.300, loss: 0.0063, took: 38.1207s
Batch 300/3907, reward: 4.314, loss: 0.0579, took: 38.1336s
Batch 400/3907, reward: 4.306, loss: 0.0131, took: 38.3270s
Batch 500/3907, reward: 4.311, loss: 0.0175, took: 38.1176s
Batch 600/3907, reward: 4.311, loss: 0.0162, took: 35.2550s
Batch 700/3907, reward: 4.304, loss: 0.0193, took: 38.1537s
Batch 800/3907, reward: 4.317, loss: 0.0124, took: 38.0511s
Batch 900/3907, reward: 4.310, loss: -0.0041, took: 38.1440s
Batch 1000/3907, reward: 4.304, loss: 0.0219, took: 38.2380s
Batch 1100/3907, reward: 4.307, loss: -0.0020, took: 38.0810s
Batch 1200/3907, reward: 4.307, loss: 0.0223, took: 38.2630s
Batch 1300/3907, reward: 4.311, loss: 0.0196, took: 38.0806s
Batch 1400/3907, reward: 4.301, loss: 0.0114, took: 38.0144s
Batch 1500/3907, reward: 4.304, loss: 0.0154, took: 38.1976s
Batch 1600/3907, reward: 4.313, loss: 0.0113, took: 37.9946s
Batch 1700/3907, reward: 4.307, loss: 0.0105, took: 38.0837s
Batch 1800/3907, reward: 4.301, loss: 0.0308, took: 38.1793s
Batch 1900/3907, reward: 4.312, loss: 0.0095, took: 38.1747s
Batch 2000/3907, reward: 4.308, loss: 0.0438, took: 38.1597s
Batch 2100/3907, reward: 4.304, loss: 0.0061, took: 38.2499s
Batch 2200/3907, reward: 4.305, loss: 0.0208, took: 38.2448s
Batch 2300/3907, reward: 4.315, loss: 0.0164, took: 38.3118s
Batch 2400/3907, reward: 4.306, loss: 0.0135, took: 38.2319s
Batch 2500/3907, reward: 4.308, loss: 0.0216, took: 38.2256s
Batch 2600/3907, reward: 4.300, loss: 0.0043, took: 38.2997s
Batch 2700/3907, reward: 4.303, loss: 0.0184, took: 38.0955s
Batch 2800/3907, reward: 4.302, loss: 0.0240, took: 38.3797s
Batch 2900/3907, reward: 4.314, loss: 0.0196, took: 37.5301s
Batch 3000/3907, reward: 4.301, loss: 0.0231, took: 36.1264s
Batch 3100/3907, reward: 4.304, loss: 0.0477, took: 38.0383s
Batch 3200/3907, reward: 4.304, loss: 0.0204, took: 38.1809s
Batch 3300/3907, reward: 4.304, loss: 0.0062, took: 38.0277s
Batch 3400/3907, reward: 4.305, loss: 0.0216, took: 38.1046s
Batch 3500/3907, reward: 4.300, loss: 0.0185, took: 38.0974s
Batch 3600/3907, reward: 4.302, loss: 0.0327, took: 37.0498s
Batch 3700/3907, reward: 4.311, loss: 0.0078, took: 36.6536s
Batch 3800/3907, reward: 4.301, loss: 0.0036, took: 38.1322s
Batch 3900/3907, reward: 4.311, loss: 0.0393, took: 38.1702s
Mean epoch loss/reward: 0.0180, 4.3067, 4.2930, took: 1492.4914s (37.0215s / 100 batches)
Batch 0/3907, reward: 4.280, loss: 0.0345, took: 0.5142s
Batch 100/3907, reward: 4.307, loss: -0.0044, took: 38.4783s
Batch 200/3907, reward: 4.311, loss: -0.0110, took: 38.1357s
Batch 300/3907, reward: 4.308, loss: 0.0149, took: 38.2624s
Batch 400/3907, reward: 4.309, loss: 0.0204, took: 38.1172s
Batch 500/3907, reward: 4.303, loss: 0.0050, took: 38.0121s
Batch 600/3907, reward: 4.298, loss: 0.0004, took: 35.1559s
Batch 700/3907, reward: 4.299, loss: 0.0249, took: 38.1289s
Batch 800/3907, reward: 4.303, loss: -0.0029, took: 37.9986s
Batch 900/3907, reward: 4.303, loss: 0.0152, took: 37.9882s
Batch 1000/3907, reward: 4.314, loss: 0.0278, took: 38.0439s
Batch 1100/3907, reward: 4.305, loss: 0.0320, took: 38.0759s
Batch 1200/3907, reward: 4.302, loss: 0.0306, took: 37.9924s
Batch 1300/3907, reward: 4.295, loss: 0.0151, took: 38.0913s
Batch 1400/3907, reward: 4.304, loss: 0.0034, took: 38.0768s
Batch 1500/3907, reward: 4.298, loss: 0.0444, took: 37.9739s
Batch 1600/3907, reward: 4.298, loss: 0.0027, took: 38.0729s
Batch 1700/3907, reward: 4.299, loss: 0.0299, took: 38.1273s
Batch 1800/3907, reward: 4.307, loss: 0.0149, took: 38.1926s
Batch 1900/3907, reward: 4.299, loss: 0.0178, took: 38.2070s
Batch 2000/3907, reward: 4.301, loss: 0.0121, took: 38.5011s
Batch 2100/3907, reward: 4.304, loss: 0.0261, took: 38.2050s
Batch 2200/3907, reward: 4.316, loss: 0.0027, took: 38.2937s
Batch 2300/3907, reward: 4.308, loss: -0.0114, took: 38.2130s
Batch 2400/3907, reward: 4.302, loss: 0.0208, took: 38.1460s
Batch 2500/3907, reward: 4.303, loss: 0.0262, took: 38.2211s
Batch 2600/3907, reward: 4.301, loss: 0.0217, took: 38.0650s
Batch 2700/3907, reward: 4.305, loss: 0.0134, took: 38.0439s
Batch 2800/3907, reward: 4.303, loss: 0.0136, took: 38.2201s
Batch 2900/3907, reward: 4.300, loss: 0.0397, took: 36.8970s
Batch 3000/3907, reward: 4.296, loss: 0.0121, took: 35.9672s
Batch 3100/3907, reward: 4.297, loss: 0.0236, took: 37.9736s
Batch 3200/3907, reward: 4.296, loss: -0.0030, took: 37.9319s
Batch 3300/3907, reward: 4.304, loss: 0.0346, took: 38.1594s
Batch 3400/3907, reward: 4.301, loss: 0.0012, took: 37.9614s
Batch 3500/3907, reward: 4.296, loss: 0.0127, took: 38.1801s
Batch 3600/3907, reward: 4.305, loss: 0.0224, took: 36.7488s
Batch 3700/3907, reward: 4.297, loss: 0.0079, took: 37.0229s
Batch 3800/3907, reward: 4.300, loss: 0.0250, took: 38.1672s
Batch 3900/3907, reward: 4.297, loss: 0.0368, took: 38.2007s
Mean epoch loss/reward: 0.0159, 4.3025, 4.2989, took: 1490.3994s (36.9691s / 100 batches)
Batch 0/3907, reward: 4.261, loss: -0.1107, took: 0.5268s
Batch 100/3907, reward: 4.296, loss: 0.0281, took: 38.0882s
Batch 200/3907, reward: 4.297, loss: 0.0166, took: 38.1264s
Batch 300/3907, reward: 4.300, loss: 0.0190, took: 38.0156s
Batch 400/3907, reward: 4.291, loss: 0.0025, took: 38.2442s
Batch 500/3907, reward: 4.304, loss: 0.0135, took: 38.0056s
Batch 600/3907, reward: 4.313, loss: 0.0248, took: 35.0765s
Batch 700/3907, reward: 4.303, loss: -0.0042, took: 38.1630s
Batch 800/3907, reward: 4.298, loss: 0.0036, took: 38.0300s
Batch 900/3907, reward: 4.304, loss: 0.0106, took: 38.0670s
Batch 1000/3907, reward: 4.302, loss: 0.0347, took: 38.0328s
Batch 1100/3907, reward: 4.299, loss: 0.0163, took: 38.0381s
Batch 1200/3907, reward: 4.302, loss: 0.0031, took: 38.1683s
Batch 1300/3907, reward: 4.300, loss: 0.0083, took: 38.2642s
Batch 1400/3907, reward: 4.299, loss: 0.0121, took: 38.0756s
Batch 1500/3907, reward: 4.294, loss: 0.0098, took: 38.3188s
Batch 1600/3907, reward: 4.295, loss: 0.0220, took: 38.2698s
Batch 1700/3907, reward: 4.300, loss: 0.0395, took: 38.2033s
Batch 1800/3907, reward: 4.295, loss: -0.0091, took: 38.0177s
Batch 1900/3907, reward: 4.306, loss: 0.0131, took: 38.0031s
Batch 2000/3907, reward: 4.306, loss: 0.0022, took: 38.1828s
Batch 2100/3907, reward: 4.302, loss: 0.0492, took: 37.9440s
Batch 2200/3907, reward: 4.295, loss: 0.0078, took: 38.1030s
Batch 2300/3907, reward: 4.302, loss: 0.0276, took: 38.0363s
Batch 2400/3907, reward: 4.298, loss: 0.0134, took: 38.1086s
Batch 2500/3907, reward: 4.303, loss: 0.0340, took: 38.1608s
Batch 2600/3907, reward: 4.306, loss: 0.0191, took: 38.1719s
Batch 2700/3907, reward: 4.296, loss: 0.0186, took: 38.0455s
Batch 2800/3907, reward: 4.297, loss: 0.0072, took: 38.0510s
Batch 2900/3907, reward: 4.310, loss: 0.0209, took: 36.9984s
Batch 3000/3907, reward: 4.302, loss: 0.0017, took: 36.1099s
Batch 3100/3907, reward: 4.305, loss: 0.0284, took: 38.0282s
Batch 3200/3907, reward: 4.303, loss: 0.0503, took: 38.0952s
Batch 3300/3907, reward: 4.300, loss: 0.0129, took: 38.0170s
Batch 3400/3907, reward: 4.302, loss: 0.0226, took: 38.1293s
Batch 3500/3907, reward: 4.301, loss: 0.0102, took: 38.2939s
Batch 3600/3907, reward: 4.303, loss: -0.0025, took: 36.2653s
Batch 3700/3907, reward: 4.299, loss: 0.0197, took: 37.3098s
Batch 3800/3907, reward: 4.296, loss: 0.0208, took: 38.2362s
Batch 3900/3907, reward: 4.296, loss: 0.0256, took: 38.3214s
Mean epoch loss/reward: 0.0167, 4.3005, 4.3031, took: 1489.8093s (36.9586s / 100 batches)
Batch 0/3907, reward: 4.318, loss: 0.3225, took: 0.5253s
Batch 100/3907, reward: 4.292, loss: 0.0206, took: 38.4122s
Batch 200/3907, reward: 4.296, loss: 0.0305, took: 38.2251s
Batch 300/3907, reward: 4.295, loss: 0.0242, took: 38.2997s
Batch 400/3907, reward: 4.293, loss: 0.0084, took: 38.1535s
Batch 500/3907, reward: 4.298, loss: 0.0058, took: 38.0699s
Batch 600/3907, reward: 4.304, loss: 0.0569, took: 35.1094s
Batch 700/3907, reward: 4.300, loss: 0.0300, took: 38.1447s
Batch 800/3907, reward: 4.306, loss: 0.0494, took: 38.0915s
Batch 900/3907, reward: 4.300, loss: 0.0280, took: 38.0090s
Batch 1000/3907, reward: 4.294, loss: 0.0251, took: 38.2192s
Batch 1100/3907, reward: 4.298, loss: 0.0247, took: 38.0268s
Batch 1200/3907, reward: 4.296, loss: 0.0239, took: 38.0221s
Batch 1300/3907, reward: 4.293, loss: 0.0216, took: 38.1046s
Batch 1400/3907, reward: 4.293, loss: 0.0306, took: 38.2397s
Batch 1500/3907, reward: 4.296, loss: 0.0126, took: 38.1483s
Batch 1600/3907, reward: 4.294, loss: 0.0183, took: 37.9793s
Batch 1700/3907, reward: 4.296, loss: 0.0316, took: 38.2157s
Batch 1800/3907, reward: 4.297, loss: 0.0182, took: 38.1772s
Batch 1900/3907, reward: 4.297, loss: 0.0368, took: 38.1641s
Batch 2000/3907, reward: 4.295, loss: -0.0004, took: 38.1417s
Batch 2100/3907, reward: 4.298, loss: 0.0250, took: 38.1045s
Batch 2200/3907, reward: 4.296, loss: 0.0183, took: 38.0152s
Batch 2300/3907, reward: 4.295, loss: 0.0137, took: 38.1296s
Batch 2400/3907, reward: 4.294, loss: 0.0176, took: 38.0639s
Batch 2500/3907, reward: 4.300, loss: 0.0163, took: 38.0846s
Batch 2600/3907, reward: 4.301, loss: 0.0223, took: 37.8912s
Batch 2700/3907, reward: 4.294, loss: 0.0405, took: 37.9551s
Batch 2800/3907, reward: 4.292, loss: 0.0183, took: 37.9501s
Batch 2900/3907, reward: 4.293, loss: 0.0118, took: 36.9349s
Batch 3000/3907, reward: 4.296, loss: 0.0311, took: 36.0378s
Batch 3100/3907, reward: 4.305, loss: 0.0295, took: 38.2295s
Batch 3200/3907, reward: 4.304, loss: 0.0380, took: 38.1872s
Batch 3300/3907, reward: 4.299, loss: 0.0335, took: 38.2384s
Batch 3400/3907, reward: 4.304, loss: 0.0055, took: 38.2834s
Batch 3500/3907, reward: 4.301, loss: 0.0214, took: 38.2556s
Batch 3600/3907, reward: 4.297, loss: 0.0143, took: 36.2205s
Batch 3700/3907, reward: 4.307, loss: 0.0346, took: 37.3926s
Batch 3800/3907, reward: 4.301, loss: 0.0316, took: 38.0721s
Batch 3900/3907, reward: 4.304, loss: 0.0312, took: 38.0555s
Mean epoch loss/reward: 0.0245, 4.2978, 4.2824, took: 1490.2276s (36.9645s / 100 batches)
Batch 0/3907, reward: 4.299, loss: 0.2757, took: 0.5169s
Batch 100/3907, reward: 4.296, loss: 0.0167, took: 38.2298s
Batch 200/3907, reward: 4.291, loss: 0.0262, took: 38.0159s
Batch 300/3907, reward: 4.296, loss: 0.0171, took: 38.0541s
Batch 400/3907, reward: 4.303, loss: -0.0079, took: 38.1220s
Batch 500/3907, reward: 4.294, loss: 0.0229, took: 38.0308s
Batch 600/3907, reward: 4.294, loss: -0.0017, took: 35.2658s
Batch 700/3907, reward: 4.292, loss: 0.0140, took: 38.2281s
Batch 800/3907, reward: 4.295, loss: 0.0326, took: 38.1214s
Batch 900/3907, reward: 4.297, loss: 0.0298, took: 38.0646s
Batch 1000/3907, reward: 4.303, loss: 0.0527, took: 38.4728s
Batch 1100/3907, reward: 4.294, loss: 0.0402, took: 38.2223s
Batch 1200/3907, reward: 4.296, loss: 0.0286, took: 38.2312s
Batch 1300/3907, reward: 4.292, loss: 0.0179, took: 38.2292s
Batch 1400/3907, reward: 4.290, loss: 0.0177, took: 38.3656s
Batch 1500/3907, reward: 4.296, loss: 0.0210, took: 38.3774s
Batch 1600/3907, reward: 4.307, loss: 0.0129, took: 38.1358s
Batch 1700/3907, reward: 4.299, loss: 0.0221, took: 38.1197s
Batch 1800/3907, reward: 4.297, loss: 0.0298, took: 38.2755s
Batch 1900/3907, reward: 4.300, loss: 0.0361, took: 38.1296s
Batch 2000/3907, reward: 4.308, loss: 0.0287, took: 38.1155s
Batch 2100/3907, reward: 4.304, loss: -0.0014, took: 38.0369s
Batch 2200/3907, reward: 4.294, loss: 0.0246, took: 37.9399s
Batch 2300/3907, reward: 4.293, loss: 0.0154, took: 38.0383s
Batch 2400/3907, reward: 4.290, loss: 0.0179, took: 38.0167s
Batch 2500/3907, reward: 4.294, loss: 0.0291, took: 37.9262s
Batch 2600/3907, reward: 4.295, loss: 0.0059, took: 37.9343s
Batch 2700/3907, reward: 4.296, loss: 0.0416, took: 37.9028s
Batch 2800/3907, reward: 4.295, loss: 0.0099, took: 38.0063s
Batch 2900/3907, reward: 4.297, loss: 0.0182, took: 36.6695s
Batch 3000/3907, reward: 4.289, loss: 0.0084, took: 36.2819s
Batch 3100/3907, reward: 4.290, loss: 0.0190, took: 38.1056s
Batch 3200/3907, reward: 4.294, loss: 0.0110, took: 38.2441s
Batch 3300/3907, reward: 4.297, loss: 0.0279, took: 38.2570s
Batch 3400/3907, reward: 4.292, loss: 0.0118, took: 38.3509s
Batch 3500/3907, reward: 4.294, loss: 0.0205, took: 38.1789s
Batch 3600/3907, reward: 4.294, loss: 0.0172, took: 35.7391s
Batch 3700/3907, reward: 4.297, loss: 0.0053, took: 37.7399s
Batch 3800/3907, reward: 4.287, loss: 0.0233, took: 37.9504s
Batch 3900/3907, reward: 4.296, loss: 0.0214, took: 37.9586s
Mean epoch loss/reward: 0.0202, 4.2955, 4.2873, took: 1489.9177s (36.9650s / 100 batches)
Batch 0/3907, reward: 4.300, loss: 0.2420, took: 0.5175s
Batch 100/3907, reward: 4.292, loss: 0.0222, took: 38.1010s
Batch 200/3907, reward: 4.291, loss: 0.0239, took: 38.0700s
Batch 300/3907, reward: 4.295, loss: 0.0216, took: 38.0061s
Batch 400/3907, reward: 4.299, loss: 0.0317, took: 37.9745s
Batch 500/3907, reward: 4.296, loss: 0.0350, took: 37.8274s
Batch 600/3907, reward: 4.301, loss: -0.0030, took: 35.3044s
Batch 700/3907, reward: 4.312, loss: 0.0312, took: 37.9346s
Batch 800/3907, reward: 4.295, loss: 0.0120, took: 37.9236s
Batch 900/3907, reward: 4.294, loss: 0.0139, took: 37.9072s
Batch 1000/3907, reward: 4.297, loss: 0.0180, took: 38.0986s
Batch 1100/3907, reward: 4.292, loss: 0.0196, took: 37.8889s
Batch 1200/3907, reward: 4.295, loss: 0.0271, took: 38.1478s
Batch 1300/3907, reward: 4.303, loss: -0.0033, took: 38.0845s
Batch 1400/3907, reward: 4.290, loss: 0.0176, took: 38.1590s
Batch 1500/3907, reward: 4.303, loss: 0.0278, took: 37.8993s
Batch 1600/3907, reward: 4.297, loss: 0.0166, took: 37.8834s
Batch 1700/3907, reward: 4.296, loss: 0.0167, took: 37.9967s
Batch 1800/3907, reward: 4.292, loss: 0.0250, took: 38.0358s
Batch 1900/3907, reward: 4.291, loss: 0.0360, took: 38.0179s
Batch 2000/3907, reward: 4.293, loss: 0.0206, took: 37.9688s
Batch 2100/3907, reward: 4.290, loss: 0.0146, took: 37.8892s
Batch 2200/3907, reward: 4.292, loss: 0.0344, took: 37.9213s
Batch 2300/3907, reward: 4.297, loss: 0.0280, took: 37.9191s
Batch 2400/3907, reward: 4.293, loss: 0.0196, took: 38.0903s
Batch 2500/3907, reward: 4.284, loss: 0.0123, took: 38.0306s
Batch 2600/3907, reward: 4.293, loss: 0.0292, took: 37.9505s
Batch 2700/3907, reward: 4.296, loss: 0.0294, took: 38.0153s
Batch 2800/3907, reward: 4.303, loss: 0.0426, took: 38.0842s
Batch 2900/3907, reward: 4.288, loss: 0.0213, took: 36.8601s
Batch 3000/3907, reward: 4.295, loss: 0.0071, took: 36.2175s
Batch 3100/3907, reward: 4.293, loss: 0.0479, took: 38.0494s
Batch 3200/3907, reward: 4.297, loss: 0.0106, took: 38.0545s
Batch 3300/3907, reward: 4.284, loss: -0.0095, took: 38.0547s
Batch 3400/3907, reward: 4.293, loss: 0.0267, took: 38.1088s
Batch 3500/3907, reward: 4.296, loss: 0.0311, took: 37.9906s
Batch 3600/3907, reward: 4.300, loss: 0.0313, took: 35.6017s
Batch 3700/3907, reward: 4.292, loss: 0.0053, took: 37.6064s
Batch 3800/3907, reward: 4.298, loss: 0.0161, took: 37.9737s
Batch 3900/3907, reward: 4.296, loss: 0.0354, took: 37.9616s
Mean epoch loss/reward: 0.0218, 4.2949, 4.2926, took: 1485.5837s (36.8532s / 100 batches)
Batch 0/3907, reward: 4.321, loss: 0.0087, took: 0.5080s
Batch 100/3907, reward: 4.303, loss: 0.0304, took: 38.0368s
Batch 200/3907, reward: 4.292, loss: -0.0012, took: 38.0478s
Batch 300/3907, reward: 4.294, loss: 0.0143, took: 38.0357s
Batch 400/3907, reward: 4.290, loss: 0.0241, took: 38.0592s
Batch 500/3907, reward: 4.294, loss: 0.0163, took: 37.9645s
Batch 600/3907, reward: 4.293, loss: 0.0034, took: 35.0512s
Batch 700/3907, reward: 4.297, loss: 0.0258, took: 38.0034s
Batch 800/3907, reward: 4.289, loss: 0.0183, took: 38.0974s
Batch 900/3907, reward: 4.292, loss: 0.0045, took: 38.0698s
Batch 1000/3907, reward: 4.290, loss: 0.0275, took: 38.1350s
Batch 1100/3907, reward: 4.296, loss: 0.0268, took: 38.0369s
Batch 1200/3907, reward: 4.290, loss: 0.0362, took: 38.1536s
Batch 1300/3907, reward: 4.293, loss: 0.0321, took: 37.9830s
Batch 1400/3907, reward: 4.290, loss: 0.0386, took: 37.9426s
Batch 1500/3907, reward: 4.292, loss: 0.0152, took: 38.1327s
Batch 1600/3907, reward: 4.294, loss: 0.0197, took: 37.8762s
Batch 1700/3907, reward: 4.297, loss: 0.0203, took: 37.8966s
Batch 1800/3907, reward: 4.293, loss: 0.0154, took: 37.9780s
Batch 1900/3907, reward: 4.289, loss: 0.0186, took: 37.9334s
Batch 2000/3907, reward: 4.291, loss: 0.0219, took: 37.9886s
Batch 2100/3907, reward: 4.296, loss: 0.0205, took: 37.8977s
Batch 2200/3907, reward: 4.296, loss: 0.0230, took: 37.8966s
Batch 2300/3907, reward: 4.294, loss: 0.0288, took: 38.0626s
Batch 2400/3907, reward: 4.297, loss: 0.0155, took: 37.8625s
Batch 2500/3907, reward: 4.296, loss: 0.0072, took: 37.9405s
Batch 2600/3907, reward: 4.290, loss: 0.0281, took: 38.1716s
Batch 2700/3907, reward: 4.296, loss: 0.0282, took: 37.9454s
Batch 2800/3907, reward: 4.298, loss: 0.0435, took: 38.0240s
Batch 2900/3907, reward: 4.294, loss: 0.0367, took: 37.2096s
Batch 3000/3907, reward: 4.294, loss: 0.0246, took: 35.9941s
Batch 3100/3907, reward: 4.292, loss: 0.0205, took: 37.8822s
Batch 3200/3907, reward: 4.293, loss: 0.0228, took: 38.0426s
Batch 3300/3907, reward: 4.298, loss: 0.0473, took: 37.8530s
Batch 3400/3907, reward: 4.296, loss: 0.0002, took: 37.9276s
Batch 3500/3907, reward: 4.293, loss: 0.0229, took: 37.9987s
Batch 3600/3907, reward: 4.295, loss: 0.0220, took: 35.2818s
Batch 3700/3907, reward: 4.290, loss: 0.0272, took: 38.0851s
Batch 3800/3907, reward: 4.287, loss: 0.0286, took: 37.9825s
Batch 3900/3907, reward: 4.291, loss: 0.0037, took: 37.8840s
Mean epoch loss/reward: 0.0220, 4.2935, 4.2876, took: 1485.5195s (36.8468s / 100 batches)
Batch 0/3907, reward: 4.277, loss: -0.0320, took: 0.5271s
Batch 100/3907, reward: 4.285, loss: 0.0200, took: 37.9817s
Batch 200/3907, reward: 4.291, loss: 0.0094, took: 37.9227s
Batch 300/3907, reward: 4.294, loss: 0.0269, took: 38.0228s
Batch 400/3907, reward: 4.296, loss: 0.0291, took: 37.9802s
Batch 500/3907, reward: 4.291, loss: 0.0429, took: 37.7911s
Batch 600/3907, reward: 4.290, loss: 0.0317, took: 35.3229s
Batch 700/3907, reward: 4.292, loss: 0.0047, took: 37.8615s
Batch 800/3907, reward: 4.289, loss: 0.0460, took: 37.9460s
Batch 900/3907, reward: 4.290, loss: 0.0336, took: 38.1295s
Batch 1000/3907, reward: 4.287, loss: 0.0129, took: 38.0461s
Batch 1100/3907, reward: 4.296, loss: 0.0073, took: 37.8678s
Batch 1200/3907, reward: 4.293, loss: 0.0155, took: 37.9373s
Batch 1300/3907, reward: 4.293, loss: 0.0307, took: 38.1337s
Batch 1400/3907, reward: 4.295, loss: 0.0239, took: 38.1011s
Batch 1500/3907, reward: 4.291, loss: 0.0003, took: 37.8776s
Batch 1600/3907, reward: 4.291, loss: 0.0163, took: 37.8895s
Batch 1700/3907, reward: 4.292, loss: 0.0207, took: 37.9137s
Batch 1800/3907, reward: 4.287, loss: 0.0069, took: 38.0365s
Batch 1900/3907, reward: 4.297, loss: 0.0243, took: 37.7938s
Batch 2000/3907, reward: 4.295, loss: 0.0146, took: 37.9708s
Batch 2100/3907, reward: 4.297, loss: 0.0389, took: 37.9404s
Batch 2200/3907, reward: 4.293, loss: 0.0221, took: 37.8386s
Batch 2300/3907, reward: 4.296, loss: 0.0225, took: 38.0211s
Batch 2400/3907, reward: 4.293, loss: 0.0257, took: 37.8444s
Batch 2500/3907, reward: 4.289, loss: 0.0360, took: 37.9266s
Batch 2600/3907, reward: 4.289, loss: 0.0422, took: 37.8234s
Batch 2700/3907, reward: 4.288, loss: 0.0237, took: 37.8072s
Batch 2800/3907, reward: 4.290, loss: 0.0133, took: 37.8305s
Batch 2900/3907, reward: 4.286, loss: 0.0325, took: 37.1911s
Batch 3000/3907, reward: 4.292, loss: 0.0344, took: 35.7280s
Batch 3100/3907, reward: 4.292, loss: 0.0239, took: 37.8440s
Batch 3200/3907, reward: 4.291, loss: 0.0328, took: 37.9043s
Batch 3300/3907, reward: 4.290, loss: 0.0243, took: 37.9615s
Batch 3400/3907, reward: 4.292, loss: 0.0250, took: 38.1459s
Batch 3500/3907, reward: 4.286, loss: 0.0214, took: 37.9890s
Batch 3600/3907, reward: 4.298, loss: 0.0337, took: 35.2070s
Batch 3700/3907, reward: 4.293, loss: 0.0291, took: 38.2479s
Batch 3800/3907, reward: 4.300, loss: -0.0011, took: 38.1096s
Batch 3900/3907, reward: 4.296, loss: 0.0264, took: 38.2075s
Mean epoch loss/reward: 0.0238, 4.2920, 4.2894, took: 1484.1776s (36.8155s / 100 batches)
Batch 0/3907, reward: 4.261, loss: -0.2317, took: 0.5574s
Batch 100/3907, reward: 4.289, loss: 0.0151, took: 38.2674s
Batch 200/3907, reward: 4.290, loss: 0.0167, took: 38.0028s
Batch 300/3907, reward: 4.293, loss: 0.0239, took: 37.9559s
Batch 400/3907, reward: 4.300, loss: 0.0168, took: 38.1763s
Batch 500/3907, reward: 4.293, loss: 0.0375, took: 38.0124s
Batch 600/3907, reward: 4.298, loss: 0.0203, took: 35.1354s
Batch 700/3907, reward: 4.294, loss: 0.0423, took: 37.9244s
Batch 800/3907, reward: 4.293, loss: 0.0429, took: 37.8401s
Batch 900/3907, reward: 4.296, loss: 0.0112, took: 37.9372s
Batch 1000/3907, reward: 4.293, loss: 0.0302, took: 37.9752s
Batch 1100/3907, reward: 4.294, loss: 0.0427, took: 38.1093s
Batch 1200/3907, reward: 4.297, loss: 0.0105, took: 38.0158s
Batch 1300/3907, reward: 4.286, loss: 0.0071, took: 38.0790s
Batch 1400/3907, reward: 4.291, loss: 0.0159, took: 38.0532s
Batch 1500/3907, reward: 4.290, loss: 0.0284, took: 38.1988s
Batch 1600/3907, reward: 4.292, loss: 0.0213, took: 38.0969s
Batch 1700/3907, reward: 4.289, loss: 0.0199, took: 37.9312s
Batch 1800/3907, reward: 4.297, loss: 0.0215, took: 38.0427s
Batch 1900/3907, reward: 4.291, loss: 0.0379, took: 37.9440s
Batch 2000/3907, reward: 4.293, loss: 0.0364, took: 37.9906s
Batch 2100/3907, reward: 4.291, loss: -0.0163, took: 38.1318s
Batch 2200/3907, reward: 4.295, loss: 0.0032, took: 37.9698s
Batch 2300/3907, reward: 4.293, loss: 0.0267, took: 37.8998s
Batch 2400/3907, reward: 4.287, loss: 0.0066, took: 37.8653s
Batch 2500/3907, reward: 4.290, loss: 0.0358, took: 38.1495s
Batch 2600/3907, reward: 4.285, loss: 0.0359, took: 37.9948s
Batch 2700/3907, reward: 4.292, loss: 0.0292, took: 37.8629s
Batch 2800/3907, reward: 4.285, loss: 0.0256, took: 38.1557s
Batch 2900/3907, reward: 4.288, loss: 0.0211, took: 37.4403s
Batch 3000/3907, reward: 4.292, loss: 0.0313, took: 35.8829s
Batch 3100/3907, reward: 4.296, loss: 0.0068, took: 38.1052s
Batch 3200/3907, reward: 4.295, loss: 0.0349, took: 38.1076s
Batch 3300/3907, reward: 4.290, loss: 0.0277, took: 38.1783s
Batch 3400/3907, reward: 4.291, loss: 0.0104, took: 38.1772s
Batch 3500/3907, reward: 4.292, loss: 0.0313, took: 38.1455s
Batch 3600/3907, reward: 4.288, loss: 0.0294, took: 35.4552s
Batch 3700/3907, reward: 4.290, loss: 0.0277, took: 38.0279s
Batch 3800/3907, reward: 4.288, loss: 0.0197, took: 37.9328s
Batch 3900/3907, reward: 4.292, loss: 0.0122, took: 37.8234s
Mean epoch loss/reward: 0.0230, 4.2918, 4.3037, took: 1487.1175s (36.8888s / 100 batches)
Batch 0/3907, reward: 4.313, loss: -0.0394, took: 0.5224s
Batch 100/3907, reward: 4.302, loss: 0.0193, took: 38.1025s
Batch 200/3907, reward: 4.290, loss: 0.0199, took: 37.9410s
Batch 300/3907, reward: 4.300, loss: 0.0277, took: 38.0419s
Batch 400/3907, reward: 4.298, loss: 0.0184, took: 37.8383s
Batch 500/3907, reward: 4.288, loss: 0.0386, took: 37.6213s
Batch 600/3907, reward: 4.290, loss: 0.0072, took: 35.3831s
Batch 700/3907, reward: 4.285, loss: 0.0031, took: 37.9250s
Batch 800/3907, reward: 4.290, loss: 0.0177, took: 38.0345s
Batch 900/3907, reward: 4.284, loss: 0.0617, took: 38.0465s
Batch 1000/3907, reward: 4.297, loss: 0.0290, took: 38.1175s
Batch 1100/3907, reward: 4.287, loss: 0.0234, took: 38.1290s
Batch 1200/3907, reward: 4.290, loss: 0.0144, took: 38.2537s
Batch 1300/3907, reward: 4.287, loss: 0.0156, took: 37.9141s
Batch 1400/3907, reward: 4.295, loss: 0.0358, took: 38.1059s
Batch 1500/3907, reward: 4.289, loss: 0.0108, took: 37.9905s
Batch 1600/3907, reward: 4.289, loss: 0.0048, took: 38.0037s
Batch 1700/3907, reward: 4.291, loss: 0.0170, took: 38.0303s
Batch 1800/3907, reward: 4.288, loss: -0.0125, took: 38.1045s
Batch 1900/3907, reward: 4.288, loss: 0.0328, took: 37.8360s
Batch 2000/3907, reward: 4.280, loss: 0.0518, took: 37.9614s
Batch 2100/3907, reward: 4.287, loss: 0.0062, took: 37.9735s
Batch 2200/3907, reward: 4.295, loss: 0.0309, took: 37.9992s
Batch 2300/3907, reward: 4.294, loss: 0.0263, took: 38.0536s
Batch 2400/3907, reward: 4.293, loss: 0.0166, took: 38.0507s
Batch 2500/3907, reward: 4.293, loss: 0.0192, took: 38.1758s
Batch 2600/3907, reward: 4.294, loss: 0.0382, took: 38.2012s
Batch 2700/3907, reward: 4.295, loss: 0.0081, took: 38.0753s
Batch 2800/3907, reward: 4.292, loss: 0.0239, took: 38.1557s
Batch 2900/3907, reward: 4.287, loss: 0.0179, took: 37.1326s
Batch 3000/3907, reward: 4.290, loss: 0.0296, took: 35.8065s
Batch 3100/3907, reward: 4.284, loss: 0.0152, took: 38.0842s
Batch 3200/3907, reward: 4.289, loss: 0.0208, took: 37.9484s
Batch 3300/3907, reward: 4.289, loss: 0.0161, took: 37.9650s
Batch 3400/3907, reward: 4.291, loss: 0.0152, took: 37.9428s
Batch 3500/3907, reward: 4.283, loss: 0.0391, took: 37.8868s
Batch 3600/3907, reward: 4.294, loss: 0.0299, took: 35.1396s
Batch 3700/3907, reward: 4.293, loss: 0.0158, took: 38.0648s
Batch 3800/3907, reward: 4.297, loss: -0.0073, took: 37.8674s
Batch 3900/3907, reward: 4.299, loss: 0.0144, took: 38.0813s
Mean epoch loss/reward: 0.0207, 4.2909, 4.2962, took: 1486.2253s (36.8627s / 100 batches)
Batch 0/3907, reward: 4.306, loss: 0.2212, took: 0.5254s
Batch 100/3907, reward: 4.291, loss: 0.0189, took: 38.2512s
Batch 200/3907, reward: 4.291, loss: 0.0277, took: 38.0604s
Batch 300/3907, reward: 4.287, loss: 0.0231, took: 38.0342s
Batch 400/3907, reward: 4.287, loss: 0.0443, took: 38.0088s
Batch 500/3907, reward: 4.284, loss: 0.0204, took: 37.5625s
Batch 600/3907, reward: 4.288, loss: 0.0340, took: 35.2011s
Batch 700/3907, reward: 4.295, loss: 0.0258, took: 38.1265s
Batch 800/3907, reward: 4.290, loss: 0.0365, took: 38.1013s
Batch 900/3907, reward: 4.291, loss: 0.0142, took: 37.9867s
Batch 1000/3907, reward: 4.291, loss: 0.0315, took: 38.1558s
Batch 1100/3907, reward: 4.294, loss: 0.0253, took: 38.2051s
Batch 1200/3907, reward: 4.299, loss: 0.0384, took: 38.3471s
Batch 1300/3907, reward: 4.297, loss: 0.0170, took: 38.1377s
Batch 1400/3907, reward: 4.300, loss: 0.0209, took: 38.1470s
Batch 1500/3907, reward: 4.290, loss: 0.0207, took: 38.1208s
Batch 1600/3907, reward: 4.289, loss: 0.0177, took: 37.9871s
Batch 1700/3907, reward: 4.294, loss: 0.0161, took: 37.9026s
Batch 1800/3907, reward: 4.288, loss: 0.0360, took: 38.1438s
Batch 1900/3907, reward: 4.292, loss: 0.0274, took: 38.0633s
Batch 2000/3907, reward: 4.291, loss: 0.0351, took: 38.0305s
Batch 2100/3907, reward: 4.286, loss: 0.0224, took: 38.0536s
Batch 2200/3907, reward: 4.288, loss: 0.0272, took: 37.9482s
Batch 2300/3907, reward: 4.288, loss: 0.0441, took: 38.0861s
Batch 2400/3907, reward: 4.290, loss: 0.0162, took: 38.0182s
Batch 2500/3907, reward: 4.295, loss: 0.0417, took: 38.0274s
Batch 2600/3907, reward: 4.291, loss: 0.0130, took: 38.2387s
Batch 2700/3907, reward: 4.293, loss: 0.0371, took: 38.0837s
Batch 2800/3907, reward: 4.291, loss: 0.0353, took: 38.0719s
Batch 2900/3907, reward: 4.293, loss: 0.0250, took: 36.9629s
Batch 3000/3907, reward: 4.286, loss: 0.0380, took: 36.3301s
Batch 3100/3907, reward: 4.296, loss: 0.0084, took: 37.9393s
Batch 3200/3907, reward: 4.294, loss: 0.0284, took: 37.9647s
Batch 3300/3907, reward: 4.290, loss: 0.0342, took: 37.9002s
Batch 3400/3907, reward: 4.290, loss: 0.0160, took: 37.8338s
Batch 3500/3907, reward: 4.285, loss: 0.0276, took: 38.0389s
Batch 3600/3907, reward: 4.282, loss: 0.0419, took: 35.4127s
Batch 3700/3907, reward: 4.290, loss: 0.0296, took: 38.0831s
Batch 3800/3907, reward: 4.286, loss: 0.0173, took: 38.0264s
Batch 3900/3907, reward: 4.289, loss: 0.0081, took: 37.9510s
Mean epoch loss/reward: 0.0267, 4.2906, 4.2860, took: 1487.7905s (36.9018s / 100 batches)
Batch 0/3907, reward: 4.260, loss: 0.2130, took: 0.5251s
Batch 100/3907, reward: 4.288, loss: 0.0193, took: 38.0861s
Batch 200/3907, reward: 4.288, loss: 0.0370, took: 37.8602s
Batch 300/3907, reward: 4.293, loss: 0.0382, took: 37.9535s
Batch 400/3907, reward: 4.286, loss: 0.0372, took: 37.8617s
Batch 500/3907, reward: 4.296, loss: 0.0252, took: 37.3255s
Batch 600/3907, reward: 4.285, loss: 0.0287, took: 35.7897s
Batch 700/3907, reward: 4.284, loss: 0.0273, took: 37.9487s
Batch 800/3907, reward: 4.287, loss: 0.0143, took: 37.8339s
Batch 900/3907, reward: 4.289, loss: 0.0514, took: 37.9866s
Batch 1000/3907, reward: 4.296, loss: 0.0280, took: 38.0737s
Batch 1100/3907, reward: 4.289, loss: 0.0317, took: 37.9326s
Batch 1200/3907, reward: 4.299, loss: 0.0158, took: 37.9555s
Batch 1300/3907, reward: 4.286, loss: 0.0156, took: 37.8893s
Batch 1400/3907, reward: 4.285, loss: 0.0091, took: 38.0921s
Batch 1500/3907, reward: 4.291, loss: 0.0252, took: 37.9708s
Batch 1600/3907, reward: 4.288, loss: 0.0254, took: 38.0642s
Batch 1700/3907, reward: 4.291, loss: 0.0152, took: 37.9694s
Batch 1800/3907, reward: 4.287, loss: 0.0475, took: 38.0509s
Batch 1900/3907, reward: 4.296, loss: 0.0295, took: 38.0509s
Batch 2000/3907, reward: 4.284, loss: 0.0251, took: 38.0257s
Batch 2100/3907, reward: 4.288, loss: 0.0246, took: 38.1427s
Batch 2200/3907, reward: 4.291, loss: 0.0127, took: 37.8655s
Batch 2300/3907, reward: 4.286, loss: 0.0317, took: 38.0371s
Batch 2400/3907, reward: 4.292, loss: 0.0203, took: 38.0341s
Batch 2500/3907, reward: 4.285, loss: 0.0542, took: 38.0705s
Batch 2600/3907, reward: 4.289, loss: 0.0139, took: 37.9295s
Batch 2700/3907, reward: 4.287, loss: 0.0340, took: 37.8296s
Batch 2800/3907, reward: 4.293, loss: 0.0275, took: 37.9923s
Batch 2900/3907, reward: 4.289, loss: 0.0305, took: 36.4633s
Batch 3000/3907, reward: 4.294, loss: 0.0346, took: 36.3201s
Batch 3100/3907, reward: 4.288, loss: 0.0264, took: 38.0469s
Batch 3200/3907, reward: 4.294, loss: 0.0410, took: 38.0505s
Batch 3300/3907, reward: 4.288, loss: 0.0317, took: 37.8954s
Batch 3400/3907, reward: 4.291, loss: 0.0244, took: 38.1121s
Batch 3500/3907, reward: 4.297, loss: 0.0184, took: 37.8441s
Batch 3600/3907, reward: 4.289, loss: 0.0426, took: 35.3217s
Batch 3700/3907, reward: 4.291, loss: 0.0277, took: 37.9957s
Batch 3800/3907, reward: 4.292, loss: 0.0185, took: 38.1912s
Batch 3900/3907, reward: 4.294, loss: 0.0238, took: 37.9204s
Mean epoch loss/reward: 0.0278, 4.2898, 4.3037, took: 1484.8527s (36.8327s / 100 batches)
Batch 0/3907, reward: 4.246, loss: 0.2813, took: 0.5139s
Batch 100/3907, reward: 4.296, loss: 0.0065, took: 38.1738s
Batch 200/3907, reward: 4.286, loss: 0.0222, took: 37.9033s
Batch 300/3907, reward: 4.283, loss: 0.0368, took: 38.0393s
Batch 400/3907, reward: 4.286, loss: 0.0333, took: 37.9873s
Batch 500/3907, reward: 4.286, loss: 0.0181, took: 37.2843s
Batch 600/3907, reward: 4.293, loss: 0.0222, took: 35.5654s
Batch 700/3907, reward: 4.292, loss: 0.0182, took: 38.1136s
Batch 800/3907, reward: 4.289, loss: 0.0288, took: 38.1720s
Batch 900/3907, reward: 4.291, loss: 0.0293, took: 38.0002s
Batch 1000/3907, reward: 4.290, loss: 0.0351, took: 37.8699s
Batch 1100/3907, reward: 4.289, loss: 0.0372, took: 37.7835s
Batch 1200/3907, reward: 4.290, loss: 0.0162, took: 37.9931s
Batch 1300/3907, reward: 4.292, loss: 0.0220, took: 38.0438s
Batch 1400/3907, reward: 4.289, loss: 0.0097, took: 37.7473s
Batch 1500/3907, reward: 4.289, loss: 0.0287, took: 38.1708s
Batch 1600/3907, reward: 4.295, loss: 0.0249, took: 38.0761s
Batch 1700/3907, reward: 4.288, loss: 0.0368, took: 38.0882s
Batch 1800/3907, reward: 4.284, loss: 0.0243, took: 38.0213s
Batch 1900/3907, reward: 4.286, loss: 0.0365, took: 38.0011s
Batch 2000/3907, reward: 4.286, loss: 0.0434, took: 38.0727s
Batch 2100/3907, reward: 4.289, loss: 0.0362, took: 38.0609s
Batch 2200/3907, reward: 4.288, loss: 0.0416, took: 37.9386s
Batch 2300/3907, reward: 4.285, loss: 0.0504, took: 37.9878s
Batch 2400/3907, reward: 4.285, loss: 0.0300, took: 37.9442s
Batch 2500/3907, reward: 4.285, loss: 0.0136, took: 38.0799s
Batch 2600/3907, reward: 4.287, loss: 0.0267, took: 37.8977s
Batch 2700/3907, reward: 4.285, loss: 0.0189, took: 37.9397s
Batch 2800/3907, reward: 4.284, loss: 0.0424, took: 37.8797s
Batch 2900/3907, reward: 4.286, loss: 0.0166, took: 36.4705s
Batch 3000/3907, reward: 4.290, loss: 0.0157, took: 36.5638s
Batch 3100/3907, reward: 4.287, loss: 0.0277, took: 37.8165s
Batch 3200/3907, reward: 4.291, loss: 0.0114, took: 37.8434s
Batch 3300/3907, reward: 4.286, loss: 0.0123, took: 37.9501s
Batch 3400/3907, reward: 4.292, loss: 0.0436, took: 37.9144s
Batch 3500/3907, reward: 4.289, loss: 0.0291, took: 38.0677s
Batch 3600/3907, reward: 4.284, loss: 0.0069, took: 35.1202s
Batch 3700/3907, reward: 4.293, loss: 0.0413, took: 37.8870s
Batch 3800/3907, reward: 4.291, loss: 0.0142, took: 37.8963s
Batch 3900/3907, reward: 4.296, loss: 0.0246, took: 37.8851s
Mean epoch loss/reward: 0.0266, 4.2885, 4.2898, took: 1484.1866s (36.8191s / 100 batches)
Average tour length for uniform: 4.041287229346106
Average tour length for shifted: 3.453057228588283
Average tour length for adversary: 2.857144140544254
