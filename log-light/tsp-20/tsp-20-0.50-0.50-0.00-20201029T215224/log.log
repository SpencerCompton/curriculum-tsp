Current device: cuda
Batch 0/3907, reward: 12.017, loss: -694.7775, took: 0.5972s
Batch 100/3907, reward: 9.556, loss: -5.0019, took: 30.5618s
Batch 200/3907, reward: 5.970, loss: 0.7799, took: 29.5809s
Batch 300/3907, reward: 5.758, loss: 0.6980, took: 29.2016s
Batch 400/3907, reward: 5.667, loss: 0.0738, took: 28.6778s
Batch 500/3907, reward: 5.611, loss: -0.0826, took: 31.8496s
Batch 600/3907, reward: 5.566, loss: 0.0029, took: 30.2199s
Batch 700/3907, reward: 5.530, loss: 0.2175, took: 30.4100s
Batch 800/3907, reward: 5.517, loss: 0.8252, took: 30.6663s
Batch 900/3907, reward: 5.518, loss: -0.0109, took: 29.0094s
Batch 1000/3907, reward: 5.500, loss: 0.5311, took: 26.3841s
Batch 1100/3907, reward: 5.501, loss: 0.8804, took: 28.3398s
Batch 1200/3907, reward: 5.489, loss: 0.7787, took: 28.5298s
Batch 1300/3907, reward: 5.476, loss: 0.7673, took: 29.5111s
Batch 1400/3907, reward: 5.479, loss: 0.4406, took: 30.0261s
Batch 1500/3907, reward: 5.471, loss: 0.0974, took: 29.1083s
Batch 1600/3907, reward: 5.472, loss: 0.0900, took: 31.0004s
Batch 1700/3907, reward: 5.473, loss: 0.1228, took: 30.9206s
Batch 1800/3907, reward: 5.460, loss: -0.1967, took: 30.2366s
Batch 1900/3907, reward: 5.470, loss: -0.1650, took: 32.2453s
Batch 2000/3907, reward: 5.457, loss: 0.1369, took: 29.6739s
Batch 2100/3907, reward: 5.470, loss: 0.3000, took: 30.2849s
Batch 2200/3907, reward: 5.463, loss: 0.3539, took: 34.6016s
Batch 2300/3907, reward: 5.460, loss: -0.1430, took: 39.5256s
Batch 2400/3907, reward: 5.462, loss: -0.3873, took: 40.3124s
Batch 2500/3907, reward: 5.452, loss: -0.0107, took: 36.2960s
Batch 2600/3907, reward: 5.445, loss: 0.0849, took: 37.5390s
Batch 2700/3907, reward: 5.450, loss: 0.1183, took: 39.6614s
Batch 2800/3907, reward: 5.449, loss: 0.2296, took: 40.7186s
Batch 2900/3907, reward: 5.462, loss: -0.0511, took: 47.4378s
Batch 3000/3907, reward: 5.454, loss: 0.5743, took: 48.7766s
Batch 3100/3907, reward: 5.447, loss: 0.4035, took: 48.5240s
Batch 3200/3907, reward: 5.345, loss: 0.1773, took: 48.4647s
Batch 3300/3907, reward: 5.157, loss: -0.0858, took: 49.9980s
Batch 3400/3907, reward: 4.995, loss: 0.0263, took: 47.4888s
Batch 3500/3907, reward: 4.848, loss: 0.0105, took: 48.9492s
Batch 3600/3907, reward: 4.479, loss: -0.1540, took: 48.7479s
Batch 3700/3907, reward: 4.357, loss: -0.3232, took: 48.3421s
Batch 3800/3907, reward: 4.291, loss: -0.2237, took: 47.4940s
Batch 3900/3907, reward: 4.236, loss: -0.4310, took: 48.5468s
Mean epoch loss/reward: -0.1407, 5.4527, 4.0647, took: 1443.5714s (35.7115s / 100 batches)
Batch 0/3907, reward: 4.200, loss: -0.7760, took: 0.6238s
Batch 100/3907, reward: 4.189, loss: -0.2208, took: 48.6729s
Batch 200/3907, reward: 4.164, loss: -0.0821, took: 48.9230s
Batch 300/3907, reward: 4.135, loss: -0.5634, took: 50.1564s
Batch 400/3907, reward: 4.109, loss: -0.2354, took: 48.7458s
Batch 500/3907, reward: 4.090, loss: 0.0572, took: 49.3435s
Batch 600/3907, reward: 4.077, loss: -0.2448, took: 48.1684s
Batch 700/3907, reward: 4.056, loss: -0.1925, took: 48.1273s
Batch 800/3907, reward: 4.037, loss: -0.2084, took: 47.4622s
Batch 900/3907, reward: 4.030, loss: -0.1808, took: 48.7940s
Batch 1000/3907, reward: 4.021, loss: 0.0001, took: 49.0583s
Batch 1100/3907, reward: 3.998, loss: -0.1487, took: 44.8534s
Batch 1200/3907, reward: 4.001, loss: -0.1344, took: 49.5607s
Batch 1300/3907, reward: 3.986, loss: -0.1620, took: 48.2384s
Batch 1400/3907, reward: 3.992, loss: -0.2219, took: 49.2875s
Batch 1500/3907, reward: 3.973, loss: -0.3058, took: 48.8677s
Batch 1600/3907, reward: 3.972, loss: -0.2760, took: 48.9184s
Batch 1700/3907, reward: 3.967, loss: -0.1563, took: 49.4959s
Batch 1800/3907, reward: 3.956, loss: -0.2503, took: 47.8339s
Batch 1900/3907, reward: 3.945, loss: -0.1667, took: 47.8976s
Batch 2000/3907, reward: 3.949, loss: -0.2034, took: 48.6412s
Batch 2100/3907, reward: 3.941, loss: -0.3233, took: 46.0751s
Batch 2200/3907, reward: 3.934, loss: -0.1898, took: 48.0376s
Batch 2300/3907, reward: 3.924, loss: -0.1224, took: 48.5588s
Batch 2400/3907, reward: 3.925, loss: -0.0794, took: 47.9423s
Batch 2500/3907, reward: 3.918, loss: -0.2673, took: 44.9037s
Batch 2600/3907, reward: 3.917, loss: -0.2121, took: 49.1912s
Batch 2700/3907, reward: 3.911, loss: -0.1611, took: 48.7062s
Batch 2800/3907, reward: 3.913, loss: -0.0204, took: 46.5948s
Batch 2900/3907, reward: 3.905, loss: -0.1920, took: 48.4455s
Batch 3000/3907, reward: 3.903, loss: -0.1566, took: 48.8137s
Batch 3100/3907, reward: 3.903, loss: -0.0827, took: 47.7936s
Batch 3200/3907, reward: 3.909, loss: -0.2191, took: 49.3721s
Batch 3300/3907, reward: 3.898, loss: -0.1181, took: 49.5975s
Batch 3400/3907, reward: 3.902, loss: -0.0598, took: 48.7472s
Batch 3500/3907, reward: 3.896, loss: -0.0980, took: 48.0079s
Batch 3600/3907, reward: 3.891, loss: -0.1609, took: 47.4936s
Batch 3700/3907, reward: 3.887, loss: -0.1146, took: 48.9084s
Batch 3800/3907, reward: 3.887, loss: -0.1863, took: 48.2532s
Batch 3900/3907, reward: 3.878, loss: -0.0847, took: 48.4089s
Mean epoch loss/reward: -0.1728, 3.9715, 3.8290, took: 1899.8829s (47.1380s / 100 batches)
Batch 0/3907, reward: 3.854, loss: -0.2521, took: 0.5500s
Batch 100/3907, reward: 3.886, loss: -0.0879, took: 48.2809s
Batch 200/3907, reward: 3.882, loss: -0.1631, took: 48.9034s
Batch 300/3907, reward: 3.881, loss: -0.0693, took: 48.3704s
Batch 400/3907, reward: 3.876, loss: -0.0626, took: 47.3598s
Batch 500/3907, reward: 3.871, loss: -0.1147, took: 48.8329s
Batch 600/3907, reward: 3.874, loss: -0.1444, took: 47.7174s
Batch 700/3907, reward: 3.879, loss: -0.1228, took: 49.1960s
Batch 800/3907, reward: 3.878, loss: -0.0357, took: 47.9341s
Batch 900/3907, reward: 3.863, loss: -0.0759, took: 48.1765s
Batch 1000/3907, reward: 3.864, loss: -0.0444, took: 47.6542s
Batch 1100/3907, reward: 3.869, loss: -0.0990, took: 46.5233s
Batch 1200/3907, reward: 3.868, loss: -0.1277, took: 48.6415s
Batch 1300/3907, reward: 3.869, loss: -0.1580, took: 48.0351s
Batch 1400/3907, reward: 3.862, loss: -0.0958, took: 48.5760s
Batch 1500/3907, reward: 3.859, loss: -0.1388, took: 48.1042s
Batch 1600/3907, reward: 3.871, loss: -0.0098, took: 47.8068s
Batch 1700/3907, reward: 3.865, loss: -0.0758, took: 47.0917s
Batch 1800/3907, reward: 3.855, loss: -0.0847, took: 48.2872s
Batch 1900/3907, reward: 3.856, loss: -0.0299, took: 46.7226s
Batch 2000/3907, reward: 3.860, loss: -0.0957, took: 48.4491s
Batch 2100/3907, reward: 3.860, loss: -0.0560, took: 45.7159s
Batch 2200/3907, reward: 3.852, loss: -0.1027, took: 45.8802s
Batch 2300/3907, reward: 3.856, loss: -0.0444, took: 49.0280s
Batch 2400/3907, reward: 3.849, loss: -0.1160, took: 48.8804s
Batch 2500/3907, reward: 3.851, loss: -0.0987, took: 44.9509s
Batch 2600/3907, reward: 3.849, loss: -0.0870, took: 47.7029s
Batch 2700/3907, reward: 3.849, loss: -0.1507, took: 47.7921s
Batch 2800/3907, reward: 3.849, loss: -0.1211, took: 46.2653s
Batch 2900/3907, reward: 3.846, loss: -0.1247, took: 48.5804s
Batch 3000/3907, reward: 3.847, loss: -0.0961, took: 46.5286s
Batch 3100/3907, reward: 3.859, loss: -0.1052, took: 48.2210s
Batch 3200/3907, reward: 3.845, loss: -0.0997, took: 48.3282s
Batch 3300/3907, reward: 3.843, loss: -0.0623, took: 49.5902s
Batch 3400/3907, reward: 3.849, loss: -0.0501, took: 48.5145s
Batch 3500/3907, reward: 3.846, loss: -0.0988, took: 48.2669s
Batch 3600/3907, reward: 3.848, loss: -0.1052, took: 48.0204s
Batch 3700/3907, reward: 3.846, loss: -0.0403, took: 49.2158s
Batch 3800/3907, reward: 3.840, loss: -0.0693, took: 48.1058s
Batch 3900/3907, reward: 3.841, loss: -0.0821, took: 49.1193s
Mean epoch loss/reward: -0.0910, 3.8593, 3.8010, took: 1883.8684s (46.7480s / 100 batches)
Batch 0/3907, reward: 3.862, loss: -0.5535, took: 0.6469s
Batch 100/3907, reward: 3.842, loss: -0.0837, took: 48.0099s
Batch 200/3907, reward: 3.842, loss: -0.0359, took: 48.5559s
Batch 300/3907, reward: 3.837, loss: -0.0737, took: 47.2789s
Batch 400/3907, reward: 3.844, loss: -0.0906, took: 47.1381s
Batch 500/3907, reward: 3.841, loss: -0.0478, took: 48.6678s
Batch 600/3907, reward: 3.839, loss: -0.0769, took: 48.8582s
Batch 700/3907, reward: 3.839, loss: -0.0214, took: 46.7484s
Batch 800/3907, reward: 3.833, loss: -0.0374, took: 48.0974s
Batch 900/3907, reward: 3.838, loss: -0.0741, took: 48.6352s
Batch 1000/3907, reward: 3.830, loss: -0.0708, took: 48.6699s
Batch 1100/3907, reward: 3.843, loss: -0.0642, took: 44.5698s
Batch 1200/3907, reward: 3.835, loss: -0.0690, took: 49.0044s
Batch 1300/3907, reward: 3.838, loss: -0.0688, took: 48.3714s
Batch 1400/3907, reward: 3.833, loss: -0.0940, took: 48.0876s
Batch 1500/3907, reward: 3.832, loss: -0.0517, took: 48.0089s
Batch 1600/3907, reward: 3.835, loss: -0.0941, took: 48.1680s
Batch 1700/3907, reward: 3.830, loss: -0.1007, took: 48.2770s
Batch 1800/3907, reward: 3.836, loss: -0.0552, took: 48.1167s
Batch 1900/3907, reward: 3.834, loss: -0.0939, took: 49.0906s
Batch 2000/3907, reward: 3.837, loss: -0.0749, took: 48.5158s
Batch 2100/3907, reward: 3.834, loss: -0.0994, took: 45.0325s
Batch 2200/3907, reward: 3.832, loss: -0.0585, took: 47.5648s
Batch 2300/3907, reward: 3.837, loss: -0.0524, took: 49.5726s
Batch 2400/3907, reward: 3.831, loss: -0.0807, took: 47.7603s
Batch 2500/3907, reward: 3.835, loss: -0.0455, took: 44.6787s
Batch 2600/3907, reward: 3.832, loss: -0.0475, took: 47.8346s
Batch 2700/3907, reward: 3.830, loss: -0.1077, took: 47.2397s
Batch 2800/3907, reward: 3.831, loss: -0.0832, took: 46.0237s
Batch 2900/3907, reward: 3.830, loss: -0.0668, took: 48.3512s
Batch 3000/3907, reward: 3.824, loss: -0.0417, took: 47.3996s
Batch 3100/3907, reward: 3.824, loss: -0.0624, took: 47.2105s
Batch 3200/3907, reward: 3.825, loss: -0.0558, took: 46.9810s
Batch 3300/3907, reward: 3.832, loss: -0.0430, took: 47.3403s
Batch 3400/3907, reward: 3.831, loss: -0.0412, took: 48.8862s
Batch 3500/3907, reward: 3.824, loss: -0.0698, took: 48.4824s
Batch 3600/3907, reward: 3.830, loss: -0.0841, took: 48.4298s
Batch 3700/3907, reward: 3.826, loss: -0.0622, took: 48.3223s
Batch 3800/3907, reward: 3.823, loss: -0.0812, took: 48.0836s
Batch 3900/3907, reward: 3.820, loss: -0.0880, took: 48.0155s
Mean epoch loss/reward: -0.0680, 3.8331, 3.8054, took: 1878.2890s (46.6181s / 100 batches)
Batch 0/3907, reward: 3.809, loss: 0.0890, took: 0.5795s
Batch 100/3907, reward: 3.832, loss: -0.0534, took: 47.8114s
Batch 200/3907, reward: 3.825, loss: -0.0454, took: 48.4423s
Batch 300/3907, reward: 3.819, loss: -0.0934, took: 48.8034s
Batch 400/3907, reward: 3.824, loss: -0.0534, took: 48.2629s
Batch 500/3907, reward: 3.828, loss: -0.0874, took: 49.5355s
Batch 600/3907, reward: 3.829, loss: -0.0124, took: 48.4745s
Batch 700/3907, reward: 3.824, loss: -0.0194, took: 48.0841s
Batch 800/3907, reward: 3.822, loss: -0.0540, took: 47.2412s
Batch 900/3907, reward: 3.820, loss: -0.0830, took: 49.0767s
Batch 1000/3907, reward: 3.825, loss: -0.0822, took: 48.7668s
Batch 1100/3907, reward: 3.819, loss: -0.0675, took: 45.5565s
Batch 1200/3907, reward: 3.819, loss: -0.0754, took: 47.7110s
Batch 1300/3907, reward: 3.824, loss: -0.0715, took: 47.8398s
Batch 1400/3907, reward: 3.827, loss: -0.0895, took: 47.2553s
Batch 1500/3907, reward: 3.823, loss: -0.0691, took: 47.0355s
Batch 1600/3907, reward: 3.818, loss: -0.0643, took: 47.4908s
Batch 1700/3907, reward: 3.817, loss: -0.0564, took: 47.9382s
Batch 1800/3907, reward: 3.817, loss: -0.0263, took: 48.9441s
Batch 1900/3907, reward: 3.819, loss: -0.0545, took: 48.4834s
Batch 2000/3907, reward: 3.818, loss: -0.0587, took: 47.6616s
Batch 2100/3907, reward: 3.817, loss: -0.0557, took: 45.5394s
Batch 2200/3907, reward: 3.825, loss: -0.0490, took: 48.3147s
Batch 2300/3907, reward: 3.820, loss: -0.0375, took: 48.1966s
Batch 2400/3907, reward: 3.818, loss: -0.0493, took: 48.6178s
Batch 2500/3907, reward: 3.822, loss: -0.0288, took: 48.1457s
Batch 2600/3907, reward: 3.827, loss: -0.0599, took: 46.2073s
Batch 2700/3907, reward: 3.819, loss: -0.0440, took: 47.5127s
Batch 2800/3907, reward: 3.819, loss: -0.0586, took: 44.6288s
Batch 2900/3907, reward: 3.817, loss: -0.0664, took: 47.6500s
Batch 3000/3907, reward: 3.819, loss: -0.0735, took: 47.8497s
Batch 3100/3907, reward: 3.819, loss: -0.0597, took: 48.3464s
Batch 3200/3907, reward: 3.817, loss: -0.0610, took: 48.8469s
Batch 3300/3907, reward: 3.822, loss: -0.0631, took: 47.6003s
Batch 3400/3907, reward: 3.824, loss: -0.0547, took: 48.6335s
Batch 3500/3907, reward: 3.816, loss: -0.0502, took: 47.9468s
Batch 3600/3907, reward: 3.818, loss: -0.0553, took: 47.3210s
Batch 3700/3907, reward: 3.813, loss: -0.0785, took: 47.9594s
Batch 3800/3907, reward: 3.814, loss: -0.0212, took: 47.9325s
Batch 3900/3907, reward: 3.818, loss: -0.0651, took: 47.9862s
Mean epoch loss/reward: -0.0572, 3.8208, 3.7973, took: 1880.1135s (46.6557s / 100 batches)
Batch 0/3907, reward: 3.820, loss: -0.7244, took: 0.6169s
Batch 100/3907, reward: 3.815, loss: -0.0515, took: 47.4278s
Batch 200/3907, reward: 3.815, loss: -0.0564, took: 47.1698s
Batch 300/3907, reward: 3.813, loss: -0.0594, took: 47.8903s
Batch 400/3907, reward: 3.817, loss: -0.0512, took: 48.7568s
Batch 500/3907, reward: 3.814, loss: -0.0454, took: 48.4356s
Batch 600/3907, reward: 3.815, loss: -0.0477, took: 48.9873s
Batch 700/3907, reward: 3.814, loss: -0.0515, took: 49.0540s
Batch 800/3907, reward: 3.812, loss: -0.0385, took: 47.7833s
Batch 900/3907, reward: 3.816, loss: -0.0264, took: 47.4736s
Batch 1000/3907, reward: 3.819, loss: -0.0488, took: 47.8775s
Batch 1100/3907, reward: 3.809, loss: -0.0528, took: 45.2200s
Batch 1200/3907, reward: 3.820, loss: -0.0542, took: 47.8588s
Batch 1300/3907, reward: 3.819, loss: -0.0532, took: 48.1896s
Batch 1400/3907, reward: 3.820, loss: -0.0554, took: 48.9003s
Batch 1500/3907, reward: 3.810, loss: -0.0509, took: 48.2584s
Batch 1600/3907, reward: 3.813, loss: -0.0574, took: 47.2569s
Batch 1700/3907, reward: 3.816, loss: -0.0435, took: 47.4683s
Batch 1800/3907, reward: 3.814, loss: -0.0499, took: 48.2438s
Batch 1900/3907, reward: 3.812, loss: -0.0451, took: 47.8380s
Batch 2000/3907, reward: 3.815, loss: -0.0609, took: 47.8118s
Batch 2100/3907, reward: 3.813, loss: -0.0586, took: 46.4203s
Batch 2200/3907, reward: 3.814, loss: -0.0315, took: 47.6979s
Batch 2300/3907, reward: 3.810, loss: -0.0691, took: 47.8831s
Batch 2400/3907, reward: 3.809, loss: -0.0271, took: 46.0313s
Batch 2500/3907, reward: 3.809, loss: -0.0552, took: 47.4356s
Batch 2600/3907, reward: 3.809, loss: -0.0596, took: 46.0752s
Batch 2700/3907, reward: 3.812, loss: -0.0537, took: 48.8144s
Batch 2800/3907, reward: 3.807, loss: -0.0427, took: 46.0083s
Batch 2900/3907, reward: 3.809, loss: -0.0338, took: 46.9517s
Batch 3000/3907, reward: 3.811, loss: -0.0625, took: 48.1922s
Batch 3100/3907, reward: 3.809, loss: -0.0451, took: 47.8371s
Batch 3200/3907, reward: 3.813, loss: -0.0761, took: 47.2813s
Batch 3300/3907, reward: 3.807, loss: -0.0648, took: 49.3314s
Batch 3400/3907, reward: 3.815, loss: -0.0399, took: 49.6537s
Batch 3500/3907, reward: 3.808, loss: -0.0257, took: 47.7431s
Batch 3600/3907, reward: 3.810, loss: -0.0642, took: 47.5960s
Batch 3700/3907, reward: 3.808, loss: -0.0344, took: 48.8775s
Batch 3800/3907, reward: 3.808, loss: -0.0778, took: 46.7462s
Batch 3900/3907, reward: 3.805, loss: -0.0367, took: 47.5256s
Mean epoch loss/reward: -0.0504, 3.8123, 3.7877, took: 1876.1243s (46.5655s / 100 batches)
Batch 0/3907, reward: 3.814, loss: 0.0486, took: 0.6095s
Batch 100/3907, reward: 3.808, loss: -0.0573, took: 48.1724s
Batch 200/3907, reward: 3.810, loss: -0.0500, took: 50.1331s
Batch 300/3907, reward: 3.807, loss: -0.0413, took: 48.1900s
Batch 400/3907, reward: 3.808, loss: -0.0250, took: 47.7564s
Batch 500/3907, reward: 3.808, loss: -0.0629, took: 48.6768s
Batch 600/3907, reward: 3.806, loss: -0.0430, took: 48.7857s
Batch 700/3907, reward: 3.806, loss: -0.0293, took: 48.3642s
Batch 800/3907, reward: 3.804, loss: -0.0534, took: 47.2992s
Batch 900/3907, reward: 3.809, loss: -0.0486, took: 48.2006s
Batch 1000/3907, reward: 3.804, loss: -0.0373, took: 48.3891s
Batch 1100/3907, reward: 3.804, loss: -0.0500, took: 44.7068s
Batch 1200/3907, reward: 3.807, loss: -0.0441, took: 47.5437s
Batch 1300/3907, reward: 3.813, loss: -0.0925, took: 48.3118s
Batch 1400/3907, reward: 3.813, loss: -0.0214, took: 48.0398s
Batch 1500/3907, reward: 3.808, loss: -0.0560, took: 48.1766s
Batch 1600/3907, reward: 3.807, loss: -0.0235, took: 48.9040s
Batch 1700/3907, reward: 3.813, loss: -0.0255, took: 47.6656s
Batch 1800/3907, reward: 3.815, loss: -0.0612, took: 49.2786s
Batch 1900/3907, reward: 3.807, loss: -0.0415, took: 48.9012s
Batch 2000/3907, reward: 3.809, loss: -0.0598, took: 46.9258s
Batch 2100/3907, reward: 3.801, loss: -0.0620, took: 46.9110s
Batch 2200/3907, reward: 3.802, loss: -0.0213, took: 46.8186s
Batch 2300/3907, reward: 3.809, loss: -0.0441, took: 48.5458s
Batch 2400/3907, reward: 3.804, loss: -0.0317, took: 47.8343s
Batch 2500/3907, reward: 3.802, loss: -0.0174, took: 48.0263s
Batch 2600/3907, reward: 3.804, loss: -0.0126, took: 44.7062s
Batch 2700/3907, reward: 3.805, loss: -0.0393, took: 48.2819s
Batch 2800/3907, reward: 3.801, loss: -0.0486, took: 44.8244s
Batch 2900/3907, reward: 3.803, loss: -0.0456, took: 48.7146s
Batch 3000/3907, reward: 3.815, loss: -0.0527, took: 48.4600s
Batch 3100/3907, reward: 3.808, loss: -0.0361, took: 47.0219s
Batch 3200/3907, reward: 3.809, loss: -0.0440, took: 47.9173s
Batch 3300/3907, reward: 3.804, loss: -0.0269, took: 47.1955s
Batch 3400/3907, reward: 3.803, loss: -0.0612, took: 49.0228s
Batch 3500/3907, reward: 3.804, loss: -0.0511, took: 48.2075s
Batch 3600/3907, reward: 3.802, loss: -0.0512, took: 47.0954s
Batch 3700/3907, reward: 3.800, loss: -0.0356, took: 48.2153s
Batch 3800/3907, reward: 3.804, loss: -0.0405, took: 46.9163s
Batch 3900/3907, reward: 3.801, loss: -0.0270, took: 48.8016s
Mean epoch loss/reward: -0.0428, 3.8063, 3.7875, took: 1881.5425s (46.6637s / 100 batches)
Batch 0/3907, reward: 3.798, loss: 0.0757, took: 0.8824s
Batch 100/3907, reward: 3.799, loss: -0.0459, took: 48.9841s
Batch 200/3907, reward: 3.799, loss: -0.0406, took: 47.1971s
Batch 300/3907, reward: 3.805, loss: -0.0411, took: 47.6597s
Batch 400/3907, reward: 3.805, loss: -0.0271, took: 48.3618s
Batch 500/3907, reward: 3.804, loss: -0.0449, took: 48.0106s
Batch 600/3907, reward: 3.799, loss: -0.0451, took: 47.1217s
Batch 700/3907, reward: 3.807, loss: -0.0599, took: 48.2760s
Batch 800/3907, reward: 3.800, loss: -0.0416, took: 48.4387s
Batch 900/3907, reward: 3.807, loss: -0.0495, took: 47.1561s
Batch 1000/3907, reward: 3.807, loss: -0.0413, took: 48.8683s
Batch 1100/3907, reward: 3.800, loss: -0.0309, took: 44.1197s
Batch 1200/3907, reward: 3.800, loss: -0.0540, took: 48.3002s
Batch 1300/3907, reward: 3.797, loss: -0.0470, took: 48.2539s
Batch 1400/3907, reward: 3.806, loss: -0.0440, took: 47.9792s
Batch 1500/3907, reward: 3.798, loss: -0.0341, took: 49.1826s
Batch 1600/3907, reward: 3.800, loss: -0.0315, took: 48.2267s
Batch 1700/3907, reward: 3.797, loss: -0.0491, took: 48.7947s
Batch 1800/3907, reward: 3.798, loss: -0.0422, took: 47.9262s
Batch 1900/3907, reward: 3.798, loss: -0.0370, took: 48.5938s
Batch 2000/3907, reward: 3.801, loss: -0.0522, took: 48.0598s
Batch 2100/3907, reward: 3.800, loss: -0.0437, took: 49.2582s
Batch 2200/3907, reward: 3.797, loss: -0.0435, took: 46.4381s
Batch 2300/3907, reward: 3.799, loss: -0.0414, took: 48.6934s
Batch 2400/3907, reward: 3.795, loss: -0.0416, took: 47.0708s
Batch 2500/3907, reward: 3.798, loss: -0.0208, took: 47.5372s
Batch 2600/3907, reward: 3.800, loss: -0.0639, took: 45.8732s
Batch 2700/3907, reward: 3.797, loss: -0.0324, took: 47.5321s
Batch 2800/3907, reward: 3.809, loss: -0.0530, took: 45.0932s
Batch 2900/3907, reward: 3.796, loss: -0.0366, took: 47.2934s
Batch 3000/3907, reward: 3.801, loss: -0.0220, took: 47.9398s
Batch 3100/3907, reward: 3.798, loss: -0.0488, took: 47.4679s
Batch 3200/3907, reward: 3.798, loss: -0.0403, took: 47.9599s
Batch 3300/3907, reward: 3.801, loss: -0.0409, took: 47.8571s
Batch 3400/3907, reward: 3.804, loss: -0.0439, took: 47.7304s
Batch 3500/3907, reward: 3.799, loss: -0.0442, took: 47.8602s
Batch 3600/3907, reward: 3.804, loss: -0.0354, took: 49.0324s
Batch 3700/3907, reward: 3.798, loss: -0.0569, took: 48.1576s
Batch 3800/3907, reward: 3.800, loss: -0.0267, took: 48.1478s
Batch 3900/3907, reward: 3.803, loss: -0.0424, took: 46.6881s
Mean epoch loss/reward: -0.0420, 3.8006, 3.8033, took: 1879.0761s (46.6006s / 100 batches)
Batch 0/3907, reward: 3.799, loss: -0.1339, took: 0.6874s
Batch 100/3907, reward: 3.795, loss: -0.0422, took: 48.2935s
Batch 200/3907, reward: 3.801, loss: -0.0427, took: 46.8452s
Batch 300/3907, reward: 3.799, loss: -0.0373, took: 47.3904s
Batch 400/3907, reward: 3.798, loss: -0.0493, took: 47.3149s
Batch 500/3907, reward: 3.797, loss: -0.0492, took: 47.5818s
Batch 600/3907, reward: 3.802, loss: -0.0442, took: 47.9497s
Batch 700/3907, reward: 3.801, loss: -0.0432, took: 47.3623s
Batch 800/3907, reward: 3.799, loss: -0.0379, took: 48.0342s
Batch 900/3907, reward: 3.797, loss: -0.0371, took: 48.1593s
Batch 1000/3907, reward: 3.797, loss: -0.0398, took: 47.6487s
Batch 1100/3907, reward: 3.796, loss: -0.0390, took: 43.7933s
Batch 1200/3907, reward: 3.798, loss: -0.0451, took: 48.6107s
Batch 1300/3907, reward: 3.801, loss: -0.0387, took: 48.4584s
Batch 1400/3907, reward: 3.799, loss: -0.0294, took: 47.1227s
Batch 1500/3907, reward: 3.798, loss: -0.0479, took: 48.0936s
Batch 1600/3907, reward: 3.797, loss: -0.0440, took: 48.0392s
Batch 1700/3907, reward: 3.796, loss: -0.0276, took: 47.4214s
Batch 1800/3907, reward: 3.809, loss: -0.0421, took: 47.7026s
Batch 1900/3907, reward: 3.796, loss: -0.0285, took: 48.2598s
Batch 2000/3907, reward: 3.802, loss: -0.0290, took: 48.8870s
Batch 2100/3907, reward: 3.797, loss: -0.0402, took: 47.7486s
Batch 2200/3907, reward: 3.799, loss: -0.0456, took: 46.0228s
Batch 2300/3907, reward: 3.804, loss: -0.0393, took: 48.2331s
Batch 2400/3907, reward: 3.796, loss: -0.0286, took: 47.8028s
Batch 2500/3907, reward: 3.796, loss: -0.0366, took: 46.7881s
Batch 2600/3907, reward: 3.794, loss: -0.0195, took: 44.4815s
Batch 2700/3907, reward: 3.797, loss: -0.0488, took: 48.2512s
Batch 2800/3907, reward: 3.799, loss: -0.0444, took: 47.9926s
Batch 2900/3907, reward: 3.801, loss: -0.0323, took: 46.1748s
Batch 3000/3907, reward: 3.795, loss: -0.0394, took: 46.9168s
Batch 3100/3907, reward: 3.796, loss: -0.0688, took: 47.0023s
Batch 3200/3907, reward: 3.795, loss: -0.0446, took: 48.2656s
Batch 3300/3907, reward: 3.796, loss: -0.0456, took: 47.9927s
Batch 3400/3907, reward: 3.799, loss: -0.0441, took: 47.0229s
Batch 3500/3907, reward: 3.795, loss: -0.0425, took: 46.3656s
Batch 3600/3907, reward: 3.794, loss: -0.0478, took: 47.4818s
Batch 3700/3907, reward: 3.798, loss: -0.0242, took: 46.6874s
Batch 3800/3907, reward: 3.794, loss: -0.0566, took: 49.5457s
Batch 3900/3907, reward: 3.795, loss: -0.0469, took: 48.8861s
Mean epoch loss/reward: -0.0408, 3.7979, 3.7880, took: 1867.9386s (46.3330s / 100 batches)
Batch 0/3907, reward: 3.780, loss: 0.1074, took: 0.6571s
Batch 100/3907, reward: 3.800, loss: -0.0331, took: 49.1008s
Batch 200/3907, reward: 3.799, loss: -0.0470, took: 47.4051s
Batch 300/3907, reward: 3.797, loss: -0.0390, took: 47.7053s
Batch 400/3907, reward: 3.794, loss: -0.0461, took: 48.4148s
Batch 500/3907, reward: 3.795, loss: -0.0347, took: 48.7523s
Batch 600/3907, reward: 3.796, loss: -0.0481, took: 48.6927s
Batch 700/3907, reward: 3.800, loss: -0.0477, took: 48.1804s
Batch 800/3907, reward: 3.796, loss: -0.0217, took: 48.2431s
Batch 900/3907, reward: 3.804, loss: -0.0476, took: 48.7385s
Batch 1000/3907, reward: 3.801, loss: -0.0339, took: 49.2072s
Batch 1100/3907, reward: 3.803, loss: -0.0266, took: 45.7643s
Batch 1200/3907, reward: 3.795, loss: -0.0360, took: 46.8513s
Batch 1300/3907, reward: 3.791, loss: -0.0379, took: 48.0569s
Batch 1400/3907, reward: 3.800, loss: -0.0234, took: 46.7308s
Batch 1500/3907, reward: 3.797, loss: -0.0297, took: 47.9838s
Batch 1600/3907, reward: 3.797, loss: -0.0358, took: 47.7647s
Batch 1700/3907, reward: 3.796, loss: -0.0404, took: 47.2670s
Batch 1800/3907, reward: 3.797, loss: -0.0290, took: 48.1000s
Batch 1900/3907, reward: 3.793, loss: -0.0513, took: 48.2056s
Batch 2000/3907, reward: 3.797, loss: -0.0613, took: 48.2843s
Batch 2100/3907, reward: 3.792, loss: -0.0417, took: 48.0130s
Batch 2200/3907, reward: 3.793, loss: -0.0416, took: 44.2303s
Batch 2300/3907, reward: 3.797, loss: -0.0397, took: 47.6808s
Batch 2400/3907, reward: 3.794, loss: -0.0357, took: 47.8611s
Batch 2500/3907, reward: 3.799, loss: -0.0347, took: 47.8815s
Batch 2600/3907, reward: 3.801, loss: -0.0125, took: 45.6455s
Batch 2700/3907, reward: 3.792, loss: -0.0419, took: 47.1083s
Batch 2800/3907, reward: 3.796, loss: -0.0305, took: 48.3873s
Batch 2900/3907, reward: 3.796, loss: -0.0466, took: 46.6625s
Batch 3000/3907, reward: 3.792, loss: -0.0447, took: 47.9266s
Batch 3100/3907, reward: 3.798, loss: -0.0317, took: 48.0227s
Batch 3200/3907, reward: 3.793, loss: -0.0337, took: 48.5473s
Batch 3300/3907, reward: 3.794, loss: -0.0364, took: 47.5906s
Batch 3400/3907, reward: 3.796, loss: -0.0435, took: 48.8631s
Batch 3500/3907, reward: 3.797, loss: -0.0309, took: 48.3647s
Batch 3600/3907, reward: 3.799, loss: -0.0332, took: 48.8519s
Batch 3700/3907, reward: 3.792, loss: -0.0402, took: 47.6766s
Batch 3800/3907, reward: 3.796, loss: -0.0280, took: 47.7201s
Batch 3900/3907, reward: 3.794, loss: -0.0230, took: 47.5898s
Mean epoch loss/reward: -0.0369, 3.7963, 3.7834, took: 1878.6858s (46.6182s / 100 batches)
Batch 0/3907, reward: 3.779, loss: -0.0930, took: 0.6469s
Batch 100/3907, reward: 3.798, loss: -0.0374, took: 47.0503s
Batch 200/3907, reward: 3.793, loss: -0.0473, took: 48.2880s
Batch 300/3907, reward: 3.797, loss: -0.0350, took: 48.1240s
Batch 400/3907, reward: 3.798, loss: -0.0387, took: 47.4307s
Batch 500/3907, reward: 3.798, loss: -0.0472, took: 48.4857s
Batch 600/3907, reward: 3.792, loss: -0.0274, took: 47.9696s
Batch 700/3907, reward: 3.793, loss: -0.0408, took: 47.4582s
Batch 800/3907, reward: 3.796, loss: -0.0330, took: 47.3687s
Batch 900/3907, reward: 3.797, loss: -0.0300, took: 49.1289s
Batch 1000/3907, reward: 3.803, loss: -0.0314, took: 47.5323s
Batch 1100/3907, reward: 3.794, loss: -0.0388, took: 44.7792s
Batch 1200/3907, reward: 3.798, loss: -0.0434, took: 46.2274s
Batch 1300/3907, reward: 3.791, loss: -0.0430, took: 48.1254s
Batch 1400/3907, reward: 3.798, loss: -0.0292, took: 48.1922s
Batch 1500/3907, reward: 3.795, loss: -0.0426, took: 47.6937s
Batch 1600/3907, reward: 3.790, loss: -0.0176, took: 47.4449s
Batch 1700/3907, reward: 3.789, loss: -0.0461, took: 48.6640s
Batch 1800/3907, reward: 3.795, loss: -0.0426, took: 48.1513s
Batch 1900/3907, reward: 3.790, loss: -0.0412, took: 47.5416s
Batch 2000/3907, reward: 3.792, loss: -0.0257, took: 47.8261s
Batch 2100/3907, reward: 3.798, loss: -0.0481, took: 46.9343s
Batch 2200/3907, reward: 3.803, loss: -0.0460, took: 45.5023s
Batch 2300/3907, reward: 3.791, loss: -0.0314, took: 48.9692s
Batch 2400/3907, reward: 3.796, loss: -0.0380, took: 47.9319s
Batch 2500/3907, reward: 3.797, loss: -0.0469, took: 46.4279s
Batch 2600/3907, reward: 3.794, loss: -0.0272, took: 44.9714s
Batch 2700/3907, reward: 3.792, loss: -0.0420, took: 48.8137s
Batch 2800/3907, reward: 3.793, loss: -0.0351, took: 48.7916s
Batch 2900/3907, reward: 3.794, loss: -0.0270, took: 44.5607s
Batch 3000/3907, reward: 3.793, loss: -0.0413, took: 48.0841s
Batch 3100/3907, reward: 3.792, loss: -0.0312, took: 48.4869s
Batch 3200/3907, reward: 3.794, loss: -0.0257, took: 46.8111s
Batch 3300/3907, reward: 3.797, loss: -0.0357, took: 47.7092s
Batch 3400/3907, reward: 3.797, loss: -0.0598, took: 48.6005s
Batch 3500/3907, reward: 3.792, loss: -0.0466, took: 48.4668s
Batch 3600/3907, reward: 3.795, loss: -0.0463, took: 46.9858s
Batch 3700/3907, reward: 3.793, loss: -0.0290, took: 49.3905s
Batch 3800/3907, reward: 3.795, loss: -0.0182, took: 47.8828s
Batch 3900/3907, reward: 3.787, loss: -0.0405, took: 48.8218s
Mean epoch loss/reward: -0.0371, 3.7946, 3.7822, took: 1872.3359s (46.4568s / 100 batches)
Batch 0/3907, reward: 3.741, loss: 0.1255, took: 0.7188s
Batch 100/3907, reward: 3.794, loss: -0.0518, took: 47.6078s
Batch 200/3907, reward: 3.792, loss: -0.0250, took: 47.5193s
Batch 300/3907, reward: 3.789, loss: -0.0410, took: 48.3890s
Batch 400/3907, reward: 3.792, loss: -0.0349, took: 47.9197s
Batch 500/3907, reward: 3.797, loss: -0.0140, took: 48.5525s
Batch 600/3907, reward: 3.794, loss: -0.0440, took: 47.8755s
Batch 700/3907, reward: 3.796, loss: -0.0390, took: 48.3983s
Batch 800/3907, reward: 3.793, loss: -0.0379, took: 47.7093s
Batch 900/3907, reward: 3.796, loss: -0.0305, took: 47.6766s
Batch 1000/3907, reward: 3.793, loss: -0.0296, took: 47.6961s
Batch 1100/3907, reward: 3.795, loss: -0.0305, took: 44.7056s
Batch 1200/3907, reward: 3.791, loss: -0.0365, took: 47.5237s
Batch 1300/3907, reward: 3.789, loss: -0.0191, took: 47.8541s
Batch 1400/3907, reward: 3.794, loss: -0.0438, took: 47.4865s
Batch 1500/3907, reward: 3.790, loss: -0.0234, took: 47.0632s
Batch 1600/3907, reward: 3.791, loss: -0.0389, took: 48.4533s
Batch 1700/3907, reward: 3.794, loss: -0.0337, took: 48.7671s
Batch 1800/3907, reward: 3.790, loss: -0.0408, took: 46.5774s
Batch 1900/3907, reward: 3.792, loss: -0.0260, took: 48.8300s
Batch 2000/3907, reward: 3.789, loss: -0.0191, took: 48.5241s
Batch 2100/3907, reward: 3.789, loss: -0.0311, took: 47.4053s
Batch 2200/3907, reward: 3.790, loss: -0.0412, took: 44.3145s
Batch 2300/3907, reward: 3.790, loss: -0.0297, took: 46.9001s
Batch 2400/3907, reward: 3.788, loss: -0.0385, took: 48.9565s
Batch 2500/3907, reward: 3.797, loss: -0.0377, took: 48.3281s
Batch 2600/3907, reward: 3.788, loss: -0.0240, took: 45.9595s
Batch 2700/3907, reward: 3.797, loss: -0.0276, took: 47.6013s
Batch 2800/3907, reward: 3.797, loss: -0.0327, took: 46.4647s
Batch 2900/3907, reward: 3.792, loss: -0.0387, took: 44.8494s
Batch 3000/3907, reward: 3.798, loss: -0.0403, took: 47.0835s
Batch 3100/3907, reward: 3.792, loss: -0.0472, took: 46.7813s
Batch 3200/3907, reward: 3.797, loss: -0.0424, took: 47.1706s
Batch 3300/3907, reward: 3.798, loss: -0.0360, took: 47.9336s
Batch 3400/3907, reward: 3.794, loss: -0.0268, took: 47.0108s
Batch 3500/3907, reward: 3.794, loss: -0.0363, took: 46.2413s
Batch 3600/3907, reward: 3.794, loss: -0.0372, took: 47.3740s
Batch 3700/3907, reward: 3.791, loss: -0.0440, took: 48.2078s
Batch 3800/3907, reward: 3.795, loss: -0.0275, took: 46.9857s
Batch 3900/3907, reward: 3.794, loss: -0.0251, took: 47.8299s
Mean epoch loss/reward: -0.0339, 3.7929, 3.7802, took: 1863.2305s (46.2311s / 100 batches)
Batch 0/3907, reward: 3.831, loss: -0.0866, took: 0.7495s
Batch 100/3907, reward: 3.791, loss: -0.0430, took: 48.7756s
Batch 200/3907, reward: 3.791, loss: -0.0391, took: 47.1143s
Batch 300/3907, reward: 3.797, loss: -0.0255, took: 47.8924s
Batch 400/3907, reward: 3.789, loss: -0.0356, took: 46.6526s
Batch 500/3907, reward: 3.793, loss: -0.0310, took: 47.5052s
Batch 600/3907, reward: 3.790, loss: -0.0322, took: 47.8571s
Batch 700/3907, reward: 3.792, loss: -0.0428, took: 47.3970s
Batch 800/3907, reward: 3.792, loss: -0.0268, took: 48.3173s
Batch 900/3907, reward: 3.796, loss: -0.0264, took: 47.4497s
Batch 1000/3907, reward: 3.794, loss: -0.0390, took: 47.5609s
Batch 1100/3907, reward: 3.799, loss: -0.0377, took: 48.2370s
Batch 1200/3907, reward: 3.794, loss: -0.0209, took: 46.8849s
Batch 1300/3907, reward: 3.796, loss: -0.0331, took: 47.1032s
Batch 1400/3907, reward: 3.793, loss: -0.0407, took: 47.3128s
Batch 1500/3907, reward: 3.794, loss: -0.0277, took: 47.1585s
Batch 1600/3907, reward: 3.792, loss: -0.0264, took: 47.8116s
Batch 1700/3907, reward: 3.785, loss: -0.0363, took: 47.9183s
Batch 1800/3907, reward: 3.791, loss: -0.0292, took: 49.0802s
Batch 1900/3907, reward: 3.791, loss: -0.0186, took: 48.5000s
Batch 2000/3907, reward: 3.795, loss: -0.0306, took: 47.9743s
Batch 2100/3907, reward: 3.790, loss: -0.0373, took: 48.1980s
Batch 2200/3907, reward: 3.787, loss: -0.0382, took: 44.3667s
Batch 2300/3907, reward: 3.786, loss: -0.0306, took: 48.4620s
Batch 2400/3907, reward: 3.786, loss: -0.0407, took: 47.9670s
Batch 2500/3907, reward: 3.792, loss: -0.0292, took: 46.8164s
Batch 2600/3907, reward: 3.788, loss: -0.0266, took: 45.0314s
Batch 2700/3907, reward: 3.790, loss: -0.0381, took: 47.8145s
Batch 2800/3907, reward: 3.788, loss: -0.0449, took: 47.8639s
Batch 2900/3907, reward: 3.792, loss: -0.0220, took: 45.4926s
Batch 3000/3907, reward: 3.787, loss: -0.0404, took: 47.9371s
Batch 3100/3907, reward: 3.789, loss: -0.0328, took: 46.8641s
Batch 3200/3907, reward: 3.785, loss: -0.0306, took: 47.2374s
Batch 3300/3907, reward: 3.789, loss: -0.0288, took: 47.3702s
Batch 3400/3907, reward: 3.787, loss: -0.0276, took: 47.8938s
Batch 3500/3907, reward: 3.793, loss: -0.0360, took: 47.3659s
Batch 3600/3907, reward: 3.792, loss: -0.0324, took: 47.3458s
Batch 3700/3907, reward: 3.788, loss: -0.0358, took: 48.8746s
Batch 3800/3907, reward: 3.786, loss: -0.0258, took: 48.0977s
Batch 3900/3907, reward: 3.791, loss: -0.0478, took: 47.9411s
Mean epoch loss/reward: -0.0331, 3.7908, 3.7778, took: 1867.9677s (46.3548s / 100 batches)
Batch 0/3907, reward: 3.812, loss: -0.0656, took: 0.6232s
Batch 100/3907, reward: 3.788, loss: -0.0353, took: 48.3592s
Batch 200/3907, reward: 3.789, loss: -0.0247, took: 48.6502s
Batch 300/3907, reward: 3.788, loss: -0.0256, took: 48.0833s
Batch 400/3907, reward: 3.790, loss: -0.0361, took: 48.6569s
Batch 500/3907, reward: 3.791, loss: -0.0325, took: 47.0906s
Batch 600/3907, reward: 3.793, loss: -0.0181, took: 47.3811s
Batch 700/3907, reward: 3.791, loss: -0.0296, took: 48.3759s
Batch 800/3907, reward: 3.793, loss: -0.0282, took: 49.0560s
Batch 900/3907, reward: 3.788, loss: -0.0318, took: 47.7360s
Batch 1000/3907, reward: 3.794, loss: -0.0324, took: 46.9922s
Batch 1100/3907, reward: 3.788, loss: -0.0342, took: 48.4348s
Batch 1200/3907, reward: 3.792, loss: -0.0326, took: 43.6980s
Batch 1300/3907, reward: 3.789, loss: -0.0223, took: 46.9883s
Batch 1400/3907, reward: 3.794, loss: -0.0342, took: 48.6097s
Batch 1500/3907, reward: 3.791, loss: -0.0372, took: 47.9011s
Batch 1600/3907, reward: 3.786, loss: -0.0238, took: 47.9649s
Batch 1700/3907, reward: 3.788, loss: -0.0246, took: 47.6980s
Batch 1800/3907, reward: 3.794, loss: -0.0299, took: 48.5775s
Batch 1900/3907, reward: 3.785, loss: -0.0371, took: 47.2686s
Batch 2000/3907, reward: 3.798, loss: -0.0264, took: 47.3625s
Batch 2100/3907, reward: 3.791, loss: -0.0274, took: 47.9303s
Batch 2200/3907, reward: 3.794, loss: -0.0131, took: 44.4819s
Batch 2300/3907, reward: 3.790, loss: -0.0224, took: 46.9732s
Batch 2400/3907, reward: 3.787, loss: -0.0245, took: 47.9228s
Batch 2500/3907, reward: 3.789, loss: -0.0278, took: 48.8040s
Batch 2600/3907, reward: 3.783, loss: -0.0343, took: 45.1404s
Batch 2700/3907, reward: 3.791, loss: -0.0259, took: 48.5109s
Batch 2800/3907, reward: 3.789, loss: -0.0227, took: 47.0115s
Batch 2900/3907, reward: 3.787, loss: -0.0264, took: 45.6891s
Batch 3000/3907, reward: 3.784, loss: -0.0336, took: 47.1972s
Batch 3100/3907, reward: 3.793, loss: -0.0391, took: 48.0934s
Batch 3200/3907, reward: 3.793, loss: -0.0298, took: 48.9016s
Batch 3300/3907, reward: 3.790, loss: -0.0313, took: 48.3681s
Batch 3400/3907, reward: 3.794, loss: -0.0246, took: 48.2864s
Batch 3500/3907, reward: 3.792, loss: -0.0333, took: 47.7566s
Batch 3600/3907, reward: 3.788, loss: -0.0321, took: 47.0625s
Batch 3700/3907, reward: 3.786, loss: -0.0375, took: 47.0541s
Batch 3800/3907, reward: 3.789, loss: -0.0396, took: 48.8598s
Batch 3900/3907, reward: 3.791, loss: -0.0310, took: 47.5424s
Mean epoch loss/reward: -0.0295, 3.7900, 3.7749, took: 1871.4604s (46.4274s / 100 batches)
Batch 0/3907, reward: 3.749, loss: 0.0731, took: 0.5748s
Batch 100/3907, reward: 3.785, loss: -0.0347, took: 48.6556s
Batch 200/3907, reward: 3.788, loss: -0.0337, took: 48.2787s
Batch 300/3907, reward: 3.786, loss: -0.0388, took: 48.1542s
Batch 400/3907, reward: 3.787, loss: -0.0267, took: 46.8101s
Batch 500/3907, reward: 3.790, loss: -0.0359, took: 48.8610s
Batch 600/3907, reward: 3.790, loss: -0.0341, took: 48.7046s
Batch 700/3907, reward: 3.791, loss: -0.0269, took: 48.4811s
Batch 800/3907, reward: 3.796, loss: -0.0373, took: 47.4485s
Batch 900/3907, reward: 3.789, loss: -0.0281, took: 48.0418s
Batch 1000/3907, reward: 3.788, loss: -0.0289, took: 47.8900s
Batch 1100/3907, reward: 3.787, loss: -0.0346, took: 48.9453s
Batch 1200/3907, reward: 3.792, loss: -0.0186, took: 45.3822s
Batch 1300/3907, reward: 3.790, loss: -0.0184, took: 48.7962s
Batch 1400/3907, reward: 3.792, loss: -0.0273, took: 48.0071s
Batch 1500/3907, reward: 3.788, loss: -0.0374, took: 47.2419s
Batch 1600/3907, reward: 3.787, loss: -0.0349, took: 47.7238s
Batch 1700/3907, reward: 3.788, loss: -0.0309, took: 47.8774s
Batch 1800/3907, reward: 3.790, loss: -0.0318, took: 48.1937s
Batch 1900/3907, reward: 3.790, loss: -0.0253, took: 48.0915s
Batch 2000/3907, reward: 3.789, loss: -0.0392, took: 47.4536s
Batch 2100/3907, reward: 3.790, loss: -0.0270, took: 48.3651s
Batch 2200/3907, reward: 3.791, loss: -0.0329, took: 45.7554s
Batch 2300/3907, reward: 3.789, loss: -0.0288, took: 48.6122s
Batch 2400/3907, reward: 3.785, loss: -0.0361, took: 46.5695s
Batch 2500/3907, reward: 3.789, loss: -0.0242, took: 48.3661s
Batch 2600/3907, reward: 3.789, loss: -0.0376, took: 45.1984s
Batch 2700/3907, reward: 3.784, loss: -0.0386, took: 48.3484s
Batch 2800/3907, reward: 3.786, loss: -0.0270, took: 47.0358s
Batch 2900/3907, reward: 3.789, loss: -0.0412, took: 46.5756s
Batch 3000/3907, reward: 3.791, loss: -0.0342, took: 48.1907s
Batch 3100/3907, reward: 3.787, loss: -0.0281, took: 47.5442s
Batch 3200/3907, reward: 3.786, loss: -0.0212, took: 48.6186s
Batch 3300/3907, reward: 3.793, loss: -0.0324, took: 47.5793s
Batch 3400/3907, reward: 3.791, loss: -0.0411, took: 47.2347s
Batch 3500/3907, reward: 3.787, loss: -0.0263, took: 47.5953s
Batch 3600/3907, reward: 3.790, loss: -0.0242, took: 48.2645s
Batch 3700/3907, reward: 3.791, loss: -0.0316, took: 48.1123s
Batch 3800/3907, reward: 3.784, loss: -0.0355, took: 47.3697s
Batch 3900/3907, reward: 3.785, loss: -0.0340, took: 47.4911s
Mean epoch loss/reward: -0.0314, 3.7887, 3.7726, took: 1875.5449s (46.5610s / 100 batches)
Batch 0/3907, reward: 3.818, loss: -0.0322, took: 0.5676s
Batch 100/3907, reward: 3.789, loss: -0.0269, took: 47.0340s
Batch 200/3907, reward: 3.787, loss: -0.0128, took: 48.3100s
Batch 300/3907, reward: 3.793, loss: -0.0360, took: 47.3660s
Batch 400/3907, reward: 3.797, loss: -0.0296, took: 47.9686s
Batch 500/3907, reward: 3.792, loss: -0.0337, took: 47.2479s
Batch 600/3907, reward: 3.785, loss: -0.0285, took: 46.8318s
Batch 700/3907, reward: 3.788, loss: -0.0364, took: 47.5608s
Batch 800/3907, reward: 3.786, loss: -0.0342, took: 47.6318s
Batch 900/3907, reward: 3.782, loss: -0.0184, took: 47.6979s
Batch 1000/3907, reward: 3.789, loss: -0.0269, took: 47.1628s
Batch 1100/3907, reward: 3.790, loss: -0.0154, took: 48.3768s
Batch 1200/3907, reward: 3.790, loss: -0.0299, took: 46.1128s
Batch 1300/3907, reward: 3.792, loss: -0.0253, took: 48.0048s
Batch 1400/3907, reward: 3.797, loss: -0.0316, took: 46.9280s
Batch 1500/3907, reward: 3.789, loss: -0.0241, took: 47.5561s
Batch 1600/3907, reward: 3.791, loss: -0.0266, took: 47.9447s
Batch 1700/3907, reward: 3.786, loss: -0.0275, took: 46.9746s
Batch 1800/3907, reward: 3.787, loss: -0.0305, took: 48.0311s
Batch 1900/3907, reward: 3.783, loss: -0.0307, took: 47.2323s
Batch 2000/3907, reward: 3.789, loss: -0.0309, took: 47.6030s
Batch 2100/3907, reward: 3.790, loss: -0.0231, took: 47.9232s
Batch 2200/3907, reward: 3.787, loss: -0.0320, took: 45.6185s
Batch 2300/3907, reward: 3.792, loss: -0.0281, took: 46.4990s
Batch 2400/3907, reward: 3.789, loss: -0.0344, took: 47.9205s
Batch 2500/3907, reward: 3.792, loss: -0.0260, took: 47.2195s
Batch 2600/3907, reward: 3.796, loss: -0.0252, took: 46.3117s
Batch 2700/3907, reward: 3.796, loss: -0.0289, took: 48.2748s
Batch 2800/3907, reward: 3.788, loss: -0.0189, took: 46.9902s
Batch 2900/3907, reward: 3.788, loss: -0.0388, took: 44.2533s
Batch 3000/3907, reward: 3.783, loss: -0.0368, took: 47.7067s
Batch 3100/3907, reward: 3.784, loss: -0.0299, took: 47.2690s
Batch 3200/3907, reward: 3.786, loss: -0.0261, took: 46.8457s
Batch 3300/3907, reward: 3.789, loss: -0.0327, took: 47.7373s
Batch 3400/3907, reward: 3.788, loss: -0.0253, took: 48.1008s
Batch 3500/3907, reward: 3.785, loss: -0.0175, took: 47.4931s
Batch 3600/3907, reward: 3.785, loss: -0.0272, took: 47.1673s
Batch 3700/3907, reward: 3.787, loss: -0.0270, took: 47.3983s
Batch 3800/3907, reward: 3.787, loss: -0.0349, took: 46.6211s
Batch 3900/3907, reward: 3.792, loss: -0.0325, took: 47.5799s
Mean epoch loss/reward: -0.0282, 3.7889, 3.7761, took: 1859.7107s (46.1268s / 100 batches)
Batch 0/3907, reward: 3.808, loss: -0.1705, took: 0.8261s
Batch 100/3907, reward: 3.787, loss: -0.0236, took: 47.4816s
Batch 200/3907, reward: 3.790, loss: -0.0268, took: 47.4111s
Batch 300/3907, reward: 3.789, loss: -0.0296, took: 48.0650s
Batch 400/3907, reward: 3.795, loss: -0.0240, took: 46.5949s
Batch 500/3907, reward: 3.790, loss: -0.0393, took: 47.5603s
Batch 600/3907, reward: 3.791, loss: -0.0344, took: 47.5463s
Batch 700/3907, reward: 3.782, loss: -0.0188, took: 47.0993s
Batch 800/3907, reward: 3.787, loss: -0.0302, took: 48.5304s
Batch 900/3907, reward: 3.785, loss: -0.0249, took: 48.5954s
Batch 1000/3907, reward: 3.786, loss: -0.0318, took: 48.7591s
Batch 1100/3907, reward: 3.791, loss: -0.0178, took: 47.7112s
Batch 1200/3907, reward: 3.789, loss: -0.0249, took: 45.5052s
Batch 1300/3907, reward: 3.790, loss: -0.0222, took: 47.3933s
Batch 1400/3907, reward: 3.790, loss: -0.0297, took: 48.8413s
Batch 1500/3907, reward: 3.790, loss: -0.0270, took: 48.6537s
Batch 1600/3907, reward: 3.788, loss: -0.0434, took: 47.4803s
Batch 1700/3907, reward: 3.795, loss: -0.0329, took: 46.7140s
Batch 1800/3907, reward: 3.791, loss: -0.0360, took: 48.6022s
Batch 1900/3907, reward: 3.785, loss: -0.0348, took: 47.0154s
Batch 2000/3907, reward: 3.787, loss: -0.0360, took: 48.2621s
Batch 2100/3907, reward: 3.788, loss: -0.0375, took: 47.4849s
Batch 2200/3907, reward: 3.783, loss: -0.0375, took: 45.4709s
Batch 2300/3907, reward: 3.790, loss: -0.0213, took: 48.1572s
Batch 2400/3907, reward: 3.788, loss: -0.0294, took: 48.6706s
Batch 2500/3907, reward: 3.789, loss: -0.0241, took: 48.5582s
Batch 2600/3907, reward: 3.780, loss: -0.0342, took: 44.1172s
Batch 2700/3907, reward: 3.795, loss: -0.0206, took: 47.5420s
Batch 2800/3907, reward: 3.788, loss: -0.0234, took: 47.9584s
Batch 2900/3907, reward: 3.784, loss: -0.0352, took: 44.9885s
Batch 3000/3907, reward: 3.784, loss: -0.0262, took: 49.1624s
Batch 3100/3907, reward: 3.784, loss: -0.0250, took: 48.0518s
Batch 3200/3907, reward: 3.788, loss: -0.0427, took: 47.5441s
Batch 3300/3907, reward: 3.791, loss: -0.0265, took: 48.5610s
Batch 3400/3907, reward: 3.789, loss: -0.0293, took: 48.4228s
Batch 3500/3907, reward: 3.787, loss: -0.0243, took: 48.1662s
Batch 3600/3907, reward: 3.788, loss: -0.0273, took: 47.3489s
Batch 3700/3907, reward: 3.786, loss: -0.0288, took: 48.4016s
Batch 3800/3907, reward: 3.784, loss: -0.0350, took: 46.3625s
Batch 3900/3907, reward: 3.785, loss: -0.0222, took: 47.3250s
Mean epoch loss/reward: -0.0292, 3.7880, 3.7721, took: 1870.8939s (46.4236s / 100 batches)
Batch 0/3907, reward: 3.769, loss: -0.1883, took: 0.6882s
Batch 100/3907, reward: 3.784, loss: -0.0281, took: 47.2522s
Batch 200/3907, reward: 3.787, loss: -0.0281, took: 47.5398s
Batch 300/3907, reward: 3.790, loss: -0.0274, took: 47.3815s
Batch 400/3907, reward: 3.785, loss: -0.0143, took: 47.9990s
Batch 500/3907, reward: 3.792, loss: -0.0329, took: 49.7711s
Batch 600/3907, reward: 3.792, loss: -0.0348, took: 47.2052s
Batch 700/3907, reward: 3.786, loss: -0.0270, took: 49.1967s
Batch 800/3907, reward: 3.790, loss: -0.0202, took: 48.9244s
Batch 900/3907, reward: 3.788, loss: -0.0277, took: 47.4630s
Batch 1000/3907, reward: 3.788, loss: -0.0295, took: 48.0593s
Batch 1100/3907, reward: 3.788, loss: -0.0333, took: 47.4460s
Batch 1200/3907, reward: 3.791, loss: -0.0282, took: 47.7024s
Batch 1300/3907, reward: 3.789, loss: -0.0285, took: 47.1866s
Batch 1400/3907, reward: 3.789, loss: -0.0238, took: 47.7320s
Batch 1500/3907, reward: 3.783, loss: -0.0270, took: 47.7847s
Batch 1600/3907, reward: 3.788, loss: -0.0358, took: 48.9475s
Batch 1700/3907, reward: 3.781, loss: -0.0254, took: 48.0594s
Batch 1800/3907, reward: 3.783, loss: -0.0301, took: 47.6479s
Batch 1900/3907, reward: 3.783, loss: -0.0177, took: 47.4925s
Batch 2000/3907, reward: 3.785, loss: -0.0238, took: 48.5941s
Batch 2100/3907, reward: 3.786, loss: -0.0215, took: 47.6164s
Batch 2200/3907, reward: 3.783, loss: -0.0351, took: 45.6481s
Batch 2300/3907, reward: 3.790, loss: -0.0260, took: 47.1121s
Batch 2400/3907, reward: 3.788, loss: -0.0276, took: 46.6842s
Batch 2500/3907, reward: 3.788, loss: -0.0293, took: 47.4932s
Batch 2600/3907, reward: 3.789, loss: -0.0312, took: 45.9449s
Batch 2700/3907, reward: 3.786, loss: -0.0241, took: 47.2483s
Batch 2800/3907, reward: 3.787, loss: -0.0254, took: 47.3311s
Batch 2900/3907, reward: 3.786, loss: -0.0375, took: 46.9477s
Batch 3000/3907, reward: 3.783, loss: -0.0276, took: 47.9317s
Batch 3100/3907, reward: 3.791, loss: -0.0259, took: 47.1801s
Batch 3200/3907, reward: 3.784, loss: -0.0246, took: 47.8075s
Batch 3300/3907, reward: 3.785, loss: -0.0223, took: 46.4280s
Batch 3400/3907, reward: 3.784, loss: -0.0236, took: 48.0252s
Batch 3500/3907, reward: 3.785, loss: -0.0262, took: 47.8552s
Batch 3600/3907, reward: 3.789, loss: -0.0223, took: 47.5419s
Batch 3700/3907, reward: 3.783, loss: -0.0234, took: 48.6985s
Batch 3800/3907, reward: 3.784, loss: -0.0284, took: 48.7220s
Batch 3900/3907, reward: 3.785, loss: -0.0323, took: 47.9655s
Mean epoch loss/reward: -0.0271, 3.7866, 3.7740, took: 1875.2597s (46.5064s / 100 batches)
Batch 0/3907, reward: 3.768, loss: 0.0761, took: 0.6487s
Batch 100/3907, reward: 3.783, loss: -0.0374, took: 48.2517s
Batch 200/3907, reward: 3.787, loss: -0.0184, took: 48.6918s
Batch 300/3907, reward: 3.789, loss: -0.0242, took: 46.6163s
Batch 400/3907, reward: 3.780, loss: -0.0270, took: 47.8648s
Batch 500/3907, reward: 3.790, loss: -0.0206, took: 47.6439s
Batch 600/3907, reward: 3.782, loss: -0.0259, took: 48.1856s
Batch 700/3907, reward: 3.779, loss: -0.0164, took: 48.3807s
Batch 800/3907, reward: 3.802, loss: -0.0317, took: 47.5353s
Batch 900/3907, reward: 3.789, loss: -0.0349, took: 47.6408s
Batch 1000/3907, reward: 3.784, loss: -0.0305, took: 47.8681s
Batch 1100/3907, reward: 3.783, loss: -0.0315, took: 45.1314s
Batch 1200/3907, reward: 3.786, loss: -0.0286, took: 47.2084s
Batch 1300/3907, reward: 3.791, loss: -0.0292, took: 47.1842s
Batch 1400/3907, reward: 3.784, loss: -0.0214, took: 48.4871s
Batch 1500/3907, reward: 3.787, loss: -0.0182, took: 47.8800s
Batch 1600/3907, reward: 3.787, loss: -0.0277, took: 48.9909s
Batch 1700/3907, reward: 3.788, loss: -0.0321, took: 48.4009s
Batch 1800/3907, reward: 3.788, loss: -0.0265, took: 48.3344s
Batch 1900/3907, reward: 3.787, loss: -0.0275, took: 48.2782s
Batch 2000/3907, reward: 3.786, loss: -0.0337, took: 48.6457s
Batch 2100/3907, reward: 3.791, loss: -0.0152, took: 47.0507s
Batch 2200/3907, reward: 3.787, loss: -0.0154, took: 45.0262s
Batch 2300/3907, reward: 3.792, loss: -0.0259, took: 48.0798s
Batch 2400/3907, reward: 3.782, loss: -0.0252, took: 48.2654s
Batch 2500/3907, reward: 3.779, loss: -0.0234, took: 47.7874s
Batch 2600/3907, reward: 3.787, loss: -0.0194, took: 43.5922s
Batch 2700/3907, reward: 3.784, loss: -0.0358, took: 38.9340s
Batch 2800/3907, reward: 3.785, loss: -0.0295, took: 36.9139s
Batch 2900/3907, reward: 3.781, loss: -0.0333, took: 37.9974s
Batch 3000/3907, reward: 3.786, loss: -0.0401, took: 42.9144s
Batch 3100/3907, reward: 3.787, loss: -0.0223, took: 47.6391s
Batch 3200/3907, reward: 3.786, loss: -0.0303, took: 48.1187s
Batch 3300/3907, reward: 3.791, loss: -0.0381, took: 48.1065s
Batch 3400/3907, reward: 3.787, loss: -0.0247, took: 47.2622s
Batch 3500/3907, reward: 3.783, loss: -0.0174, took: 46.7057s
Batch 3600/3907, reward: 3.789, loss: -0.0296, took: 47.3151s
Batch 3700/3907, reward: 3.786, loss: -0.0295, took: 47.5344s
Batch 3800/3907, reward: 3.784, loss: -0.0244, took: 47.6760s
Batch 3900/3907, reward: 3.780, loss: -0.0299, took: 48.1549s
Mean epoch loss/reward: -0.0270, 3.7861, 3.7751, took: 1837.4551s (45.5736s / 100 batches)
Batch 0/3907, reward: 3.768, loss: -0.0102, took: 0.5481s
Batch 100/3907, reward: 3.787, loss: -0.0260, took: 47.7512s
Batch 200/3907, reward: 3.789, loss: -0.0220, took: 46.3435s
Batch 300/3907, reward: 3.784, loss: -0.0363, took: 44.7618s
Batch 400/3907, reward: 3.788, loss: -0.0246, took: 37.6429s
Batch 500/3907, reward: 3.787, loss: -0.0337, took: 38.4232s
Batch 600/3907, reward: 3.788, loss: -0.0272, took: 37.7634s
Batch 700/3907, reward: 3.783, loss: -0.0278, took: 38.0416s
Batch 800/3907, reward: 3.783, loss: -0.0195, took: 37.7471s
Batch 900/3907, reward: 3.788, loss: -0.0369, took: 39.6035s
Batch 1000/3907, reward: 3.787, loss: -0.0247, took: 37.9290s
Batch 1100/3907, reward: 3.779, loss: -0.0274, took: 34.6217s
Batch 1200/3907, reward: 3.786, loss: -0.0237, took: 29.7402s
Batch 1300/3907, reward: 3.785, loss: -0.0169, took: 30.5529s
Batch 1400/3907, reward: 3.778, loss: -0.0278, took: 28.6886s
Batch 1500/3907, reward: 3.786, loss: -0.0252, took: 30.0733s
Batch 1600/3907, reward: 3.788, loss: -0.0176, took: 33.6289s
Batch 1700/3907, reward: 3.782, loss: -0.0133, took: 37.9534s
Batch 1800/3907, reward: 3.786, loss: -0.0277, took: 37.6977s
Batch 1900/3907, reward: 3.781, loss: -0.0316, took: 37.6994s
Batch 2000/3907, reward: 3.783, loss: -0.0253, took: 37.2832s
Batch 2100/3907, reward: 3.778, loss: -0.0293, took: 37.6628s
Batch 2200/3907, reward: 3.781, loss: -0.0205, took: 34.8387s
Batch 2300/3907, reward: 3.784, loss: -0.0231, took: 37.8985s
Batch 2400/3907, reward: 3.785, loss: -0.0303, took: 38.9815s
Batch 2500/3907, reward: 3.783, loss: -0.0296, took: 39.3306s
Batch 2600/3907, reward: 3.787, loss: -0.0286, took: 38.5458s
Batch 2700/3907, reward: 3.784, loss: -0.0241, took: 37.1336s
Batch 2800/3907, reward: 3.785, loss: -0.0286, took: 35.6670s
Batch 2900/3907, reward: 3.781, loss: -0.0202, took: 28.7469s
Batch 3000/3907, reward: 3.784, loss: -0.0299, took: 29.0851s
Batch 3100/3907, reward: 3.783, loss: -0.0265, took: 28.5593s
Batch 3200/3907, reward: 3.781, loss: -0.0252, took: 29.2763s
Batch 3300/3907, reward: 3.783, loss: -0.0244, took: 30.1938s
Batch 3400/3907, reward: 3.785, loss: -0.0360, took: 28.6281s
Batch 3500/3907, reward: 3.785, loss: -0.0222, took: 30.8067s
Batch 3600/3907, reward: 3.778, loss: -0.0216, took: 30.3272s
Batch 3700/3907, reward: 3.786, loss: -0.0280, took: 28.3614s
Batch 3800/3907, reward: 3.787, loss: -0.0233, took: 30.9085s
Batch 3900/3907, reward: 3.784, loss: -0.0266, took: 29.3494s
Mean epoch loss/reward: -0.0260, 3.7841, 3.7821, took: 1380.4828s (34.2199s / 100 batches)
Average tour length for uniform: 4.110219802031408
Average tour length for shifted: 3.1729205878823805
Average tour length for adversary: 2.8664990505172616
