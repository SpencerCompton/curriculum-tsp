Current device: cuda
Batch 0/3907, reward: 14.183, loss: -786.6501, took: 0.4822s
Batch 100/3907, reward: 6.705, loss: -14.5144, took: 29.1559s
Batch 200/3907, reward: 3.604, loss: 3.4945, took: 29.2598s
Batch 300/3907, reward: 3.534, loss: 2.6041, took: 29.3744s
Batch 400/3907, reward: 3.517, loss: 1.0330, took: 29.4696s
Batch 500/3907, reward: 3.509, loss: 1.1961, took: 29.3880s
Batch 600/3907, reward: 3.493, loss: 1.8504, took: 29.2519s
Batch 700/3907, reward: 3.493, loss: 1.5727, took: 29.6896s
Batch 800/3907, reward: 3.493, loss: -0.8861, took: 38.7067s
Batch 900/3907, reward: 3.492, loss: 0.1185, took: 38.8396s
Batch 1000/3907, reward: 3.488, loss: 0.1135, took: 37.0641s
Batch 1100/3907, reward: 3.476, loss: 0.0718, took: 37.9800s
Batch 1200/3907, reward: 3.483, loss: -0.7063, took: 38.6709s
Batch 1300/3907, reward: 3.483, loss: -0.5592, took: 38.7835s
Batch 1400/3907, reward: 3.488, loss: -0.1411, took: 38.7191s
Batch 1500/3907, reward: 3.480, loss: -1.2123, took: 38.7599s
Batch 1600/3907, reward: 3.479, loss: 0.3994, took: 35.7821s
Batch 1700/3907, reward: 3.474, loss: 0.1520, took: 38.4803s
Batch 1800/3907, reward: 3.475, loss: -0.5727, took: 38.7583s
Batch 1900/3907, reward: 3.461, loss: 0.7307, took: 38.7721s
Batch 2000/3907, reward: 3.456, loss: 0.8274, took: 38.5818s
Batch 2100/3907, reward: 3.448, loss: 0.3002, took: 38.5924s
Batch 2200/3907, reward: 3.444, loss: 0.7871, took: 38.6251s
Batch 2300/3907, reward: 3.445, loss: 0.6090, took: 38.5927s
Batch 2400/3907, reward: 3.438, loss: 0.6675, took: 38.7058s
Batch 2500/3907, reward: 3.438, loss: -0.2461, took: 39.0313s
Batch 2600/3907, reward: 3.440, loss: 1.1869, took: 38.7203s
Batch 2700/3907, reward: 3.439, loss: 0.1092, took: 38.7641s
Batch 2800/3907, reward: 3.438, loss: 0.1407, took: 38.7742s
Batch 2900/3907, reward: 3.436, loss: -0.0151, took: 38.7587s
Batch 3000/3907, reward: 3.433, loss: 0.7838, took: 38.6742s
Batch 3100/3907, reward: 3.427, loss: 0.6652, took: 38.6259s
Batch 3200/3907, reward: 3.432, loss: 0.0786, took: 38.4076s
Batch 3300/3907, reward: 3.425, loss: -0.1967, took: 38.5341s
Batch 3400/3907, reward: 3.426, loss: 0.2251, took: 38.4750s
Batch 3500/3907, reward: 3.425, loss: 0.5901, took: 38.5651s
Batch 3600/3907, reward: 3.422, loss: 0.0946, took: 38.5572s
Batch 3700/3907, reward: 3.422, loss: -0.2950, took: 38.6046s
Batch 3800/3907, reward: 3.423, loss: -0.4978, took: 38.5751s
Batch 3900/3907, reward: 3.424, loss: -0.7452, took: 38.6943s
Mean epoch loss/reward: -0.2057, 3.5488, 3.4217, took: 1450.2244s (35.9562s / 100 batches)
Batch 0/3907, reward: 3.428, loss: 3.6003, took: 0.5739s
Batch 100/3907, reward: 3.420, loss: 0.4910, took: 38.5442s
Batch 200/3907, reward: 3.415, loss: -0.2386, took: 38.5094s
Batch 300/3907, reward: 3.413, loss: -0.1763, took: 38.7556s
Batch 400/3907, reward: 3.415, loss: 0.5055, took: 38.8023s
Batch 500/3907, reward: 3.419, loss: -0.6696, took: 38.7163s
Batch 600/3907, reward: 3.417, loss: -0.4135, took: 38.6962s
Batch 700/3907, reward: 3.416, loss: -0.0423, took: 35.8193s
Batch 800/3907, reward: 3.418, loss: 0.3867, took: 38.8560s
Batch 900/3907, reward: 3.416, loss: 0.4422, took: 38.7707s
Batch 1000/3907, reward: 3.412, loss: 0.5007, took: 37.0466s
Batch 1100/3907, reward: 3.408, loss: -0.4692, took: 37.5632s
Batch 1200/3907, reward: 3.417, loss: -0.0285, took: 38.7156s
Batch 1300/3907, reward: 3.415, loss: 0.9704, took: 38.7739s
Batch 1400/3907, reward: 3.413, loss: 0.9949, took: 38.6938s
Batch 1500/3907, reward: 3.413, loss: -0.6383, took: 38.6816s
Batch 1600/3907, reward: 3.412, loss: -0.0009, took: 35.7065s
Batch 1700/3907, reward: 3.410, loss: -0.2508, took: 38.5904s
Batch 1800/3907, reward: 3.409, loss: -1.2870, took: 38.8280s
Batch 1900/3907, reward: 3.408, loss: -0.5055, took: 38.7108s
Batch 2000/3907, reward: 3.409, loss: 0.7465, took: 38.6490s
Batch 2100/3907, reward: 3.407, loss: -0.2187, took: 38.6072s
Batch 2200/3907, reward: 3.405, loss: 0.2392, took: 38.6580s
Batch 2300/3907, reward: 3.404, loss: -0.3042, took: 38.6561s
Batch 2400/3907, reward: 3.400, loss: 0.7455, took: 38.6799s
Batch 2500/3907, reward: 3.408, loss: 0.9946, took: 38.7842s
Batch 2600/3907, reward: 3.405, loss: 0.8085, took: 38.7968s
Batch 2700/3907, reward: 3.409, loss: 1.0542, took: 38.7831s
Batch 2800/3907, reward: 3.409, loss: 1.1028, took: 38.5163s
Batch 2900/3907, reward: 3.406, loss: 0.3711, took: 38.6200s
Batch 3000/3907, reward: 3.407, loss: -0.3025, took: 38.6175s
Batch 3100/3907, reward: 3.403, loss: 0.3004, took: 38.5951s
Batch 3200/3907, reward: 3.404, loss: 0.2300, took: 38.5319s
Batch 3300/3907, reward: 3.403, loss: -0.5185, took: 38.7608s
Batch 3400/3907, reward: 3.399, loss: 0.0186, took: 38.5730s
Batch 3500/3907, reward: 3.405, loss: 0.1552, took: 38.5345s
Batch 3600/3907, reward: 3.405, loss: 0.2489, took: 38.7295s
Batch 3700/3907, reward: 3.405, loss: 0.5771, took: 38.5362s
Batch 3800/3907, reward: 3.403, loss: 0.8507, took: 38.5609s
Batch 3900/3907, reward: 3.404, loss: 0.1342, took: 38.7175s
Mean epoch loss/reward: 0.1758, 3.4093, 3.4087, took: 1511.8625s (37.5065s / 100 batches)
Batch 0/3907, reward: 3.394, loss: 3.4401, took: 0.5185s
Batch 100/3907, reward: 3.402, loss: 0.2262, took: 38.6631s
Batch 200/3907, reward: 3.406, loss: -0.4744, took: 38.5290s
Batch 300/3907, reward: 3.405, loss: 0.3072, took: 38.6997s
Batch 400/3907, reward: 3.400, loss: -0.3204, took: 38.5463s
Batch 500/3907, reward: 3.409, loss: 0.0055, took: 38.5354s
Batch 600/3907, reward: 3.399, loss: 0.2675, took: 38.7239s
Batch 700/3907, reward: 3.403, loss: -0.1697, took: 35.8208s
Batch 800/3907, reward: 3.403, loss: -0.5764, took: 38.3292s
Batch 900/3907, reward: 3.401, loss: -0.0881, took: 38.7062s
Batch 1000/3907, reward: 3.402, loss: -0.0459, took: 37.1581s
Batch 1100/3907, reward: 3.402, loss: 0.7248, took: 37.3689s
Batch 1200/3907, reward: 3.401, loss: 0.6823, took: 38.5934s
Batch 1300/3907, reward: 3.401, loss: 0.2079, took: 38.6719s
Batch 1400/3907, reward: 3.406, loss: 0.1293, took: 38.6047s
Batch 1500/3907, reward: 3.403, loss: 0.8939, took: 38.6038s
Batch 1600/3907, reward: 3.403, loss: 0.0264, took: 35.6111s
Batch 1700/3907, reward: 3.401, loss: 0.2014, took: 38.6369s
Batch 1800/3907, reward: 3.400, loss: 0.5206, took: 38.4866s
Batch 1900/3907, reward: 3.398, loss: -0.2308, took: 38.5734s
Batch 2000/3907, reward: 3.405, loss: 0.1302, took: 38.4977s
Batch 2100/3907, reward: 3.405, loss: 0.5523, took: 38.5342s
Batch 2200/3907, reward: 3.395, loss: 0.5007, took: 38.5313s
Batch 2300/3907, reward: 3.399, loss: 0.8836, took: 38.3404s
Batch 2400/3907, reward: 3.398, loss: 0.7535, took: 38.3836s
Batch 2500/3907, reward: 3.398, loss: 0.9224, took: 38.0693s
Batch 2600/3907, reward: 3.397, loss: 0.0870, took: 38.3360s
Batch 2700/3907, reward: 3.397, loss: 0.1517, took: 38.3906s
Batch 2800/3907, reward: 3.398, loss: -0.1602, took: 38.5030s
Batch 2900/3907, reward: 3.395, loss: 0.0506, took: 38.4232s
Batch 3000/3907, reward: 3.339, loss: -0.0559, took: 38.4367s
Batch 3100/3907, reward: 3.262, loss: -0.0263, took: 38.5716s
Batch 3200/3907, reward: 3.247, loss: 0.2145, took: 38.4896s
Batch 3300/3907, reward: 3.237, loss: 0.0543, took: 38.3434s
Batch 3400/3907, reward: 3.228, loss: 0.2100, took: 38.4689s
Batch 3500/3907, reward: 3.225, loss: 0.1294, took: 38.4254s
Batch 3600/3907, reward: 3.224, loss: -0.1936, took: 38.2640s
Batch 3700/3907, reward: 3.221, loss: 0.1376, took: 38.2367s
Batch 3800/3907, reward: 3.218, loss: 0.0392, took: 38.1722s
Batch 3900/3907, reward: 3.215, loss: -0.1297, took: 38.1981s
Mean epoch loss/reward: 0.1683, 3.3600, 3.1794, took: 1504.8610s (37.3249s / 100 batches)
Batch 0/3907, reward: 3.218, loss: -1.7742, took: 0.5637s
Batch 100/3907, reward: 3.213, loss: 0.0706, took: 38.2729s
Batch 200/3907, reward: 3.210, loss: -0.0465, took: 37.9303s
Batch 300/3907, reward: 3.211, loss: -0.2614, took: 38.2263s
Batch 400/3907, reward: 3.208, loss: -0.5602, took: 38.2697s
Batch 500/3907, reward: 3.206, loss: -0.2893, took: 38.0322s
Batch 600/3907, reward: 3.208, loss: 0.1029, took: 38.0256s
Batch 700/3907, reward: 3.204, loss: 0.2139, took: 35.4167s
Batch 800/3907, reward: 3.207, loss: 0.9124, took: 38.1101s
Batch 900/3907, reward: 3.201, loss: -0.1383, took: 38.1259s
Batch 1000/3907, reward: 3.204, loss: 0.0401, took: 36.4736s
Batch 1100/3907, reward: 3.201, loss: -0.0102, took: 36.9899s
Batch 1200/3907, reward: 3.201, loss: 0.3281, took: 38.1558s
Batch 1300/3907, reward: 3.200, loss: -0.0338, took: 38.1246s
Batch 1400/3907, reward: 3.199, loss: -0.0168, took: 38.1256s
Batch 1500/3907, reward: 3.199, loss: 0.2152, took: 38.2720s
Batch 1600/3907, reward: 3.198, loss: -0.0045, took: 35.0129s
Batch 1700/3907, reward: 3.198, loss: -0.0164, took: 38.3427s
Batch 1800/3907, reward: 3.197, loss: 0.0644, took: 38.2844s
Batch 1900/3907, reward: 3.194, loss: 0.0360, took: 38.2761s
Batch 2000/3907, reward: 3.194, loss: 0.1607, took: 38.1445s
Batch 2100/3907, reward: 3.193, loss: 0.1339, took: 38.2003s
Batch 2200/3907, reward: 3.192, loss: 0.0067, took: 38.1323s
Batch 2300/3907, reward: 3.193, loss: 0.0301, took: 38.1758s
Batch 2400/3907, reward: 3.191, loss: -0.0205, took: 38.2930s
Batch 2500/3907, reward: 3.190, loss: -0.0686, took: 38.3168s
Batch 2600/3907, reward: 3.190, loss: 0.1484, took: 38.3578s
Batch 2700/3907, reward: 3.190, loss: 0.1585, took: 38.2352s
Batch 2800/3907, reward: 3.191, loss: 0.1199, took: 38.3056s
Batch 2900/3907, reward: 3.190, loss: 0.1639, took: 38.2609s
Batch 3000/3907, reward: 3.188, loss: 0.0344, took: 38.2281s
Batch 3100/3907, reward: 3.187, loss: -0.1130, took: 38.0966s
Batch 3200/3907, reward: 3.187, loss: -0.1955, took: 38.0836s
Batch 3300/3907, reward: 3.186, loss: -0.0361, took: 37.9541s
Batch 3400/3907, reward: 3.187, loss: -0.0467, took: 38.0526s
Batch 3500/3907, reward: 3.184, loss: 0.0740, took: 38.1394s
Batch 3600/3907, reward: 3.187, loss: 0.1206, took: 37.9497s
Batch 3700/3907, reward: 3.187, loss: 0.2284, took: 38.2917s
Batch 3800/3907, reward: 3.185, loss: -0.0351, took: 38.2022s
Batch 3900/3907, reward: 3.186, loss: 0.0952, took: 38.2365s
Mean epoch loss/reward: 0.0399, 3.1958, 3.1701, took: 1492.0545s (37.0172s / 100 batches)
Batch 0/3907, reward: 3.192, loss: 0.1103, took: 0.5165s
Batch 100/3907, reward: 3.184, loss: -0.1059, took: 38.3353s
Batch 200/3907, reward: 3.185, loss: 0.0678, took: 38.3351s
Batch 300/3907, reward: 3.185, loss: 0.0135, took: 38.4346s
Batch 400/3907, reward: 3.180, loss: 0.0421, took: 38.3872s
Batch 500/3907, reward: 3.182, loss: 0.0592, took: 38.4174s
Batch 600/3907, reward: 3.182, loss: 0.1390, took: 38.5014s
Batch 700/3907, reward: 3.183, loss: -0.0441, took: 35.5324s
Batch 800/3907, reward: 3.180, loss: 0.0607, took: 38.3963s
Batch 900/3907, reward: 3.182, loss: -0.0503, took: 38.2356s
Batch 1000/3907, reward: 3.182, loss: -0.0301, took: 36.3687s
Batch 1100/3907, reward: 3.180, loss: 0.0965, took: 36.9788s
Batch 1200/3907, reward: 3.187, loss: 0.2269, took: 38.3571s
Batch 1300/3907, reward: 3.189, loss: 0.0078, took: 38.0694s
Batch 1400/3907, reward: 3.182, loss: 0.0645, took: 38.1196s
Batch 1500/3907, reward: 3.183, loss: 0.1315, took: 38.1286s
Batch 1600/3907, reward: 3.181, loss: 0.0405, took: 35.1468s
Batch 1700/3907, reward: 3.182, loss: 0.0126, took: 38.2886s
Batch 1800/3907, reward: 3.182, loss: -0.0515, took: 38.0540s
Batch 1900/3907, reward: 3.182, loss: 0.0034, took: 38.0980s
Batch 2000/3907, reward: 3.182, loss: 0.0984, took: 38.2441s
Batch 2100/3907, reward: 3.182, loss: 0.1108, took: 38.3538s
Batch 2200/3907, reward: 3.181, loss: 0.0465, took: 38.3236s
Batch 2300/3907, reward: 3.180, loss: -0.0251, took: 38.4504s
Batch 2400/3907, reward: 3.180, loss: -0.0233, took: 38.1602s
Batch 2500/3907, reward: 3.177, loss: 0.0920, took: 38.0673s
Batch 2600/3907, reward: 3.179, loss: -0.0183, took: 38.1733s
Batch 2700/3907, reward: 3.177, loss: 0.0215, took: 38.1605s
Batch 2800/3907, reward: 3.178, loss: -0.0399, took: 38.3508s
Batch 2900/3907, reward: 3.179, loss: -0.0512, took: 37.9839s
Batch 3000/3907, reward: 3.177, loss: 0.0626, took: 38.0870s
Batch 3100/3907, reward: 3.180, loss: -0.0072, took: 38.0729s
Batch 3200/3907, reward: 3.179, loss: -0.0285, took: 37.9893s
Batch 3300/3907, reward: 3.180, loss: 0.1830, took: 38.0138s
Batch 3400/3907, reward: 3.178, loss: -0.0138, took: 38.3264s
Batch 3500/3907, reward: 3.177, loss: -0.0379, took: 38.1665s
Batch 3600/3907, reward: 3.174, loss: -0.0825, took: 38.1223s
Batch 3700/3907, reward: 3.176, loss: 0.0480, took: 38.2727s
Batch 3800/3907, reward: 3.176, loss: 0.0376, took: 38.1660s
Batch 3900/3907, reward: 3.177, loss: 0.0109, took: 38.2541s
Mean epoch loss/reward: 0.0277, 3.1806, 3.1667, took: 1493.9768s (37.0610s / 100 batches)
Batch 0/3907, reward: 3.194, loss: 0.5389, took: 0.5162s
Batch 100/3907, reward: 3.178, loss: -0.0054, took: 38.2753s
Batch 200/3907, reward: 3.176, loss: -0.0309, took: 38.1249s
Batch 300/3907, reward: 3.175, loss: -0.0925, took: 38.0923s
Batch 400/3907, reward: 3.176, loss: -0.0099, took: 38.1020s
Batch 500/3907, reward: 3.176, loss: -0.0829, took: 38.0643s
Batch 600/3907, reward: 3.176, loss: -0.0291, took: 38.1586s
Batch 700/3907, reward: 3.177, loss: -0.0201, took: 35.4223s
Batch 800/3907, reward: 3.172, loss: 0.0183, took: 37.8940s
Batch 900/3907, reward: 3.174, loss: 0.0845, took: 38.2078s
Batch 1000/3907, reward: 3.176, loss: 0.0752, took: 36.2290s
Batch 1100/3907, reward: 3.175, loss: -0.0119, took: 37.0792s
Batch 1200/3907, reward: 3.177, loss: 0.0109, took: 38.1712s
Batch 1300/3907, reward: 3.171, loss: -0.0094, took: 38.0392s
Batch 1400/3907, reward: 3.176, loss: 0.0150, took: 38.0625s
Batch 1500/3907, reward: 3.174, loss: -0.0517, took: 38.1104s
Batch 1600/3907, reward: 3.176, loss: -0.0142, took: 35.3218s
Batch 1700/3907, reward: 3.173, loss: -0.0119, took: 38.1500s
Batch 1800/3907, reward: 3.173, loss: 0.0078, took: 38.1424s
Batch 1900/3907, reward: 3.174, loss: -0.0806, took: 38.0663s
Batch 2000/3907, reward: 3.176, loss: -0.0318, took: 37.9765s
Batch 2100/3907, reward: 3.171, loss: -0.0220, took: 38.0844s
Batch 2200/3907, reward: 3.170, loss: -0.0269, took: 38.1435s
Batch 2300/3907, reward: 3.176, loss: -0.0182, took: 38.1408s
Batch 2400/3907, reward: 3.175, loss: 0.0108, took: 38.1408s
Batch 2500/3907, reward: 3.172, loss: 0.0280, took: 38.0995s
Batch 2600/3907, reward: 3.173, loss: -0.0616, took: 38.0494s
Batch 2700/3907, reward: 3.172, loss: 0.0048, took: 38.0177s
Batch 2800/3907, reward: 3.172, loss: -0.0658, took: 38.2736s
Batch 2900/3907, reward: 3.173, loss: 0.0300, took: 38.1945s
Batch 3000/3907, reward: 3.173, loss: -0.0465, took: 38.0868s
Batch 3100/3907, reward: 3.175, loss: 0.0024, took: 38.2710s
Batch 3200/3907, reward: 3.174, loss: 0.0218, took: 38.2864s
Batch 3300/3907, reward: 3.169, loss: 0.0139, took: 38.2311s
Batch 3400/3907, reward: 3.176, loss: -0.0062, took: 38.0308s
Batch 3500/3907, reward: 3.174, loss: -0.0029, took: 38.1800s
Batch 3600/3907, reward: 3.174, loss: -0.0006, took: 38.2457s
Batch 3700/3907, reward: 3.171, loss: 0.0267, took: 38.2735s
Batch 3800/3907, reward: 3.173, loss: 0.0076, took: 38.1984s
Batch 3900/3907, reward: 3.174, loss: -0.0312, took: 38.0540s
Mean epoch loss/reward: -0.0107, 3.1741, 3.1651, took: 1490.8156s (36.9802s / 100 batches)
Batch 0/3907, reward: 3.173, loss: -0.4624, took: 0.5166s
Batch 100/3907, reward: 3.172, loss: -0.0117, took: 38.2890s
Batch 200/3907, reward: 3.172, loss: -0.0328, took: 38.0805s
Batch 300/3907, reward: 3.171, loss: 0.0061, took: 38.0437s
Batch 400/3907, reward: 3.170, loss: -0.0458, took: 38.1344s
Batch 500/3907, reward: 3.171, loss: 0.0426, took: 38.1170s
Batch 600/3907, reward: 3.170, loss: -0.0182, took: 37.9881s
Batch 700/3907, reward: 3.173, loss: -0.0345, took: 35.2372s
Batch 800/3907, reward: 3.173, loss: -0.0776, took: 38.1892s
Batch 900/3907, reward: 3.173, loss: 0.1412, took: 38.1780s
Batch 1000/3907, reward: 3.170, loss: -0.0044, took: 36.5017s
Batch 1100/3907, reward: 3.174, loss: -0.0150, took: 36.7829s
Batch 1200/3907, reward: 3.170, loss: 0.0329, took: 38.3274s
Batch 1300/3907, reward: 3.173, loss: -0.0102, took: 38.1006s
Batch 1400/3907, reward: 3.169, loss: -0.0070, took: 38.0834s
Batch 1500/3907, reward: 3.181, loss: -0.0226, took: 38.0114s
Batch 1600/3907, reward: 3.170, loss: -0.0198, took: 35.0694s
Batch 1700/3907, reward: 3.172, loss: -0.1472, took: 38.0238s
Batch 1800/3907, reward: 3.169, loss: 0.0056, took: 38.0277s
Batch 1900/3907, reward: 3.173, loss: 0.0048, took: 38.0617s
Batch 2000/3907, reward: 3.172, loss: 0.0428, took: 38.0392s
Batch 2100/3907, reward: 3.175, loss: -0.0202, took: 38.1120s
Batch 2200/3907, reward: 3.175, loss: -0.0652, took: 37.9110s
Batch 2300/3907, reward: 3.172, loss: -0.0170, took: 38.1571s
Batch 2400/3907, reward: 3.171, loss: 0.0395, took: 38.0664s
Batch 2500/3907, reward: 3.170, loss: 0.0049, took: 38.1093s
Batch 2600/3907, reward: 3.170, loss: 0.0893, took: 38.0329s
Batch 2700/3907, reward: 3.173, loss: -0.0220, took: 38.0397s
Batch 2800/3907, reward: 3.170, loss: 0.0283, took: 38.2371s
Batch 2900/3907, reward: 3.171, loss: -0.0079, took: 38.2409s
Batch 3000/3907, reward: 3.171, loss: -0.0166, took: 38.4115s
Batch 3100/3907, reward: 3.172, loss: -0.0233, took: 38.2675s
Batch 3200/3907, reward: 3.169, loss: -0.0716, took: 38.3350s
Batch 3300/3907, reward: 3.170, loss: -0.0017, took: 38.0061s
Batch 3400/3907, reward: 3.170, loss: 0.0252, took: 38.1249s
Batch 3500/3907, reward: 3.169, loss: 0.0118, took: 38.2645s
Batch 3600/3907, reward: 3.172, loss: -0.0610, took: 38.0632s
Batch 3700/3907, reward: 3.175, loss: 0.0319, took: 38.1457s
Batch 3800/3907, reward: 3.171, loss: -0.0036, took: 38.0029s
Batch 3900/3907, reward: 3.172, loss: 0.0320, took: 38.0856s
Mean epoch loss/reward: -0.0056, 3.1716, 3.1651, took: 1489.9635s (36.9604s / 100 batches)
Batch 0/3907, reward: 3.156, loss: 0.6968, took: 0.5251s
Batch 100/3907, reward: 3.175, loss: -0.0095, took: 38.0879s
Batch 200/3907, reward: 3.177, loss: -0.0133, took: 37.8558s
Batch 300/3907, reward: 3.171, loss: 0.0201, took: 37.9880s
Batch 400/3907, reward: 3.170, loss: -0.0701, took: 37.9962s
Batch 500/3907, reward: 3.171, loss: -0.0084, took: 37.9295s
Batch 600/3907, reward: 3.171, loss: 0.0328, took: 38.1414s
Batch 700/3907, reward: 3.168, loss: -0.0244, took: 35.4171s
Batch 800/3907, reward: 3.174, loss: 0.0421, took: 38.2421s
Batch 900/3907, reward: 3.171, loss: -0.0057, took: 38.2420s
Batch 1000/3907, reward: 3.167, loss: 0.0180, took: 36.7505s
Batch 1100/3907, reward: 3.169, loss: 0.0139, took: 36.7205s
Batch 1200/3907, reward: 3.172, loss: 0.0022, took: 38.1960s
Batch 1300/3907, reward: 3.172, loss: 0.0298, took: 38.1477s
Batch 1400/3907, reward: 3.171, loss: -0.0288, took: 38.0141s
Batch 1500/3907, reward: 3.167, loss: -0.0124, took: 38.1568s
Batch 1600/3907, reward: 3.169, loss: -0.0074, took: 35.3292s
Batch 1700/3907, reward: 3.172, loss: -0.0194, took: 38.0180s
Batch 1800/3907, reward: 3.169, loss: -0.0119, took: 38.1860s
Batch 1900/3907, reward: 3.170, loss: -0.0104, took: 38.1053s
Batch 2000/3907, reward: 3.170, loss: -0.0360, took: 38.0033s
Batch 2100/3907, reward: 3.169, loss: -0.0084, took: 38.0914s
Batch 2200/3907, reward: 3.171, loss: -0.0154, took: 38.0673s
Batch 2300/3907, reward: 3.171, loss: 0.0119, took: 38.5195s
Batch 2400/3907, reward: 3.170, loss: -0.0199, took: 38.2855s
Batch 2500/3907, reward: 3.171, loss: -0.0529, took: 38.1689s
Batch 2600/3907, reward: 3.169, loss: 0.0204, took: 38.1632s
Batch 2700/3907, reward: 3.169, loss: -0.0117, took: 37.9228s
Batch 2800/3907, reward: 3.168, loss: 0.0299, took: 38.0873s
Batch 2900/3907, reward: 3.169, loss: 0.0099, took: 38.1558s
Batch 3000/3907, reward: 3.168, loss: -0.0064, took: 38.0282s
Batch 3100/3907, reward: 3.168, loss: 0.0511, took: 37.9838s
Batch 3200/3907, reward: 3.168, loss: -0.0079, took: 37.9693s
Batch 3300/3907, reward: 3.167, loss: -0.0339, took: 38.0109s
Batch 3400/3907, reward: 3.166, loss: -0.0416, took: 38.0547s
Batch 3500/3907, reward: 3.170, loss: -0.0094, took: 38.2447s
Batch 3600/3907, reward: 3.169, loss: -0.0515, took: 37.9751s
Batch 3700/3907, reward: 3.171, loss: -0.0330, took: 38.2206s
Batch 3800/3907, reward: 3.169, loss: 0.0058, took: 38.0187s
Batch 3900/3907, reward: 3.167, loss: -0.0429, took: 38.0553s
Mean epoch loss/reward: -0.0083, 3.1699, 3.1636, took: 1489.5152s (36.9519s / 100 batches)
Batch 0/3907, reward: 3.169, loss: -1.1279, took: 0.5046s
Batch 100/3907, reward: 3.168, loss: 0.0682, took: 38.2543s
Batch 200/3907, reward: 3.171, loss: 0.0251, took: 37.9842s
Batch 300/3907, reward: 3.167, loss: -0.0409, took: 37.9851s
Batch 400/3907, reward: 3.169, loss: 0.0154, took: 38.0844s
Batch 500/3907, reward: 3.167, loss: -0.0338, took: 38.2146s
Batch 600/3907, reward: 3.169, loss: -0.0043, took: 38.2186s
Batch 700/3907, reward: 3.169, loss: 0.0132, took: 35.3711s
Batch 800/3907, reward: 3.169, loss: -0.0065, took: 38.3169s
Batch 900/3907, reward: 3.172, loss: 0.0026, took: 38.2507s
Batch 1000/3907, reward: 3.171, loss: 0.0538, took: 36.8256s
Batch 1100/3907, reward: 3.169, loss: 0.0203, took: 36.9107s
Batch 1200/3907, reward: 3.169, loss: 0.0218, took: 38.3318s
Batch 1300/3907, reward: 3.169, loss: -0.0134, took: 38.1548s
Batch 1400/3907, reward: 3.167, loss: -0.0312, took: 38.2602s
Batch 1500/3907, reward: 3.166, loss: -0.0187, took: 38.0759s
Batch 1600/3907, reward: 3.171, loss: -0.0348, took: 35.0992s
Batch 1700/3907, reward: 3.170, loss: 0.0407, took: 38.0584s
Batch 1800/3907, reward: 3.170, loss: -0.0273, took: 38.1093s
Batch 1900/3907, reward: 3.165, loss: -0.0235, took: 38.0575s
Batch 2000/3907, reward: 3.169, loss: 0.0320, took: 38.2437s
Batch 2100/3907, reward: 3.168, loss: -0.0047, took: 38.0176s
Batch 2200/3907, reward: 3.165, loss: -0.0166, took: 37.9869s
Batch 2300/3907, reward: 3.167, loss: -0.0079, took: 38.0544s
Batch 2400/3907, reward: 3.169, loss: -0.0154, took: 38.1721s
Batch 2500/3907, reward: 3.167, loss: 0.0037, took: 38.2689s
Batch 2600/3907, reward: 3.170, loss: 0.0112, took: 38.1443s
Batch 2700/3907, reward: 3.172, loss: -0.0756, took: 38.1364s
Batch 2800/3907, reward: 3.170, loss: 0.0110, took: 38.2582s
Batch 2900/3907, reward: 3.169, loss: 0.0091, took: 38.1002s
Batch 3000/3907, reward: 3.170, loss: -0.0533, took: 38.1048s
Batch 3100/3907, reward: 3.167, loss: 0.0020, took: 38.1743s
Batch 3200/3907, reward: 3.166, loss: -0.0217, took: 38.0526s
Batch 3300/3907, reward: 3.166, loss: 0.0152, took: 38.0369s
Batch 3400/3907, reward: 3.168, loss: -0.0427, took: 38.0481s
Batch 3500/3907, reward: 3.167, loss: 0.0074, took: 37.9136s
Batch 3600/3907, reward: 3.165, loss: -0.0198, took: 37.9875s
Batch 3700/3907, reward: 3.170, loss: 0.0031, took: 37.9273s
Batch 3800/3907, reward: 3.165, loss: -0.0123, took: 38.0073s
Batch 3900/3907, reward: 3.163, loss: 0.0576, took: 38.0624s
Mean epoch loss/reward: -0.0027, 3.1682, 3.1599, took: 1490.3377s (36.9691s / 100 batches)
Batch 0/3907, reward: 3.173, loss: -0.2175, took: 0.5090s
Batch 100/3907, reward: 3.166, loss: -0.0361, took: 38.2380s
Batch 200/3907, reward: 3.166, loss: -0.0071, took: 38.1554s
Batch 300/3907, reward: 3.170, loss: -0.0009, took: 38.1866s
Batch 400/3907, reward: 3.163, loss: -0.0231, took: 38.2749s
Batch 500/3907, reward: 3.169, loss: -0.0146, took: 38.2366s
Batch 600/3907, reward: 3.165, loss: 0.0102, took: 38.2776s
Batch 700/3907, reward: 3.166, loss: 0.0157, took: 35.4990s
Batch 800/3907, reward: 3.165, loss: 0.0028, took: 37.9943s
Batch 900/3907, reward: 3.166, loss: -0.0180, took: 37.9965s
Batch 1000/3907, reward: 3.167, loss: -0.0301, took: 36.4365s
Batch 1100/3907, reward: 3.172, loss: -0.0194, took: 36.5261s
Batch 1200/3907, reward: 3.168, loss: 0.0014, took: 37.9821s
Batch 1300/3907, reward: 3.166, loss: -0.0032, took: 37.9918s
Batch 1400/3907, reward: 3.166, loss: -0.0132, took: 38.1296s
Batch 1500/3907, reward: 3.165, loss: -0.0006, took: 38.1213s
Batch 1600/3907, reward: 3.166, loss: -0.0297, took: 35.1609s
Batch 1700/3907, reward: 3.165, loss: -0.0170, took: 38.0091s
Batch 1800/3907, reward: 3.168, loss: 0.0177, took: 38.0764s
Batch 1900/3907, reward: 3.167, loss: -0.0158, took: 38.0408s
Batch 2000/3907, reward: 3.164, loss: -0.0015, took: 38.0460s
Batch 2100/3907, reward: 3.171, loss: 0.0169, took: 38.2781s
Batch 2200/3907, reward: 3.167, loss: 0.0015, took: 38.2913s
Batch 2300/3907, reward: 3.166, loss: -0.0209, took: 38.1711s
Batch 2400/3907, reward: 3.166, loss: -0.0222, took: 38.3428s
Batch 2500/3907, reward: 3.165, loss: -0.0102, took: 38.2267s
Batch 2600/3907, reward: 3.165, loss: -0.0194, took: 38.2689s
Batch 2700/3907, reward: 3.167, loss: -0.0498, took: 38.0589s
Batch 2800/3907, reward: 3.167, loss: -0.0072, took: 38.1152s
Batch 2900/3907, reward: 3.167, loss: 0.0051, took: 38.0893s
Batch 3000/3907, reward: 3.169, loss: -0.0235, took: 38.0576s
Batch 3100/3907, reward: 3.165, loss: 0.0128, took: 38.1155s
Batch 3200/3907, reward: 3.164, loss: -0.0163, took: 37.9521s
Batch 3300/3907, reward: 3.166, loss: -0.0040, took: 38.0052s
Batch 3400/3907, reward: 3.164, loss: -0.0061, took: 38.0082s
Batch 3500/3907, reward: 3.166, loss: 0.0083, took: 37.9394s
Batch 3600/3907, reward: 3.165, loss: -0.0054, took: 37.8812s
Batch 3700/3907, reward: 3.165, loss: -0.0215, took: 37.9400s
Batch 3800/3907, reward: 3.165, loss: -0.0054, took: 37.8779s
Batch 3900/3907, reward: 3.169, loss: 0.0097, took: 38.0024s
Mean epoch loss/reward: -0.0087, 3.1664, 3.1609, took: 1488.8998s (36.9378s / 100 batches)
Batch 0/3907, reward: 3.170, loss: -1.2744, took: 0.5309s
Batch 100/3907, reward: 3.165, loss: 0.0399, took: 38.0821s
Batch 200/3907, reward: 3.164, loss: 0.0049, took: 37.9189s
Batch 300/3907, reward: 3.165, loss: -0.0210, took: 38.2857s
Batch 400/3907, reward: 3.164, loss: -0.0153, took: 38.2262s
Batch 500/3907, reward: 3.162, loss: -0.0042, took: 38.2805s
Batch 600/3907, reward: 3.164, loss: -0.0014, took: 38.2452s
Batch 700/3907, reward: 3.163, loss: 0.0116, took: 35.4173s
Batch 800/3907, reward: 3.167, loss: -0.0294, took: 38.2028s
Batch 900/3907, reward: 3.169, loss: -0.0050, took: 38.1366s
Batch 1000/3907, reward: 3.168, loss: -0.0000, took: 36.7528s
Batch 1100/3907, reward: 3.167, loss: 0.0026, took: 36.4153s
Batch 1200/3907, reward: 3.164, loss: 0.0159, took: 38.1232s
Batch 1300/3907, reward: 3.166, loss: 0.0060, took: 37.9340s
Batch 1400/3907, reward: 3.164, loss: -0.0027, took: 38.0917s
Batch 1500/3907, reward: 3.164, loss: -0.0273, took: 37.9114s
Batch 1600/3907, reward: 3.165, loss: -0.0101, took: 35.2048s
Batch 1700/3907, reward: 3.164, loss: -0.0104, took: 38.1214s
Batch 1800/3907, reward: 3.165, loss: 0.0105, took: 37.9115s
Batch 1900/3907, reward: 3.166, loss: -0.0013, took: 37.9789s
Batch 2000/3907, reward: 3.162, loss: 0.0179, took: 38.0937s
Batch 2100/3907, reward: 3.177, loss: -0.0039, took: 37.8625s
Batch 2200/3907, reward: 3.173, loss: -0.0513, took: 37.8329s
Batch 2300/3907, reward: 3.166, loss: -0.0141, took: 38.1254s
Batch 2400/3907, reward: 3.164, loss: -0.0313, took: 38.0573s
Batch 2500/3907, reward: 3.165, loss: 0.0171, took: 38.2278s
Batch 2600/3907, reward: 3.166, loss: -0.0103, took: 37.9142s
Batch 2700/3907, reward: 3.165, loss: -0.0246, took: 37.9459s
Batch 2800/3907, reward: 3.164, loss: -0.0125, took: 38.0502s
Batch 2900/3907, reward: 3.168, loss: -0.0227, took: 38.0415s
Batch 3000/3907, reward: 3.164, loss: 0.0095, took: 37.9969s
Batch 3100/3907, reward: 3.166, loss: -0.0196, took: 38.0378s
Batch 3200/3907, reward: 3.166, loss: -0.0050, took: 37.8567s
Batch 3300/3907, reward: 3.165, loss: 0.0025, took: 37.9739s
Batch 3400/3907, reward: 3.165, loss: -0.0097, took: 38.1637s
Batch 3500/3907, reward: 3.166, loss: -0.0118, took: 38.1352s
Batch 3600/3907, reward: 3.166, loss: 0.0002, took: 37.9471s
Batch 3700/3907, reward: 3.161, loss: 0.0105, took: 38.1053s
Batch 3800/3907, reward: 3.163, loss: 0.0086, took: 37.9589s
Batch 3900/3907, reward: 3.166, loss: 0.0004, took: 38.0233s
Mean epoch loss/reward: -0.0052, 3.1655, 3.1586, took: 1487.6017s (36.9030s / 100 batches)
Batch 0/3907, reward: 3.156, loss: 0.2116, took: 0.5266s
Batch 100/3907, reward: 3.163, loss: -0.0128, took: 38.2678s
Batch 200/3907, reward: 3.167, loss: 0.0013, took: 37.9382s
Batch 300/3907, reward: 3.165, loss: 0.0000, took: 38.0507s
Batch 400/3907, reward: 3.170, loss: -0.0406, took: 38.1528s
Batch 500/3907, reward: 3.161, loss: -0.0045, took: 38.1511s
Batch 600/3907, reward: 3.167, loss: 0.0024, took: 37.9940s
Batch 700/3907, reward: 3.163, loss: -0.0281, took: 35.3772s
Batch 800/3907, reward: 3.165, loss: 0.0072, took: 38.0015s
Batch 900/3907, reward: 3.166, loss: -0.0150, took: 37.8607s
Batch 1000/3907, reward: 3.168, loss: -0.0157, took: 36.5667s
Batch 1100/3907, reward: 3.168, loss: -0.0168, took: 36.5940s
Batch 1200/3907, reward: 3.171, loss: -0.0079, took: 38.0890s
Batch 1300/3907, reward: 3.163, loss: 0.0247, took: 37.9547s
Batch 1400/3907, reward: 3.166, loss: -0.0001, took: 37.9787s
Batch 1500/3907, reward: 3.168, loss: -0.0062, took: 38.1708s
Batch 1600/3907, reward: 3.164, loss: -0.0052, took: 34.9258s
Batch 1700/3907, reward: 3.167, loss: -0.0160, took: 38.0301s
Batch 1800/3907, reward: 3.161, loss: 0.0278, took: 38.0550s
Batch 1900/3907, reward: 3.166, loss: -0.0151, took: 38.0530s
Batch 2000/3907, reward: 3.168, loss: -0.0189, took: 38.1205s
Batch 2100/3907, reward: 3.163, loss: -0.0001, took: 38.1016s
Batch 2200/3907, reward: 3.167, loss: -0.0117, took: 38.0603s
Batch 2300/3907, reward: 3.164, loss: -0.0152, took: 38.0015s
Batch 2400/3907, reward: 3.165, loss: 0.0019, took: 38.0540s
Batch 2500/3907, reward: 3.163, loss: 0.0110, took: 38.0598s
Batch 2600/3907, reward: 3.163, loss: 0.0080, took: 38.0506s
Batch 2700/3907, reward: 3.164, loss: -0.0099, took: 37.9272s
Batch 2800/3907, reward: 3.163, loss: -0.0105, took: 37.9201s
Batch 2900/3907, reward: 3.166, loss: -0.0052, took: 37.9014s
Batch 3000/3907, reward: 3.162, loss: -0.0230, took: 37.9664s
Batch 3100/3907, reward: 3.165, loss: -0.0047, took: 37.9368s
Batch 3200/3907, reward: 3.166, loss: -0.0125, took: 38.0093s
Batch 3300/3907, reward: 3.166, loss: -0.0025, took: 38.0092s
Batch 3400/3907, reward: 3.163, loss: -0.0041, took: 38.0834s
Batch 3500/3907, reward: 3.163, loss: -0.0076, took: 37.9101s
Batch 3600/3907, reward: 3.165, loss: -0.0114, took: 38.0273s
Batch 3700/3907, reward: 3.162, loss: 0.0104, took: 37.9447s
Batch 3800/3907, reward: 3.165, loss: 0.0105, took: 37.9577s
Batch 3900/3907, reward: 3.164, loss: -0.0031, took: 38.0296s
Mean epoch loss/reward: -0.0056, 3.1650, 3.1607, took: 1486.4372s (36.8703s / 100 batches)
Batch 0/3907, reward: 3.169, loss: -0.2475, took: 0.5174s
Batch 100/3907, reward: 3.164, loss: -0.0017, took: 38.0712s
Batch 200/3907, reward: 3.160, loss: -0.0012, took: 37.8676s
Batch 300/3907, reward: 3.163, loss: 0.0011, took: 38.0263s
Batch 400/3907, reward: 3.164, loss: 0.0072, took: 37.9152s
Batch 500/3907, reward: 3.163, loss: -0.0058, took: 37.8453s
Batch 600/3907, reward: 3.164, loss: -0.0107, took: 38.0024s
Batch 700/3907, reward: 3.167, loss: 0.0153, took: 35.1789s
Batch 800/3907, reward: 3.166, loss: 0.0016, took: 37.9365s
Batch 900/3907, reward: 3.163, loss: -0.0070, took: 37.9669s
Batch 1000/3907, reward: 3.164, loss: 0.0008, took: 36.4979s
Batch 1100/3907, reward: 3.165, loss: -0.0008, took: 36.5762s
Batch 1200/3907, reward: 3.165, loss: -0.0107, took: 37.9943s
Batch 1300/3907, reward: 3.161, loss: 0.0032, took: 37.9687s
Batch 1400/3907, reward: 3.161, loss: -0.0034, took: 38.2387s
Batch 1500/3907, reward: 3.171, loss: 0.0151, took: 37.9326s
Batch 1600/3907, reward: 3.169, loss: 0.0086, took: 35.0699s
Batch 1700/3907, reward: 3.165, loss: -0.0144, took: 38.1402s
Batch 1800/3907, reward: 3.167, loss: -0.0035, took: 37.9479s
Batch 1900/3907, reward: 3.163, loss: -0.0069, took: 38.1178s
Batch 2000/3907, reward: 3.165, loss: -0.0057, took: 37.9979s
Batch 2100/3907, reward: 3.164, loss: -0.0108, took: 37.9930s
Batch 2200/3907, reward: 3.164, loss: -0.0009, took: 38.0854s
Batch 2300/3907, reward: 3.165, loss: -0.0078, took: 38.1121s
Batch 2400/3907, reward: 3.163, loss: 0.0139, took: 38.1102s
Batch 2500/3907, reward: 3.159, loss: 0.0064, took: 37.9888s
Batch 2600/3907, reward: 3.167, loss: -0.0029, took: 37.9536s
Batch 2700/3907, reward: 3.162, loss: 0.0000, took: 37.9232s
Batch 2800/3907, reward: 3.165, loss: -0.0126, took: 38.0763s
Batch 2900/3907, reward: 3.163, loss: 0.0027, took: 37.7869s
Batch 3000/3907, reward: 3.164, loss: 0.0006, took: 37.8036s
Batch 3100/3907, reward: 3.164, loss: -0.0145, took: 37.8514s
Batch 3200/3907, reward: 3.161, loss: -0.0033, took: 37.8696s
Batch 3300/3907, reward: 3.162, loss: -0.0074, took: 37.9117s
Batch 3400/3907, reward: 3.162, loss: -0.0031, took: 37.7932s
Batch 3500/3907, reward: 3.162, loss: 0.0021, took: 37.8578s
Batch 3600/3907, reward: 3.164, loss: -0.0060, took: 37.9247s
Batch 3700/3907, reward: 3.163, loss: 0.0031, took: 37.9210s
Batch 3800/3907, reward: 3.163, loss: -0.0029, took: 37.8736s
Batch 3900/3907, reward: 3.165, loss: -0.0062, took: 38.1607s
Mean epoch loss/reward: -0.0018, 3.1639, 3.1596, took: 1484.5397s (36.8202s / 100 batches)
Batch 0/3907, reward: 3.136, loss: 0.0801, took: 0.5226s
Batch 100/3907, reward: 3.163, loss: -0.0185, took: 38.1321s
Batch 200/3907, reward: 3.162, loss: -0.0063, took: 37.9331s
Batch 300/3907, reward: 3.164, loss: -0.0069, took: 37.9258s
Batch 400/3907, reward: 3.162, loss: -0.0131, took: 37.9536s
Batch 500/3907, reward: 3.167, loss: -0.0018, took: 38.0023s
Batch 600/3907, reward: 3.166, loss: -0.0090, took: 37.8956s
Batch 700/3907, reward: 3.162, loss: -0.0173, took: 35.3557s
Batch 800/3907, reward: 3.166, loss: -0.0114, took: 38.1517s
Batch 900/3907, reward: 3.163, loss: -0.0022, took: 38.1167s
Batch 1000/3907, reward: 3.165, loss: 0.0059, took: 36.4949s
Batch 1100/3907, reward: 3.163, loss: 0.0022, took: 37.0431s
Batch 1200/3907, reward: 3.162, loss: -0.0265, took: 38.1585s
Batch 1300/3907, reward: 3.162, loss: 0.0005, took: 37.8942s
Batch 1400/3907, reward: 3.165, loss: -0.0048, took: 37.8520s
Batch 1500/3907, reward: 3.161, loss: -0.0017, took: 38.1600s
Batch 1600/3907, reward: 3.164, loss: 0.0011, took: 34.8754s
Batch 1700/3907, reward: 3.165, loss: -0.0079, took: 37.9545s
Batch 1800/3907, reward: 3.164, loss: -0.0006, took: 37.8388s
Batch 1900/3907, reward: 3.162, loss: -0.0048, took: 37.8956s
Batch 2000/3907, reward: 3.163, loss: -0.0057, took: 38.0209s
Batch 2100/3907, reward: 3.164, loss: -0.0023, took: 38.1447s
Batch 2200/3907, reward: 3.164, loss: -0.0141, took: 38.0760s
Batch 2300/3907, reward: 3.164, loss: 0.0215, took: 38.1217s
Batch 2400/3907, reward: 3.162, loss: -0.0004, took: 38.2138s
Batch 2500/3907, reward: 3.164, loss: -0.0123, took: 38.2171s
Batch 2600/3907, reward: 3.165, loss: -0.0135, took: 38.1716s
Batch 2700/3907, reward: 3.159, loss: -0.0036, took: 37.9345s
Batch 2800/3907, reward: 3.163, loss: -0.0130, took: 38.0882s
Batch 2900/3907, reward: 3.159, loss: -0.0008, took: 38.0364s
Batch 3000/3907, reward: 3.163, loss: -0.0052, took: 37.9189s
Batch 3100/3907, reward: 3.161, loss: -0.0135, took: 38.0395s
Batch 3200/3907, reward: 3.162, loss: -0.0054, took: 37.9936s
Batch 3300/3907, reward: 3.164, loss: -0.0190, took: 37.8822s
Batch 3400/3907, reward: 3.161, loss: -0.0170, took: 37.9016s
Batch 3500/3907, reward: 3.162, loss: -0.0058, took: 38.0522s
Batch 3600/3907, reward: 3.161, loss: -0.0165, took: 37.9138s
Batch 3700/3907, reward: 3.163, loss: -0.0079, took: 38.0665s
Batch 3800/3907, reward: 3.163, loss: -0.0051, took: 38.1509s
Batch 3900/3907, reward: 3.160, loss: -0.0070, took: 38.0515s
Mean epoch loss/reward: -0.0068, 3.1629, 3.1610, took: 1486.6418s (36.8788s / 100 batches)
Batch 0/3907, reward: 3.159, loss: 0.0991, took: 0.5717s
Batch 100/3907, reward: 3.160, loss: -0.0087, took: 38.3673s
Batch 200/3907, reward: 3.161, loss: -0.0056, took: 38.1415s
Batch 300/3907, reward: 3.164, loss: -0.0081, took: 38.1507s
Batch 400/3907, reward: 3.161, loss: -0.0149, took: 38.0646s
Batch 500/3907, reward: 3.166, loss: 0.0085, took: 38.2136s
Batch 600/3907, reward: 3.162, loss: -0.0135, took: 38.2431s
Batch 700/3907, reward: 3.163, loss: -0.0119, took: 35.3639s
Batch 800/3907, reward: 3.162, loss: -0.0135, took: 37.9288s
Batch 900/3907, reward: 3.161, loss: 0.0035, took: 38.0482s
Batch 1000/3907, reward: 3.161, loss: -0.0111, took: 36.1827s
Batch 1100/3907, reward: 3.161, loss: -0.0005, took: 36.9694s
Batch 1200/3907, reward: 3.161, loss: -0.0022, took: 37.9129s
Batch 1300/3907, reward: 3.163, loss: -0.0098, took: 37.8682s
Batch 1400/3907, reward: 3.163, loss: -0.0078, took: 37.8709s
Batch 1500/3907, reward: 3.166, loss: -0.0054, took: 37.8414s
Batch 1600/3907, reward: 3.161, loss: -0.0074, took: 34.8277s
Batch 1700/3907, reward: 3.161, loss: 0.0147, took: 38.0745s
Batch 1800/3907, reward: 3.164, loss: -0.0019, took: 37.9850s
Batch 1900/3907, reward: 3.164, loss: -0.0122, took: 38.0615s
Batch 2000/3907, reward: 3.162, loss: -0.0054, took: 38.0797s
Batch 2100/3907, reward: 3.164, loss: -0.0040, took: 38.1063s
Batch 2200/3907, reward: 3.161, loss: -0.0010, took: 38.1145s
Batch 2300/3907, reward: 3.165, loss: -0.0023, took: 38.0808s
Batch 2400/3907, reward: 3.160, loss: 0.0001, took: 37.9474s
Batch 2500/3907, reward: 3.162, loss: -0.0184, took: 38.0220s
Batch 2600/3907, reward: 3.165, loss: -0.0090, took: 37.9414s
Batch 2700/3907, reward: 3.161, loss: -0.0145, took: 37.8943s
Batch 2800/3907, reward: 3.162, loss: -0.0052, took: 38.0885s
Batch 2900/3907, reward: 3.163, loss: 0.0000, took: 37.8452s
Batch 3000/3907, reward: 3.159, loss: -0.0174, took: 37.9512s
Batch 3100/3907, reward: 3.161, loss: 0.0017, took: 38.0124s
Batch 3200/3907, reward: 3.166, loss: 0.0130, took: 38.0554s
Batch 3300/3907, reward: 3.162, loss: -0.0123, took: 38.1278s
Batch 3400/3907, reward: 3.162, loss: -0.0091, took: 38.2298s
Batch 3500/3907, reward: 3.161, loss: -0.0015, took: 38.0908s
Batch 3600/3907, reward: 3.159, loss: -0.0040, took: 38.0272s
Batch 3700/3907, reward: 3.160, loss: -0.0027, took: 38.1655s
Batch 3800/3907, reward: 3.164, loss: -0.0108, took: 38.0911s
Batch 3900/3907, reward: 3.161, loss: -0.0121, took: 38.0378s
Mean epoch loss/reward: -0.0058, 3.1622, 3.1593, took: 1487.0929s (36.8899s / 100 batches)
Batch 0/3907, reward: 3.161, loss: 0.3031, took: 0.5214s
Batch 100/3907, reward: 3.161, loss: -0.0033, took: 38.1306s
Batch 200/3907, reward: 3.164, loss: -0.0020, took: 38.0450s
Batch 300/3907, reward: 3.162, loss: -0.0102, took: 37.9601s
Batch 400/3907, reward: 3.162, loss: -0.0064, took: 38.0725s
Batch 500/3907, reward: 3.161, loss: 0.0074, took: 37.8900s
Batch 600/3907, reward: 3.160, loss: -0.0052, took: 37.8210s
Batch 700/3907, reward: 3.162, loss: 0.0043, took: 35.2354s
Batch 800/3907, reward: 3.160, loss: -0.0039, took: 37.8779s
Batch 900/3907, reward: 3.161, loss: 0.0220, took: 37.9105s
Batch 1000/3907, reward: 3.163, loss: -0.0013, took: 36.3577s
Batch 1100/3907, reward: 3.162, loss: -0.0058, took: 36.7436s
Batch 1200/3907, reward: 3.165, loss: 0.0122, took: 38.0383s
Batch 1300/3907, reward: 3.165, loss: 0.0023, took: 37.9541s
Batch 1400/3907, reward: 3.163, loss: -0.0064, took: 37.9457s
Batch 1500/3907, reward: 3.162, loss: -0.0011, took: 37.9151s
Batch 1600/3907, reward: 3.161, loss: -0.0018, took: 34.9995s
Batch 1700/3907, reward: 3.159, loss: -0.0016, took: 38.0885s
Batch 1800/3907, reward: 3.160, loss: -0.0092, took: 38.0495s
Batch 1900/3907, reward: 3.162, loss: -0.0053, took: 38.1030s
Batch 2000/3907, reward: 3.161, loss: -0.0067, took: 38.0594s
Batch 2100/3907, reward: 3.162, loss: 0.0031, took: 38.1712s
Batch 2200/3907, reward: 3.163, loss: -0.0121, took: 38.1970s
Batch 2300/3907, reward: 3.162, loss: -0.0070, took: 38.0983s
Batch 2400/3907, reward: 3.162, loss: -0.0094, took: 38.3170s
Batch 2500/3907, reward: 3.163, loss: 0.0011, took: 37.8880s
Batch 2600/3907, reward: 3.160, loss: -0.0017, took: 38.0230s
Batch 2700/3907, reward: 3.162, loss: -0.0044, took: 37.8497s
Batch 2800/3907, reward: 3.163, loss: -0.0076, took: 37.9586s
Batch 2900/3907, reward: 3.162, loss: -0.0064, took: 38.1256s
Batch 3000/3907, reward: 3.163, loss: -0.0089, took: 37.9320s
Batch 3100/3907, reward: 3.162, loss: -0.0059, took: 37.8981s
Batch 3200/3907, reward: 3.162, loss: -0.0088, took: 37.9851s
Batch 3300/3907, reward: 3.162, loss: -0.0136, took: 37.8078s
Batch 3400/3907, reward: 3.163, loss: -0.0056, took: 37.8423s
Batch 3500/3907, reward: 3.163, loss: 0.0102, took: 38.0232s
Batch 3600/3907, reward: 3.161, loss: -0.0061, took: 38.1575s
Batch 3700/3907, reward: 3.158, loss: -0.0106, took: 38.1123s
Batch 3800/3907, reward: 3.162, loss: 0.0094, took: 38.0593s
Batch 3900/3907, reward: 3.160, loss: 0.0010, took: 37.9721s
Mean epoch loss/reward: -0.0027, 3.1618, 3.1607, took: 1485.5545s (36.8534s / 100 batches)
Batch 0/3907, reward: 3.169, loss: -0.3931, took: 0.5175s
Batch 100/3907, reward: 3.165, loss: -0.0084, took: 37.9007s
Batch 200/3907, reward: 3.162, loss: -0.0062, took: 37.7630s
Batch 300/3907, reward: 3.162, loss: -0.0005, took: 38.0017s
Batch 400/3907, reward: 3.163, loss: -0.0065, took: 37.8673s
Batch 500/3907, reward: 3.159, loss: 0.0007, took: 38.0073s
Batch 600/3907, reward: 3.161, loss: -0.0112, took: 37.9675s
Batch 700/3907, reward: 3.163, loss: -0.0003, took: 35.0620s
Batch 800/3907, reward: 3.164, loss: 0.0108, took: 37.9863s
Batch 900/3907, reward: 3.161, loss: -0.0047, took: 38.0876s
Batch 1000/3907, reward: 3.167, loss: -0.0171, took: 36.8646s
Batch 1100/3907, reward: 3.163, loss: -0.0058, took: 35.9436s
Batch 1200/3907, reward: 3.160, loss: -0.0066, took: 37.8818s
Batch 1300/3907, reward: 3.160, loss: -0.0002, took: 37.8854s
Batch 1400/3907, reward: 3.162, loss: -0.0074, took: 38.0890s
Batch 1500/3907, reward: 3.161, loss: -0.0106, took: 37.9071s
Batch 1600/3907, reward: 3.164, loss: -0.0048, took: 35.2843s
Batch 1700/3907, reward: 3.164, loss: -0.0036, took: 37.8756s
Batch 1800/3907, reward: 3.161, loss: 0.0052, took: 37.8795s
Batch 1900/3907, reward: 3.161, loss: -0.0052, took: 37.9193s
Batch 2000/3907, reward: 3.164, loss: -0.0124, took: 38.0407s
Batch 2100/3907, reward: 3.163, loss: -0.0066, took: 37.7226s
Batch 2200/3907, reward: 3.164, loss: -0.0017, took: 37.8350s
Batch 2300/3907, reward: 3.161, loss: 0.0221, took: 37.9088s
Batch 2400/3907, reward: 3.162, loss: -0.0016, took: 38.0215s
Batch 2500/3907, reward: 3.160, loss: -0.0091, took: 38.1423s
Batch 2600/3907, reward: 3.167, loss: -0.0128, took: 37.9269s
Batch 2700/3907, reward: 3.164, loss: -0.0011, took: 37.9981s
Batch 2800/3907, reward: 3.161, loss: 0.0049, took: 38.2176s
Batch 2900/3907, reward: 3.160, loss: -0.0029, took: 38.0306s
Batch 3000/3907, reward: 3.162, loss: -0.0041, took: 38.0186s
Batch 3100/3907, reward: 3.161, loss: -0.0014, took: 38.0840s
Batch 3200/3907, reward: 3.160, loss: -0.0037, took: 38.0039s
Batch 3300/3907, reward: 3.162, loss: -0.0107, took: 37.8054s
Batch 3400/3907, reward: 3.165, loss: -0.0022, took: 38.1204s
Batch 3500/3907, reward: 3.162, loss: -0.0069, took: 37.9709s
Batch 3600/3907, reward: 3.160, loss: -0.0061, took: 37.8665s
Batch 3700/3907, reward: 3.162, loss: -0.0035, took: 37.9186s
Batch 3800/3907, reward: 3.164, loss: -0.0080, took: 37.8969s
Batch 3900/3907, reward: 3.160, loss: 0.0055, took: 38.1419s
Mean epoch loss/reward: -0.0038, 3.1622, 3.1601, took: 1483.9343s (36.8091s / 100 batches)
Batch 0/3907, reward: 3.155, loss: 0.0049, took: 0.5170s
Batch 100/3907, reward: 3.160, loss: -0.0043, took: 38.0124s
Batch 200/3907, reward: 3.159, loss: -0.0054, took: 37.9113s
Batch 300/3907, reward: 3.160, loss: -0.0005, took: 37.9795s
Batch 400/3907, reward: 3.163, loss: 0.0054, took: 38.0034s
Batch 500/3907, reward: 3.165, loss: -0.0101, took: 37.9577s
Batch 600/3907, reward: 3.161, loss: -0.0074, took: 37.9574s
Batch 700/3907, reward: 3.163, loss: -0.0019, took: 35.3952s
Batch 800/3907, reward: 3.160, loss: 0.0019, took: 38.1029s
Batch 900/3907, reward: 3.161, loss: -0.0052, took: 37.9998s
Batch 1000/3907, reward: 3.161, loss: -0.0022, took: 36.9585s
Batch 1100/3907, reward: 3.160, loss: 0.0053, took: 35.8543s
Batch 1200/3907, reward: 3.164, loss: -0.0110, took: 37.9267s
Batch 1300/3907, reward: 3.158, loss: 0.0246, took: 37.9142s
Batch 1400/3907, reward: 3.161, loss: -0.0024, took: 37.9152s
Batch 1500/3907, reward: 3.160, loss: -0.0046, took: 38.0183s
Batch 1600/3907, reward: 3.162, loss: -0.0155, took: 35.1648s
Batch 1700/3907, reward: 3.157, loss: -0.0079, took: 37.9022s
Batch 1800/3907, reward: 3.163, loss: -0.0050, took: 38.2178s
Batch 1900/3907, reward: 3.166, loss: -0.0028, took: 37.9247s
Batch 2000/3907, reward: 3.162, loss: -0.0024, took: 37.8913s
Batch 2100/3907, reward: 3.161, loss: -0.0018, took: 38.0656s
Batch 2200/3907, reward: 3.160, loss: -0.0033, took: 37.9115s
Batch 2300/3907, reward: 3.162, loss: 0.0042, took: 37.9386s
Batch 2400/3907, reward: 3.161, loss: -0.0023, took: 37.9931s
Batch 2500/3907, reward: 3.160, loss: -0.0023, took: 37.9520s
Batch 2600/3907, reward: 3.161, loss: -0.0124, took: 38.1362s
Batch 2700/3907, reward: 3.164, loss: -0.0014, took: 38.0448s
Batch 2800/3907, reward: 3.161, loss: -0.0091, took: 38.0480s
Batch 2900/3907, reward: 3.163, loss: -0.0030, took: 38.0062s
Batch 3000/3907, reward: 3.161, loss: -0.0089, took: 38.0659s
Batch 3100/3907, reward: 3.165, loss: -0.0039, took: 37.9007s
Batch 3200/3907, reward: 3.166, loss: 0.0010, took: 37.9499s
Batch 3300/3907, reward: 3.163, loss: -0.0118, took: 37.8564s
Batch 3400/3907, reward: 3.161, loss: -0.0099, took: 37.9563s
Batch 3500/3907, reward: 3.164, loss: -0.0083, took: 38.1073s
Batch 3600/3907, reward: 3.164, loss: 0.0040, took: 37.9349s
Batch 3700/3907, reward: 3.165, loss: -0.0062, took: 37.9539s
Batch 3800/3907, reward: 3.162, loss: -0.0042, took: 37.9752s
Batch 3900/3907, reward: 3.161, loss: -0.0055, took: 37.7921s
Mean epoch loss/reward: -0.0036, 3.1618, 3.1601, took: 1484.5375s (36.8278s / 100 batches)
Batch 0/3907, reward: 3.137, loss: 0.3508, took: 0.5240s
Batch 100/3907, reward: 3.163, loss: 0.0009, took: 37.9037s
Batch 200/3907, reward: 3.165, loss: -0.0015, took: 37.8614s
Batch 300/3907, reward: 3.162, loss: -0.0045, took: 37.9470s
Batch 400/3907, reward: 3.161, loss: -0.0016, took: 37.8374s
Batch 500/3907, reward: 3.162, loss: -0.0043, took: 37.8550s
Batch 600/3907, reward: 3.163, loss: -0.0071, took: 37.9668s
Batch 700/3907, reward: 3.161, loss: -0.0048, took: 35.2450s
Batch 800/3907, reward: 3.162, loss: -0.0025, took: 37.8082s
Batch 900/3907, reward: 3.159, loss: -0.0076, took: 37.9015s
Batch 1000/3907, reward: 3.164, loss: -0.0070, took: 36.9734s
Batch 1100/3907, reward: 3.162, loss: -0.0003, took: 28.9160s
Batch 1200/3907, reward: 3.164, loss: -0.0321, took: 29.1866s
Batch 1300/3907, reward: 3.161, loss: -0.0045, took: 28.9590s
Batch 1400/3907, reward: 3.162, loss: -0.0018, took: 29.4848s
Batch 1500/3907, reward: 3.160, loss: -0.0101, took: 36.9079s
Batch 1600/3907, reward: 3.162, loss: -0.0096, took: 35.0057s
Batch 1700/3907, reward: 3.162, loss: -0.0109, took: 37.6850s
Batch 1800/3907, reward: 3.162, loss: -0.0030, took: 37.8595s
Batch 1900/3907, reward: 3.165, loss: -0.0004, took: 37.9578s
Batch 2000/3907, reward: 3.162, loss: -0.0095, took: 37.9795s
Batch 2100/3907, reward: 3.161, loss: -0.0066, took: 37.8171s
Batch 2200/3907, reward: 3.160, loss: -0.0002, took: 37.8677s
Batch 2300/3907, reward: 3.159, loss: -0.0074, took: 38.0541s
Batch 2400/3907, reward: 3.161, loss: -0.0014, took: 37.6179s
Batch 2500/3907, reward: 3.160, loss: -0.0042, took: 37.8476s
Batch 2600/3907, reward: 3.163, loss: -0.0107, took: 37.7686s
Batch 2700/3907, reward: 3.158, loss: -0.0034, took: 32.8098s
Batch 2800/3907, reward: 3.161, loss: -0.0042, took: 29.0855s
Batch 2900/3907, reward: 3.160, loss: -0.0018, took: 28.9409s
Batch 3000/3907, reward: 3.164, loss: -0.0080, took: 28.6254s
Batch 3100/3907, reward: 3.162, loss: -0.0060, took: 28.8379s
Batch 3200/3907, reward: 3.160, loss: -0.0053, took: 28.8354s
Batch 3300/3907, reward: 3.161, loss: -0.0054, took: 29.1064s
Batch 3400/3907, reward: 3.161, loss: -0.0093, took: 29.0191s
Batch 3500/3907, reward: 3.162, loss: -0.0162, took: 28.5292s
Batch 3600/3907, reward: 3.160, loss: 0.0009, took: 28.7701s
Batch 3700/3907, reward: 3.160, loss: -0.0036, took: 29.0684s
Batch 3800/3907, reward: 3.161, loss: -0.0007, took: 28.4456s
Batch 3900/3907, reward: 3.162, loss: -0.0113, took: 28.7810s
Mean epoch loss/reward: -0.0057, 3.1615, 3.1588, took: 1332.4057s (33.0398s / 100 batches)
Batch 0/3907, reward: 3.179, loss: -0.1392, took: 0.4227s
Batch 100/3907, reward: 3.162, loss: -0.0037, took: 29.0032s
Batch 200/3907, reward: 3.158, loss: -0.0018, took: 28.5868s
Batch 300/3907, reward: 3.158, loss: 0.0001, took: 28.6123s
Batch 400/3907, reward: 3.160, loss: -0.0016, took: 29.2246s
Batch 500/3907, reward: 3.160, loss: -0.0057, took: 28.9118s
Batch 600/3907, reward: 3.160, loss: -0.0018, took: 29.3002s
Batch 700/3907, reward: 3.162, loss: -0.0043, took: 25.5163s
Batch 800/3907, reward: 3.160, loss: 0.0000, took: 29.0460s
Batch 900/3907, reward: 3.163, loss: -0.0036, took: 29.1298s
Batch 1000/3907, reward: 3.161, loss: -0.0018, took: 29.2487s
Batch 1100/3907, reward: 3.161, loss: -0.0002, took: 28.6813s
Batch 1200/3907, reward: 3.159, loss: 0.0080, took: 28.8914s
Batch 1300/3907, reward: 3.160, loss: -0.0011, took: 28.7511s
Batch 1400/3907, reward: 3.161, loss: -0.0062, took: 29.1464s
Batch 1500/3907, reward: 3.161, loss: -0.0046, took: 28.9813s
Batch 1600/3907, reward: 3.164, loss: -0.0037, took: 20.5012s
Batch 1700/3907, reward: 3.159, loss: -0.0054, took: 19.6922s
Batch 1800/3907, reward: 3.161, loss: -0.0075, took: 19.9519s
Batch 1900/3907, reward: 3.163, loss: -0.0033, took: 20.2605s
Batch 2000/3907, reward: 3.163, loss: -0.0069, took: 19.7584s
Batch 2100/3907, reward: 3.163, loss: -0.0004, took: 22.2499s
Batch 2200/3907, reward: 3.164, loss: -0.0065, took: 28.3919s
Batch 2300/3907, reward: 3.159, loss: -0.0025, took: 28.6552s
Batch 2400/3907, reward: 3.163, loss: -0.0029, took: 28.6394s
Batch 2500/3907, reward: 3.160, loss: -0.0029, took: 28.6054s
Batch 2600/3907, reward: 3.161, loss: -0.0025, took: 28.6567s
Batch 2700/3907, reward: 3.161, loss: -0.0029, took: 28.5957s
Batch 2800/3907, reward: 3.160, loss: -0.0013, took: 28.5677s
Batch 2900/3907, reward: 3.158, loss: -0.0042, took: 28.6145s
Batch 3000/3907, reward: 3.158, loss: 0.0027, took: 28.5984s
Batch 3100/3907, reward: 3.162, loss: -0.0066, took: 28.7316s
Batch 3200/3907, reward: 3.162, loss: 0.0025, took: 28.6871s
Batch 3300/3907, reward: 3.157, loss: 0.0019, took: 28.7347s
Batch 3400/3907, reward: 3.159, loss: -0.0054, took: 21.0537s
Batch 3500/3907, reward: 3.160, loss: 0.0009, took: 19.9141s
Batch 3600/3907, reward: 3.161, loss: -0.0050, took: 19.7303s
Batch 3700/3907, reward: 3.160, loss: -0.0027, took: 19.9990s
Batch 3800/3907, reward: 3.162, loss: -0.0058, took: 20.2259s
Batch 3900/3907, reward: 3.160, loss: 0.0005, took: 19.8515s
Mean epoch loss/reward: -0.0025, 3.1607, 3.1589, took: 1028.2343s (25.4530s / 100 batches)
Average tour length for uniform: 4.53531621525835
Average tour length for shifted: 3.586575453745791
Average tour length for adversary: 2.8572560596075514
