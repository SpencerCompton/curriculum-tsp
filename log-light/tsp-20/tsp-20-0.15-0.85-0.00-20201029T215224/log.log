Current device: cuda
Batch 0/3907, reward: 12.408, loss: -710.4131, took: 0.5805s
Batch 100/3907, reward: 7.168, loss: -7.2277, took: 38.8326s
Batch 200/3907, reward: 4.544, loss: 0.8162, took: 38.4148s
Batch 300/3907, reward: 4.369, loss: 1.1593, took: 38.1919s
Batch 400/3907, reward: 4.258, loss: 0.9549, took: 34.7485s
Batch 500/3907, reward: 4.198, loss: 0.3241, took: 39.3484s
Batch 600/3907, reward: 4.157, loss: 1.2746, took: 38.1006s
Batch 700/3907, reward: 4.137, loss: 0.4361, took: 43.7641s
Batch 800/3907, reward: 4.119, loss: -0.4819, took: 47.8547s
Batch 900/3907, reward: 4.112, loss: 0.8288, took: 47.3189s
Batch 1000/3907, reward: 4.096, loss: -0.1358, took: 47.3147s
Batch 1100/3907, reward: 4.085, loss: 0.3919, took: 48.5972s
Batch 1200/3907, reward: 4.080, loss: 1.2579, took: 47.6693s
Batch 1300/3907, reward: 4.078, loss: 0.9899, took: 48.1198s
Batch 1400/3907, reward: 4.071, loss: 1.2429, took: 47.4454s
Batch 1500/3907, reward: 4.077, loss: 0.8535, took: 48.3050s
Batch 1600/3907, reward: 4.065, loss: 1.1814, took: 48.5955s
Batch 1700/3907, reward: 4.066, loss: 0.5961, took: 47.7529s
Batch 1800/3907, reward: 4.062, loss: 0.8990, took: 45.7312s
Batch 1900/3907, reward: 4.055, loss: 0.5518, took: 47.4623s
Batch 2000/3907, reward: 4.049, loss: 0.2526, took: 49.4536s
Batch 2100/3907, reward: 4.047, loss: 0.6742, took: 48.1202s
Batch 2200/3907, reward: 4.041, loss: 0.5893, took: 48.2113s
Batch 2300/3907, reward: 4.043, loss: 0.1604, took: 47.2182s
Batch 2400/3907, reward: 4.040, loss: 0.2776, took: 49.3326s
Batch 2500/3907, reward: 4.034, loss: 0.1718, took: 49.4447s
Batch 2600/3907, reward: 4.028, loss: 0.2768, took: 47.9329s
Batch 2700/3907, reward: 4.030, loss: -0.0472, took: 48.9225s
Batch 2800/3907, reward: 4.030, loss: -0.0683, took: 47.9247s
Batch 2900/3907, reward: 4.025, loss: 0.3318, took: 45.6067s
Batch 3000/3907, reward: 4.025, loss: 0.2179, took: 48.1307s
Batch 3100/3907, reward: 4.023, loss: 0.4852, took: 49.1883s
Batch 3200/3907, reward: 4.020, loss: 0.7734, took: 47.4734s
Batch 3300/3907, reward: 4.018, loss: 0.8351, took: 48.7257s
Batch 3400/3907, reward: 4.027, loss: 0.4673, took: 46.4622s
Batch 3500/3907, reward: 4.015, loss: -0.1007, took: 47.7499s
Batch 3600/3907, reward: 4.022, loss: -0.0358, took: 47.6307s
Batch 3700/3907, reward: 4.019, loss: 0.1489, took: 48.6897s
Batch 3800/3907, reward: 4.019, loss: 0.0208, took: 49.1517s
Batch 3900/3907, reward: 4.019, loss: 0.1646, took: 47.0700s
Mean epoch loss/reward: 0.1133, 4.1653, 4.0618, took: 1821.2831s (45.1647s / 100 batches)
Batch 0/3907, reward: 4.047, loss: -2.5340, took: 0.5887s
Batch 100/3907, reward: 4.019, loss: -0.1426, took: 47.3842s
Batch 200/3907, reward: 4.015, loss: 0.5397, took: 48.1979s
Batch 300/3907, reward: 4.012, loss: 0.4383, took: 48.3405s
Batch 400/3907, reward: 4.015, loss: 0.0565, took: 44.8311s
Batch 500/3907, reward: 4.013, loss: -0.1015, took: 48.7128s
Batch 600/3907, reward: 4.015, loss: 0.0416, took: 49.0764s
Batch 700/3907, reward: 4.014, loss: 0.0549, took: 46.1293s
Batch 800/3907, reward: 4.013, loss: 0.2674, took: 48.9813s
Batch 900/3907, reward: 4.018, loss: -0.0510, took: 47.1231s
Batch 1000/3907, reward: 4.014, loss: 0.0518, took: 48.7221s
Batch 1100/3907, reward: 4.014, loss: 0.1193, took: 48.4294s
Batch 1200/3907, reward: 4.017, loss: 0.4958, took: 48.2924s
Batch 1300/3907, reward: 4.014, loss: -0.0491, took: 48.6352s
Batch 1400/3907, reward: 4.009, loss: 0.1912, took: 48.4127s
Batch 1500/3907, reward: 4.007, loss: 0.1064, took: 47.9394s
Batch 1600/3907, reward: 4.008, loss: 0.0840, took: 48.4561s
Batch 1700/3907, reward: 4.011, loss: -0.0126, took: 48.6979s
Batch 1800/3907, reward: 4.006, loss: -0.0493, took: 48.2353s
Batch 1900/3907, reward: 4.006, loss: 0.1228, took: 46.1569s
Batch 2000/3907, reward: 4.002, loss: -0.0977, took: 48.1220s
Batch 2100/3907, reward: 4.002, loss: 0.1496, took: 49.4914s
Batch 2200/3907, reward: 3.995, loss: 0.1418, took: 48.4421s
Batch 2300/3907, reward: 3.987, loss: 0.1435, took: 49.0059s
Batch 2400/3907, reward: 3.976, loss: 0.1830, took: 48.2327s
Batch 2500/3907, reward: 3.968, loss: 0.0954, took: 48.3178s
Batch 2600/3907, reward: 3.946, loss: 0.2656, took: 47.8780s
Batch 2700/3907, reward: 3.930, loss: -0.0531, took: 48.8201s
Batch 2800/3907, reward: 3.907, loss: 0.0239, took: 48.0160s
Batch 2900/3907, reward: 3.887, loss: 0.0224, took: 46.0301s
Batch 3000/3907, reward: 3.873, loss: 0.1025, took: 48.1097s
Batch 3100/3907, reward: 3.866, loss: 0.1663, took: 49.4726s
Batch 3200/3907, reward: 3.864, loss: 0.0497, took: 48.3562s
Batch 3300/3907, reward: 3.847, loss: -0.0090, took: 48.7034s
Batch 3400/3907, reward: 3.841, loss: 0.0720, took: 47.8419s
Batch 3500/3907, reward: 3.843, loss: 0.1096, took: 49.2066s
Batch 3600/3907, reward: 3.847, loss: -0.0917, took: 48.5255s
Batch 3700/3907, reward: 3.834, loss: -0.0115, took: 47.4919s
Batch 3800/3907, reward: 3.827, loss: -0.0954, took: 47.8708s
Batch 3900/3907, reward: 3.783, loss: 0.0367, took: 47.6445s
Mean epoch loss/reward: 0.0857, 3.9551, 3.5897, took: 1891.3965s (46.9230s / 100 batches)
Batch 0/3907, reward: 3.671, loss: -0.0308, took: 0.5350s
Batch 100/3907, reward: 3.659, loss: -0.0325, took: 48.2505s
Batch 200/3907, reward: 3.610, loss: 0.0229, took: 47.5716s
Batch 300/3907, reward: 3.569, loss: -0.1397, took: 48.6555s
Batch 400/3907, reward: 3.547, loss: -0.0605, took: 44.8890s
Batch 500/3907, reward: 3.523, loss: -0.0821, took: 46.7558s
Batch 600/3907, reward: 3.511, loss: 0.0590, took: 47.8294s
Batch 700/3907, reward: 3.505, loss: -0.1345, took: 45.4011s
Batch 800/3907, reward: 3.501, loss: 0.0113, took: 49.3261s
Batch 900/3907, reward: 3.490, loss: -0.0180, took: 47.7965s
Batch 1000/3907, reward: 3.482, loss: -0.1334, took: 47.6554s
Batch 1100/3907, reward: 3.474, loss: -0.1798, took: 48.3761s
Batch 1200/3907, reward: 3.470, loss: -0.1335, took: 47.6169s
Batch 1300/3907, reward: 3.463, loss: -0.1782, took: 47.4598s
Batch 1400/3907, reward: 3.454, loss: -0.0582, took: 49.0446s
Batch 1500/3907, reward: 3.458, loss: -0.1559, took: 48.0174s
Batch 1600/3907, reward: 3.450, loss: 0.0455, took: 47.7339s
Batch 1700/3907, reward: 3.449, loss: -0.1608, took: 48.9157s
Batch 1800/3907, reward: 3.444, loss: -0.0732, took: 49.2892s
Batch 1900/3907, reward: 3.441, loss: -0.0905, took: 45.1578s
Batch 2000/3907, reward: 3.434, loss: -0.0834, took: 47.8362s
Batch 2100/3907, reward: 3.434, loss: -0.1180, took: 48.2388s
Batch 2200/3907, reward: 3.430, loss: -0.1591, took: 48.6563s
Batch 2300/3907, reward: 3.427, loss: -0.1273, took: 48.3436s
Batch 2400/3907, reward: 3.419, loss: -0.0456, took: 47.9730s
Batch 2500/3907, reward: 3.431, loss: -0.0340, took: 48.7770s
Batch 2600/3907, reward: 3.420, loss: -0.0758, took: 49.1731s
Batch 2700/3907, reward: 3.413, loss: -0.0387, took: 48.3337s
Batch 2800/3907, reward: 3.413, loss: -0.0535, took: 47.8796s
Batch 2900/3907, reward: 3.409, loss: -0.0589, took: 45.5413s
Batch 3000/3907, reward: 3.409, loss: -0.0391, took: 47.6470s
Batch 3100/3907, reward: 3.401, loss: -0.0303, took: 48.6942s
Batch 3200/3907, reward: 3.401, loss: -0.0728, took: 47.4476s
Batch 3300/3907, reward: 3.398, loss: -0.1089, took: 48.4089s
Batch 3400/3907, reward: 3.396, loss: -0.0375, took: 47.5374s
Batch 3500/3907, reward: 3.395, loss: -0.0692, took: 47.1889s
Batch 3600/3907, reward: 3.396, loss: -0.0531, took: 47.6287s
Batch 3700/3907, reward: 3.390, loss: -0.0419, took: 47.7361s
Batch 3800/3907, reward: 3.389, loss: -0.0531, took: 47.6798s
Batch 3900/3907, reward: 3.394, loss: -0.0757, took: 48.4379s
Mean epoch loss/reward: -0.0737, 3.4538, 3.3626, took: 1879.3215s (46.6359s / 100 batches)
Batch 0/3907, reward: 3.403, loss: 1.0344, took: 0.6332s
Batch 100/3907, reward: 3.385, loss: -0.0106, took: 49.0381s
Batch 200/3907, reward: 3.381, loss: -0.1604, took: 47.4769s
Batch 300/3907, reward: 3.389, loss: -0.0148, took: 49.5345s
Batch 400/3907, reward: 3.386, loss: -0.0658, took: 44.9312s
Batch 500/3907, reward: 3.382, loss: -0.0685, took: 47.9368s
Batch 600/3907, reward: 3.385, loss: -0.0328, took: 48.1305s
Batch 700/3907, reward: 3.380, loss: -0.0569, took: 44.5705s
Batch 800/3907, reward: 3.378, loss: -0.0488, took: 48.1101s
Batch 900/3907, reward: 3.377, loss: -0.0958, took: 47.3805s
Batch 1000/3907, reward: 3.372, loss: -0.0566, took: 48.7927s
Batch 1100/3907, reward: 3.381, loss: -0.0892, took: 48.4727s
Batch 1200/3907, reward: 3.368, loss: -0.0616, took: 47.8527s
Batch 1300/3907, reward: 3.373, loss: -0.0720, took: 46.8619s
Batch 1400/3907, reward: 3.373, loss: -0.0839, took: 47.9117s
Batch 1500/3907, reward: 3.372, loss: -0.0700, took: 48.3086s
Batch 1600/3907, reward: 3.365, loss: -0.0635, took: 48.5259s
Batch 1700/3907, reward: 3.361, loss: -0.0620, took: 48.3841s
Batch 1800/3907, reward: 3.363, loss: -0.0375, took: 48.2907s
Batch 1900/3907, reward: 3.363, loss: -0.0803, took: 45.9911s
Batch 2000/3907, reward: 3.365, loss: -0.0610, took: 48.1068s
Batch 2100/3907, reward: 3.363, loss: -0.0326, took: 47.6780s
Batch 2200/3907, reward: 3.360, loss: -0.0345, took: 47.9980s
Batch 2300/3907, reward: 3.358, loss: -0.0264, took: 48.3889s
Batch 2400/3907, reward: 3.357, loss: -0.0763, took: 47.8703s
Batch 2500/3907, reward: 3.357, loss: -0.0730, took: 46.8161s
Batch 2600/3907, reward: 3.359, loss: -0.0808, took: 48.9644s
Batch 2700/3907, reward: 3.356, loss: -0.0709, took: 47.3819s
Batch 2800/3907, reward: 3.361, loss: -0.0290, took: 48.1311s
Batch 2900/3907, reward: 3.355, loss: -0.0732, took: 45.8201s
Batch 3000/3907, reward: 3.352, loss: -0.0509, took: 47.1639s
Batch 3100/3907, reward: 3.358, loss: -0.0797, took: 47.9418s
Batch 3200/3907, reward: 3.349, loss: -0.0481, took: 47.9930s
Batch 3300/3907, reward: 3.355, loss: -0.0905, took: 48.7556s
Batch 3400/3907, reward: 3.358, loss: -0.0387, took: 48.8698s
Batch 3500/3907, reward: 3.350, loss: -0.0520, took: 47.9306s
Batch 3600/3907, reward: 3.350, loss: -0.0569, took: 47.3173s
Batch 3700/3907, reward: 3.346, loss: -0.0736, took: 47.8457s
Batch 3800/3907, reward: 3.353, loss: -0.0367, took: 47.6988s
Batch 3900/3907, reward: 3.349, loss: -0.0708, took: 48.3212s
Mean epoch loss/reward: -0.0609, 3.3653, 3.3212, took: 1878.2006s (46.6032s / 100 batches)
Batch 0/3907, reward: 3.351, loss: 0.3690, took: 0.5545s
Batch 100/3907, reward: 3.347, loss: 0.0047, took: 48.3327s
Batch 200/3907, reward: 3.355, loss: -0.0461, took: 48.5523s
Batch 300/3907, reward: 3.346, loss: -0.0893, took: 48.6449s
Batch 400/3907, reward: 3.342, loss: -0.0667, took: 47.3119s
Batch 500/3907, reward: 3.344, loss: -0.0033, took: 46.6400s
Batch 600/3907, reward: 3.339, loss: -0.0612, took: 48.2978s
Batch 700/3907, reward: 3.343, loss: -0.0634, took: 45.1779s
Batch 800/3907, reward: 3.340, loss: -0.0197, took: 47.4933s
Batch 900/3907, reward: 3.344, loss: -0.0377, took: 48.2107s
Batch 1000/3907, reward: 3.342, loss: -0.0472, took: 47.6176s
Batch 1100/3907, reward: 3.341, loss: -0.0460, took: 48.2104s
Batch 1200/3907, reward: 3.342, loss: -0.0814, took: 48.9463s
Batch 1300/3907, reward: 3.339, loss: -0.0208, took: 47.9244s
Batch 1400/3907, reward: 3.339, loss: -0.0381, took: 48.8342s
Batch 1500/3907, reward: 3.339, loss: -0.0510, took: 48.5911s
Batch 1600/3907, reward: 3.342, loss: -0.0547, took: 48.6500s
Batch 1700/3907, reward: 3.336, loss: -0.0417, took: 48.8098s
Batch 1800/3907, reward: 3.341, loss: -0.0300, took: 46.7989s
Batch 1900/3907, reward: 3.338, loss: -0.0718, took: 46.0095s
Batch 2000/3907, reward: 3.338, loss: -0.0118, took: 47.7204s
Batch 2100/3907, reward: 3.340, loss: -0.0332, took: 49.1950s
Batch 2200/3907, reward: 3.340, loss: -0.0571, took: 48.3608s
Batch 2300/3907, reward: 3.334, loss: -0.0471, took: 49.2669s
Batch 2400/3907, reward: 3.335, loss: -0.0610, took: 48.9305s
Batch 2500/3907, reward: 3.334, loss: -0.0610, took: 48.8158s
Batch 2600/3907, reward: 3.332, loss: -0.0623, took: 46.9239s
Batch 2700/3907, reward: 3.334, loss: -0.0238, took: 48.1377s
Batch 2800/3907, reward: 3.335, loss: -0.0431, took: 47.6845s
Batch 2900/3907, reward: 3.332, loss: -0.0359, took: 47.5834s
Batch 3000/3907, reward: 3.330, loss: -0.0519, took: 45.8865s
Batch 3100/3907, reward: 3.332, loss: -0.0288, took: 48.4024s
Batch 3200/3907, reward: 3.340, loss: -0.0481, took: 47.8046s
Batch 3300/3907, reward: 3.336, loss: -0.0382, took: 49.0469s
Batch 3400/3907, reward: 3.331, loss: -0.0309, took: 47.9465s
Batch 3500/3907, reward: 3.336, loss: -0.0505, took: 46.9292s
Batch 3600/3907, reward: 3.331, loss: -0.0421, took: 49.2495s
Batch 3700/3907, reward: 3.330, loss: -0.0534, took: 47.9141s
Batch 3800/3907, reward: 3.327, loss: -0.0415, took: 48.3647s
Batch 3900/3907, reward: 3.332, loss: -0.0493, took: 48.9818s
Mean epoch loss/reward: -0.0445, 3.3379, 3.3120, took: 1887.2502s (46.8188s / 100 batches)
Batch 0/3907, reward: 3.350, loss: -0.2098, took: 0.6332s
Batch 100/3907, reward: 3.333, loss: -0.0414, took: 47.5161s
Batch 200/3907, reward: 3.330, loss: -0.0179, took: 48.1260s
Batch 300/3907, reward: 3.335, loss: -0.0575, took: 49.3931s
Batch 400/3907, reward: 3.335, loss: -0.0573, took: 45.2711s
Batch 500/3907, reward: 3.325, loss: -0.0114, took: 47.5016s
Batch 600/3907, reward: 3.329, loss: -0.0472, took: 48.2098s
Batch 700/3907, reward: 3.327, loss: -0.0353, took: 46.1062s
Batch 800/3907, reward: 3.327, loss: -0.0491, took: 47.9252s
Batch 900/3907, reward: 3.327, loss: -0.0294, took: 47.8329s
Batch 1000/3907, reward: 3.326, loss: -0.0387, took: 49.1419s
Batch 1100/3907, reward: 3.324, loss: -0.0310, took: 49.4960s
Batch 1200/3907, reward: 3.325, loss: -0.0392, took: 47.2840s
Batch 1300/3907, reward: 3.328, loss: -0.0140, took: 48.1061s
Batch 1400/3907, reward: 3.327, loss: -0.0289, took: 48.5408s
Batch 1500/3907, reward: 3.328, loss: -0.0437, took: 48.1488s
Batch 1600/3907, reward: 3.326, loss: -0.0481, took: 47.8133s
Batch 1700/3907, reward: 3.323, loss: -0.0378, took: 48.1347s
Batch 1800/3907, reward: 3.324, loss: -0.0426, took: 45.5965s
Batch 1900/3907, reward: 3.326, loss: -0.0455, took: 46.5798s
Batch 2000/3907, reward: 3.323, loss: -0.0354, took: 47.3080s
Batch 2100/3907, reward: 3.322, loss: -0.0453, took: 47.5816s
Batch 2200/3907, reward: 3.328, loss: -0.0359, took: 48.4377s
Batch 2300/3907, reward: 3.327, loss: -0.0280, took: 48.2187s
Batch 2400/3907, reward: 3.330, loss: -0.0322, took: 49.7485s
Batch 2500/3907, reward: 3.321, loss: -0.0437, took: 48.1989s
Batch 2600/3907, reward: 3.322, loss: -0.0317, took: 48.0921s
Batch 2700/3907, reward: 3.322, loss: -0.0169, took: 48.9548s
Batch 2800/3907, reward: 3.323, loss: -0.0309, took: 49.2274s
Batch 2900/3907, reward: 3.328, loss: -0.0306, took: 45.8304s
Batch 3000/3907, reward: 3.325, loss: -0.0252, took: 46.8373s
Batch 3100/3907, reward: 3.321, loss: -0.0407, took: 47.8057s
Batch 3200/3907, reward: 3.321, loss: -0.0407, took: 48.0320s
Batch 3300/3907, reward: 3.322, loss: -0.0310, took: 47.8950s
Batch 3400/3907, reward: 3.321, loss: -0.0279, took: 47.5222s
Batch 3500/3907, reward: 3.324, loss: -0.0460, took: 48.8048s
Batch 3600/3907, reward: 3.322, loss: -0.0337, took: 47.3332s
Batch 3700/3907, reward: 3.320, loss: -0.0478, took: 47.8827s
Batch 3800/3907, reward: 3.320, loss: -0.0454, took: 47.1568s
Batch 3900/3907, reward: 3.323, loss: -0.0202, took: 48.6854s
Mean epoch loss/reward: -0.0362, 3.3253, 3.2936, took: 1881.0681s (46.6728s / 100 batches)
Batch 0/3907, reward: 3.314, loss: 0.1639, took: 0.6353s
Batch 100/3907, reward: 3.328, loss: -0.0146, took: 47.5864s
Batch 200/3907, reward: 3.319, loss: -0.0343, took: 48.2745s
Batch 300/3907, reward: 3.321, loss: -0.0236, took: 48.5370s
Batch 400/3907, reward: 3.320, loss: -0.0319, took: 46.1155s
Batch 500/3907, reward: 3.319, loss: -0.0546, took: 46.5117s
Batch 600/3907, reward: 3.322, loss: -0.0204, took: 48.6438s
Batch 700/3907, reward: 3.320, loss: -0.0320, took: 46.3087s
Batch 800/3907, reward: 3.324, loss: -0.0333, took: 47.6839s
Batch 900/3907, reward: 3.320, loss: -0.0457, took: 48.9116s
Batch 1000/3907, reward: 3.321, loss: -0.0290, took: 48.4395s
Batch 1100/3907, reward: 3.322, loss: -0.0429, took: 48.2879s
Batch 1200/3907, reward: 3.318, loss: -0.0219, took: 47.7799s
Batch 1300/3907, reward: 3.320, loss: -0.0329, took: 47.9772s
Batch 1400/3907, reward: 3.312, loss: -0.0388, took: 49.1945s
Batch 1500/3907, reward: 3.319, loss: -0.0448, took: 47.3726s
Batch 1600/3907, reward: 3.318, loss: -0.0251, took: 48.0511s
Batch 1700/3907, reward: 3.320, loss: -0.0419, took: 47.3146s
Batch 1800/3907, reward: 3.320, loss: -0.0196, took: 46.2946s
Batch 1900/3907, reward: 3.316, loss: -0.0221, took: 48.4286s
Batch 2000/3907, reward: 3.319, loss: -0.0389, took: 48.7756s
Batch 2100/3907, reward: 3.323, loss: -0.0393, took: 48.5406s
Batch 2200/3907, reward: 3.318, loss: -0.0180, took: 47.4954s
Batch 2300/3907, reward: 3.321, loss: -0.0429, took: 48.5207s
Batch 2400/3907, reward: 3.318, loss: -0.0436, took: 48.4185s
Batch 2500/3907, reward: 3.316, loss: -0.0316, took: 47.5367s
Batch 2600/3907, reward: 3.317, loss: -0.0417, took: 47.8196s
Batch 2700/3907, reward: 3.314, loss: -0.0279, took: 47.5476s
Batch 2800/3907, reward: 3.314, loss: -0.0307, took: 48.5520s
Batch 2900/3907, reward: 3.316, loss: -0.0382, took: 45.3981s
Batch 3000/3907, reward: 3.317, loss: -0.0349, took: 48.6281s
Batch 3100/3907, reward: 3.318, loss: -0.0433, took: 47.5845s
Batch 3200/3907, reward: 3.323, loss: -0.0173, took: 48.0741s
Batch 3300/3907, reward: 3.316, loss: -0.0333, took: 48.4008s
Batch 3400/3907, reward: 3.316, loss: -0.0340, took: 48.8952s
Batch 3500/3907, reward: 3.327, loss: -0.0296, took: 47.1733s
Batch 3600/3907, reward: 3.320, loss: -0.0299, took: 47.7181s
Batch 3700/3907, reward: 3.315, loss: -0.0255, took: 47.8680s
Batch 3800/3907, reward: 3.320, loss: -0.0267, took: 47.0873s
Batch 3900/3907, reward: 3.323, loss: -0.0218, took: 48.4941s
Mean epoch loss/reward: -0.0321, 3.3192, 3.2919, took: 1881.7202s (46.6719s / 100 batches)
Batch 0/3907, reward: 3.302, loss: -0.0442, took: 0.7009s
Batch 100/3907, reward: 3.315, loss: -0.0470, took: 48.2758s
Batch 200/3907, reward: 3.313, loss: -0.0415, took: 49.0174s
Batch 300/3907, reward: 3.317, loss: -0.0342, took: 49.2894s
Batch 400/3907, reward: 3.312, loss: -0.0253, took: 45.8413s
Batch 500/3907, reward: 3.316, loss: -0.0290, took: 47.6624s
Batch 600/3907, reward: 3.312, loss: -0.0288, took: 47.3338s
Batch 700/3907, reward: 3.310, loss: -0.0298, took: 45.1758s
Batch 800/3907, reward: 3.317, loss: -0.0304, took: 48.2564s
Batch 900/3907, reward: 3.311, loss: -0.0170, took: 47.4877s
Batch 1000/3907, reward: 3.314, loss: -0.0324, took: 49.2348s
Batch 1100/3907, reward: 3.314, loss: -0.0284, took: 47.7791s
Batch 1200/3907, reward: 3.321, loss: -0.0313, took: 48.3922s
Batch 1300/3907, reward: 3.317, loss: -0.0249, took: 48.9319s
Batch 1400/3907, reward: 3.311, loss: -0.0396, took: 48.2571s
Batch 1500/3907, reward: 3.313, loss: -0.0340, took: 48.7879s
Batch 1600/3907, reward: 3.311, loss: -0.0192, took: 47.2394s
Batch 1700/3907, reward: 3.311, loss: -0.0170, took: 47.7522s
Batch 1800/3907, reward: 3.313, loss: -0.0221, took: 46.0264s
Batch 1900/3907, reward: 3.311, loss: -0.0318, took: 47.4348s
Batch 2000/3907, reward: 3.311, loss: -0.0259, took: 48.4200s
Batch 2100/3907, reward: 3.309, loss: -0.0336, took: 48.2079s
Batch 2200/3907, reward: 3.311, loss: -0.0368, took: 48.5732s
Batch 2300/3907, reward: 3.312, loss: -0.0210, took: 47.8810s
Batch 2400/3907, reward: 3.311, loss: -0.0311, took: 47.0325s
Batch 2500/3907, reward: 3.310, loss: -0.0282, took: 48.2179s
Batch 2600/3907, reward: 3.312, loss: -0.0342, took: 47.2955s
Batch 2700/3907, reward: 3.317, loss: -0.0293, took: 47.9845s
Batch 2800/3907, reward: 3.310, loss: -0.0318, took: 47.3091s
Batch 2900/3907, reward: 3.310, loss: -0.0323, took: 45.9473s
Batch 3000/3907, reward: 3.311, loss: -0.0289, took: 47.8096s
Batch 3100/3907, reward: 3.313, loss: -0.0330, took: 46.2685s
Batch 3200/3907, reward: 3.310, loss: -0.0161, took: 48.8759s
Batch 3300/3907, reward: 3.308, loss: -0.0322, took: 47.9719s
Batch 3400/3907, reward: 3.307, loss: -0.0280, took: 49.4637s
Batch 3500/3907, reward: 3.315, loss: -0.0174, took: 47.6828s
Batch 3600/3907, reward: 3.313, loss: -0.0197, took: 46.9839s
Batch 3700/3907, reward: 3.308, loss: -0.0369, took: 47.4849s
Batch 3800/3907, reward: 3.309, loss: -0.0195, took: 47.2821s
Batch 3900/3907, reward: 3.306, loss: -0.0293, took: 47.8518s
Mean epoch loss/reward: -0.0289, 3.3120, 3.2880, took: 1877.3688s (46.5855s / 100 batches)
Batch 0/3907, reward: 3.317, loss: 0.1154, took: 0.8911s
Batch 100/3907, reward: 3.307, loss: -0.0219, took: 48.2780s
Batch 200/3907, reward: 3.314, loss: -0.0287, took: 48.2772s
Batch 300/3907, reward: 3.306, loss: -0.0313, took: 48.3853s
Batch 400/3907, reward: 3.315, loss: -0.0230, took: 46.9582s
Batch 500/3907, reward: 3.310, loss: -0.0209, took: 46.4521s
Batch 600/3907, reward: 3.307, loss: -0.0317, took: 48.2477s
Batch 700/3907, reward: 3.309, loss: -0.0341, took: 45.4560s
Batch 800/3907, reward: 3.311, loss: -0.0309, took: 48.7788s
Batch 900/3907, reward: 3.308, loss: -0.0267, took: 49.5754s
Batch 1000/3907, reward: 3.305, loss: -0.0229, took: 49.1138s
Batch 1100/3907, reward: 3.310, loss: -0.0211, took: 47.7541s
Batch 1200/3907, reward: 3.307, loss: -0.0333, took: 48.3622s
Batch 1300/3907, reward: 3.313, loss: -0.0264, took: 47.8014s
Batch 1400/3907, reward: 3.306, loss: -0.0286, took: 47.9931s
Batch 1500/3907, reward: 3.311, loss: -0.0197, took: 49.7681s
Batch 1600/3907, reward: 3.308, loss: -0.0273, took: 48.2169s
Batch 1700/3907, reward: 3.310, loss: -0.0249, took: 46.7234s
Batch 1800/3907, reward: 3.310, loss: -0.0172, took: 43.7696s
Batch 1900/3907, reward: 3.303, loss: -0.0281, took: 48.2493s
Batch 2000/3907, reward: 3.312, loss: -0.0206, took: 48.3993s
Batch 2100/3907, reward: 3.311, loss: -0.0233, took: 47.2820s
Batch 2200/3907, reward: 3.307, loss: -0.0273, took: 47.2214s
Batch 2300/3907, reward: 3.306, loss: -0.0311, took: 47.7112s
Batch 2400/3907, reward: 3.306, loss: -0.0233, took: 47.8810s
Batch 2500/3907, reward: 3.314, loss: -0.0353, took: 48.6070s
Batch 2600/3907, reward: 3.305, loss: -0.0186, took: 46.8266s
Batch 2700/3907, reward: 3.303, loss: -0.0239, took: 48.3513s
Batch 2800/3907, reward: 3.310, loss: -0.0299, took: 47.5773s
Batch 2900/3907, reward: 3.306, loss: -0.0351, took: 45.1749s
Batch 3000/3907, reward: 3.305, loss: -0.0322, took: 48.3831s
Batch 3100/3907, reward: 3.305, loss: -0.0305, took: 47.7762s
Batch 3200/3907, reward: 3.308, loss: -0.0221, took: 48.5237s
Batch 3300/3907, reward: 3.304, loss: -0.0342, took: 48.0512s
Batch 3400/3907, reward: 3.307, loss: -0.0123, took: 48.1648s
Batch 3500/3907, reward: 3.307, loss: -0.0244, took: 48.0767s
Batch 3600/3907, reward: 3.306, loss: -0.0338, took: 48.2825s
Batch 3700/3907, reward: 3.308, loss: -0.0200, took: 48.6683s
Batch 3800/3907, reward: 3.305, loss: -0.0286, took: 47.7672s
Batch 3900/3907, reward: 3.310, loss: -0.0212, took: 47.5125s
Mean epoch loss/reward: -0.0262, 3.3080, 3.2865, took: 1880.6271s (46.6323s / 100 batches)
Batch 0/3907, reward: 3.327, loss: -0.0262, took: 0.7113s
Batch 100/3907, reward: 3.302, loss: -0.0204, took: 47.8616s
Batch 200/3907, reward: 3.305, loss: -0.0258, took: 46.8471s
Batch 300/3907, reward: 3.304, loss: -0.0207, took: 47.4319s
Batch 400/3907, reward: 3.310, loss: -0.0286, took: 45.5900s
Batch 500/3907, reward: 3.305, loss: -0.0168, took: 47.1786s
Batch 600/3907, reward: 3.303, loss: -0.0274, took: 48.9550s
Batch 700/3907, reward: 3.305, loss: -0.0309, took: 44.8481s
Batch 800/3907, reward: 3.302, loss: -0.0217, took: 46.7985s
Batch 900/3907, reward: 3.302, loss: -0.0312, took: 47.4025s
Batch 1000/3907, reward: 3.306, loss: -0.0299, took: 47.9787s
Batch 1100/3907, reward: 3.304, loss: -0.0250, took: 48.2309s
Batch 1200/3907, reward: 3.300, loss: -0.0195, took: 48.3284s
Batch 1300/3907, reward: 3.302, loss: -0.0250, took: 48.3140s
Batch 1400/3907, reward: 3.304, loss: -0.0184, took: 47.5089s
Batch 1500/3907, reward: 3.303, loss: -0.0190, took: 47.8763s
Batch 1600/3907, reward: 3.303, loss: -0.0250, took: 48.6836s
Batch 1700/3907, reward: 3.308, loss: -0.0219, took: 47.9831s
Batch 1800/3907, reward: 3.305, loss: -0.0233, took: 44.5350s
Batch 1900/3907, reward: 3.305, loss: -0.0193, took: 48.9639s
Batch 2000/3907, reward: 3.307, loss: -0.0241, took: 47.8325s
Batch 2100/3907, reward: 3.302, loss: -0.0215, took: 47.8944s
Batch 2200/3907, reward: 3.302, loss: -0.0220, took: 48.1809s
Batch 2300/3907, reward: 3.304, loss: -0.0212, took: 46.9159s
Batch 2400/3907, reward: 3.303, loss: -0.0262, took: 47.8861s
Batch 2500/3907, reward: 3.310, loss: -0.0140, took: 48.6406s
Batch 2600/3907, reward: 3.304, loss: -0.0133, took: 47.5735s
Batch 2700/3907, reward: 3.302, loss: -0.0123, took: 47.9844s
Batch 2800/3907, reward: 3.301, loss: -0.0216, took: 47.7828s
Batch 2900/3907, reward: 3.303, loss: -0.0222, took: 46.1015s
Batch 3000/3907, reward: 3.300, loss: -0.0210, took: 48.2864s
Batch 3100/3907, reward: 3.302, loss: -0.0216, took: 46.9276s
Batch 3200/3907, reward: 3.308, loss: -0.0277, took: 48.2923s
Batch 3300/3907, reward: 3.305, loss: -0.0210, took: 47.3800s
Batch 3400/3907, reward: 3.303, loss: -0.0186, took: 48.0631s
Batch 3500/3907, reward: 3.306, loss: -0.0274, took: 48.3191s
Batch 3600/3907, reward: 3.307, loss: -0.0189, took: 48.0325s
Batch 3700/3907, reward: 3.303, loss: -0.0261, took: 47.7043s
Batch 3800/3907, reward: 3.300, loss: -0.0216, took: 46.7839s
Batch 3900/3907, reward: 3.301, loss: -0.0252, took: 48.2971s
Mean epoch loss/reward: -0.0225, 3.3039, 3.2799, took: 1871.1696s (46.4226s / 100 batches)
Batch 0/3907, reward: 3.294, loss: -0.1082, took: 0.6317s
Batch 100/3907, reward: 3.303, loss: -0.0211, took: 46.5818s
Batch 200/3907, reward: 3.303, loss: -0.0269, took: 47.3444s
Batch 300/3907, reward: 3.304, loss: -0.0277, took: 48.1468s
Batch 400/3907, reward: 3.298, loss: -0.0158, took: 45.8869s
Batch 500/3907, reward: 3.299, loss: -0.0225, took: 46.9914s
Batch 600/3907, reward: 3.307, loss: -0.0164, took: 47.9650s
Batch 700/3907, reward: 3.302, loss: -0.0288, took: 45.3879s
Batch 800/3907, reward: 3.297, loss: -0.0204, took: 47.1631s
Batch 900/3907, reward: 3.302, loss: -0.0259, took: 47.0702s
Batch 1000/3907, reward: 3.301, loss: -0.0227, took: 49.3499s
Batch 1100/3907, reward: 3.304, loss: -0.0213, took: 48.0899s
Batch 1200/3907, reward: 3.302, loss: -0.0149, took: 47.1370s
Batch 1300/3907, reward: 3.310, loss: -0.0226, took: 48.1354s
Batch 1400/3907, reward: 3.297, loss: -0.0160, took: 48.1589s
Batch 1500/3907, reward: 3.301, loss: -0.0156, took: 47.6912s
Batch 1600/3907, reward: 3.300, loss: -0.0184, took: 48.0381s
Batch 1700/3907, reward: 3.298, loss: -0.0206, took: 48.8413s
Batch 1800/3907, reward: 3.301, loss: -0.0232, took: 44.5687s
Batch 1900/3907, reward: 3.301, loss: -0.0191, took: 48.0902s
Batch 2000/3907, reward: 3.302, loss: -0.0215, took: 47.9536s
Batch 2100/3907, reward: 3.306, loss: -0.0210, took: 48.4794s
Batch 2200/3907, reward: 3.300, loss: -0.0277, took: 46.8694s
Batch 2300/3907, reward: 3.302, loss: -0.0255, took: 48.5413s
Batch 2400/3907, reward: 3.303, loss: -0.0229, took: 47.4291s
Batch 2500/3907, reward: 3.302, loss: -0.0161, took: 46.6931s
Batch 2600/3907, reward: 3.299, loss: -0.0237, took: 47.7619s
Batch 2700/3907, reward: 3.299, loss: -0.0219, took: 48.7819s
Batch 2800/3907, reward: 3.302, loss: -0.0158, took: 47.7654s
Batch 2900/3907, reward: 3.305, loss: -0.0175, took: 45.0548s
Batch 3000/3907, reward: 3.299, loss: -0.0189, took: 46.6600s
Batch 3100/3907, reward: 3.305, loss: -0.0150, took: 48.3826s
Batch 3200/3907, reward: 3.299, loss: -0.0252, took: 47.6689s
Batch 3300/3907, reward: 3.296, loss: -0.0230, took: 47.7564s
Batch 3400/3907, reward: 3.302, loss: -0.0189, took: 48.5728s
Batch 3500/3907, reward: 3.301, loss: -0.0237, took: 48.1309s
Batch 3600/3907, reward: 3.300, loss: -0.0200, took: 48.4843s
Batch 3700/3907, reward: 3.299, loss: -0.0199, took: 48.2251s
Batch 3800/3907, reward: 3.299, loss: -0.0247, took: 47.1216s
Batch 3900/3907, reward: 3.300, loss: -0.0216, took: 48.3926s
Mean epoch loss/reward: -0.0211, 3.3012, 3.2787, took: 1870.0507s (46.3999s / 100 batches)
Batch 0/3907, reward: 3.316, loss: -0.0367, took: 0.5885s
Batch 100/3907, reward: 3.296, loss: -0.0295, took: 48.9767s
Batch 200/3907, reward: 3.301, loss: -0.0166, took: 47.5076s
Batch 300/3907, reward: 3.299, loss: -0.0204, took: 48.8641s
Batch 400/3907, reward: 3.296, loss: -0.0206, took: 45.1387s
Batch 500/3907, reward: 3.293, loss: -0.0204, took: 48.6231s
Batch 600/3907, reward: 3.298, loss: -0.0201, took: 47.4439s
Batch 700/3907, reward: 3.298, loss: -0.0289, took: 44.8191s
Batch 800/3907, reward: 3.303, loss: -0.0198, took: 48.0115s
Batch 900/3907, reward: 3.298, loss: -0.0182, took: 49.4539s
Batch 1000/3907, reward: 3.300, loss: -0.0213, took: 48.7473s
Batch 1100/3907, reward: 3.298, loss: -0.0194, took: 48.3388s
Batch 1200/3907, reward: 3.299, loss: -0.0272, took: 48.6709s
Batch 1300/3907, reward: 3.301, loss: -0.0179, took: 47.7960s
Batch 1400/3907, reward: 3.300, loss: -0.0222, took: 48.1350s
Batch 1500/3907, reward: 3.301, loss: -0.0198, took: 48.7015s
Batch 1600/3907, reward: 3.300, loss: -0.0188, took: 47.4547s
Batch 1700/3907, reward: 3.300, loss: -0.0229, took: 48.0659s
Batch 1800/3907, reward: 3.300, loss: -0.0199, took: 45.6181s
Batch 1900/3907, reward: 3.299, loss: -0.0148, took: 46.7201s
Batch 2000/3907, reward: 3.299, loss: -0.0275, took: 47.9224s
Batch 2100/3907, reward: 3.300, loss: -0.0204, took: 47.7080s
Batch 2200/3907, reward: 3.300, loss: -0.0184, took: 48.9869s
Batch 2300/3907, reward: 3.299, loss: -0.0190, took: 47.3644s
Batch 2400/3907, reward: 3.302, loss: -0.0238, took: 47.8157s
Batch 2500/3907, reward: 3.300, loss: -0.0121, took: 48.1969s
Batch 2600/3907, reward: 3.296, loss: -0.0215, took: 47.3037s
Batch 2700/3907, reward: 3.298, loss: -0.0218, took: 47.3618s
Batch 2800/3907, reward: 3.296, loss: -0.0170, took: 48.7644s
Batch 2900/3907, reward: 3.303, loss: -0.0232, took: 44.7766s
Batch 3000/3907, reward: 3.294, loss: -0.0189, took: 47.5443s
Batch 3100/3907, reward: 3.302, loss: -0.0163, took: 47.0203s
Batch 3200/3907, reward: 3.300, loss: -0.0246, took: 47.8999s
Batch 3300/3907, reward: 3.298, loss: -0.0220, took: 47.1720s
Batch 3400/3907, reward: 3.303, loss: -0.0183, took: 48.2459s
Batch 3500/3907, reward: 3.293, loss: -0.0184, took: 47.8613s
Batch 3600/3907, reward: 3.297, loss: -0.0156, took: 48.2134s
Batch 3700/3907, reward: 3.297, loss: -0.0198, took: 46.4683s
Batch 3800/3907, reward: 3.299, loss: -0.0192, took: 48.1182s
Batch 3900/3907, reward: 3.297, loss: -0.0225, took: 48.4407s
Mean epoch loss/reward: -0.0205, 3.2988, 3.2767, took: 1874.7093s (46.5215s / 100 batches)
Batch 0/3907, reward: 3.294, loss: -0.0099, took: 0.6599s
Batch 100/3907, reward: 3.298, loss: -0.0226, took: 47.6084s
Batch 200/3907, reward: 3.295, loss: -0.0235, took: 48.3866s
Batch 300/3907, reward: 3.297, loss: -0.0175, took: 47.0951s
Batch 400/3907, reward: 3.299, loss: -0.0180, took: 46.7794s
Batch 500/3907, reward: 3.303, loss: -0.0201, took: 44.7351s
Batch 600/3907, reward: 3.299, loss: -0.0197, took: 47.5128s
Batch 700/3907, reward: 3.297, loss: -0.0170, took: 45.2287s
Batch 800/3907, reward: 3.301, loss: -0.0149, took: 47.6291s
Batch 900/3907, reward: 3.296, loss: -0.0231, took: 47.5947s
Batch 1000/3907, reward: 3.297, loss: -0.0210, took: 47.9365s
Batch 1100/3907, reward: 3.302, loss: -0.0160, took: 47.7131s
Batch 1200/3907, reward: 3.299, loss: -0.0175, took: 48.4719s
Batch 1300/3907, reward: 3.296, loss: -0.0192, took: 47.4290s
Batch 1400/3907, reward: 3.295, loss: -0.0180, took: 47.9498s
Batch 1500/3907, reward: 3.298, loss: -0.0162, took: 46.9926s
Batch 1600/3907, reward: 3.300, loss: -0.0187, took: 48.6192s
Batch 1700/3907, reward: 3.295, loss: -0.0191, took: 48.1280s
Batch 1800/3907, reward: 3.294, loss: -0.0138, took: 44.8361s
Batch 1900/3907, reward: 3.295, loss: -0.0244, took: 47.9434s
Batch 2000/3907, reward: 3.300, loss: -0.0219, took: 46.9869s
Batch 2100/3907, reward: 3.299, loss: -0.0182, took: 47.9141s
Batch 2200/3907, reward: 3.296, loss: -0.0159, took: 47.7883s
Batch 2300/3907, reward: 3.300, loss: -0.0194, took: 47.6580s
Batch 2400/3907, reward: 3.293, loss: -0.0194, took: 46.7661s
Batch 2500/3907, reward: 3.293, loss: -0.0198, took: 47.1983s
Batch 2600/3907, reward: 3.295, loss: -0.0173, took: 47.6391s
Batch 2700/3907, reward: 3.294, loss: -0.0221, took: 47.5132s
Batch 2800/3907, reward: 3.294, loss: -0.0210, took: 47.6215s
Batch 2900/3907, reward: 3.299, loss: -0.0195, took: 47.2270s
Batch 3000/3907, reward: 3.297, loss: -0.0173, took: 46.2668s
Batch 3100/3907, reward: 3.297, loss: -0.0152, took: 47.9682s
Batch 3200/3907, reward: 3.294, loss: -0.0153, took: 48.1082s
Batch 3300/3907, reward: 3.294, loss: -0.0190, took: 48.2916s
Batch 3400/3907, reward: 3.298, loss: -0.0207, took: 47.4652s
Batch 3500/3907, reward: 3.298, loss: -0.0143, took: 47.4070s
Batch 3600/3907, reward: 3.294, loss: -0.0200, took: 48.7386s
Batch 3700/3907, reward: 3.295, loss: -0.0169, took: 48.7544s
Batch 3800/3907, reward: 3.293, loss: -0.0209, took: 48.1870s
Batch 3900/3907, reward: 3.296, loss: -0.0195, took: 48.1955s
Mean epoch loss/reward: -0.0189, 3.2969, 3.2838, took: 1867.1509s (46.3236s / 100 batches)
Batch 0/3907, reward: 3.286, loss: 0.0607, took: 0.7822s
Batch 100/3907, reward: 3.295, loss: -0.0178, took: 48.3061s
Batch 200/3907, reward: 3.298, loss: -0.0198, took: 48.3739s
Batch 300/3907, reward: 3.297, loss: -0.0169, took: 47.0742s
Batch 400/3907, reward: 3.298, loss: -0.0195, took: 46.3504s
Batch 500/3907, reward: 3.297, loss: -0.0163, took: 44.4559s
Batch 600/3907, reward: 3.297, loss: -0.0175, took: 47.5195s
Batch 700/3907, reward: 3.296, loss: -0.0191, took: 46.6227s
Batch 800/3907, reward: 3.291, loss: -0.0123, took: 47.6220s
Batch 900/3907, reward: 3.303, loss: -0.0152, took: 48.8077s
Batch 1000/3907, reward: 3.297, loss: -0.0153, took: 47.6926s
Batch 1100/3907, reward: 3.292, loss: -0.0126, took: 47.3561s
Batch 1200/3907, reward: 3.293, loss: -0.0187, took: 48.0914s
Batch 1300/3907, reward: 3.296, loss: -0.0165, took: 49.0851s
Batch 1400/3907, reward: 3.294, loss: -0.0186, took: 47.0624s
Batch 1500/3907, reward: 3.294, loss: -0.0171, took: 46.8684s
Batch 1600/3907, reward: 3.298, loss: -0.0172, took: 48.0804s
Batch 1700/3907, reward: 3.296, loss: -0.0170, took: 48.8613s
Batch 1800/3907, reward: 3.295, loss: -0.0176, took: 46.0212s
Batch 1900/3907, reward: 3.291, loss: -0.0180, took: 47.8607s
Batch 2000/3907, reward: 3.304, loss: -0.0210, took: 48.3440s
Batch 2100/3907, reward: 3.299, loss: -0.0202, took: 48.9715s
Batch 2200/3907, reward: 3.295, loss: -0.0145, took: 47.5525s
Batch 2300/3907, reward: 3.296, loss: -0.0189, took: 48.5306s
Batch 2400/3907, reward: 3.296, loss: -0.0186, took: 48.0546s
Batch 2500/3907, reward: 3.297, loss: -0.0158, took: 47.8601s
Batch 2600/3907, reward: 3.299, loss: -0.0171, took: 48.5315s
Batch 2700/3907, reward: 3.295, loss: -0.0184, took: 47.0313s
Batch 2800/3907, reward: 3.302, loss: -0.0194, took: 47.9704s
Batch 2900/3907, reward: 3.298, loss: -0.0210, took: 45.1057s
Batch 3000/3907, reward: 3.290, loss: -0.0159, took: 47.2803s
Batch 3100/3907, reward: 3.295, loss: -0.0220, took: 47.8369s
Batch 3200/3907, reward: 3.290, loss: -0.0186, took: 49.4443s
Batch 3300/3907, reward: 3.299, loss: -0.0161, took: 47.7428s
Batch 3400/3907, reward: 3.296, loss: -0.0181, took: 47.2387s
Batch 3500/3907, reward: 3.301, loss: -0.0165, took: 48.8700s
Batch 3600/3907, reward: 3.295, loss: -0.0180, took: 47.0965s
Batch 3700/3907, reward: 3.295, loss: -0.0186, took: 47.2889s
Batch 3800/3907, reward: 3.295, loss: -0.0161, took: 47.8509s
Batch 3900/3907, reward: 3.304, loss: -0.0208, took: 48.1140s
Mean epoch loss/reward: -0.0177, 3.2965, 3.2899, took: 1874.1043s (46.4902s / 100 batches)
Batch 0/3907, reward: 3.314, loss: -0.0282, took: 0.6109s
Batch 100/3907, reward: 3.295, loss: -0.0249, took: 48.5353s
Batch 200/3907, reward: 3.296, loss: -0.0133, took: 47.2717s
Batch 300/3907, reward: 3.294, loss: -0.0147, took: 47.4171s
Batch 400/3907, reward: 3.299, loss: -0.0181, took: 47.4759s
Batch 500/3907, reward: 3.293, loss: -0.0169, took: 45.4765s
Batch 600/3907, reward: 3.293, loss: -0.0161, took: 48.4306s
Batch 700/3907, reward: 3.296, loss: -0.0177, took: 44.7064s
Batch 800/3907, reward: 3.288, loss: -0.0160, took: 47.6731s
Batch 900/3907, reward: 3.294, loss: -0.0168, took: 48.4278s
Batch 1000/3907, reward: 3.294, loss: -0.0212, took: 46.9491s
Batch 1100/3907, reward: 3.292, loss: -0.0200, took: 48.4690s
Batch 1200/3907, reward: 3.295, loss: -0.0179, took: 47.3934s
Batch 1300/3907, reward: 3.293, loss: -0.0181, took: 47.7522s
Batch 1400/3907, reward: 3.294, loss: -0.0141, took: 48.0811s
Batch 1500/3907, reward: 3.293, loss: -0.0249, took: 47.1981s
Batch 1600/3907, reward: 3.289, loss: -0.0192, took: 47.3842s
Batch 1700/3907, reward: 3.291, loss: -0.0195, took: 48.0276s
Batch 1800/3907, reward: 3.296, loss: -0.0187, took: 45.0181s
Batch 1900/3907, reward: 3.293, loss: -0.0222, took: 47.3657s
Batch 2000/3907, reward: 3.296, loss: -0.0195, took: 47.9849s
Batch 2100/3907, reward: 3.297, loss: -0.0140, took: 48.4418s
Batch 2200/3907, reward: 3.290, loss: -0.0166, took: 47.5261s
Batch 2300/3907, reward: 3.293, loss: -0.0144, took: 48.6924s
Batch 2400/3907, reward: 3.294, loss: -0.0190, took: 48.7643s
Batch 2500/3907, reward: 3.292, loss: -0.0184, took: 48.5589s
Batch 2600/3907, reward: 3.292, loss: -0.0167, took: 46.9599s
Batch 2700/3907, reward: 3.297, loss: -0.0217, took: 47.9978s
Batch 2800/3907, reward: 3.291, loss: -0.0183, took: 47.8278s
Batch 2900/3907, reward: 3.294, loss: -0.0157, took: 43.8040s
Batch 3000/3907, reward: 3.294, loss: -0.0183, took: 47.8427s
Batch 3100/3907, reward: 3.297, loss: -0.0187, took: 48.4245s
Batch 3200/3907, reward: 3.290, loss: -0.0185, took: 48.4091s
Batch 3300/3907, reward: 3.295, loss: -0.0150, took: 47.9866s
Batch 3400/3907, reward: 3.297, loss: -0.0198, took: 47.4383s
Batch 3500/3907, reward: 3.294, loss: -0.0188, took: 47.6315s
Batch 3600/3907, reward: 3.293, loss: -0.0160, took: 47.9217s
Batch 3700/3907, reward: 3.293, loss: -0.0171, took: 47.4662s
Batch 3800/3907, reward: 3.290, loss: -0.0152, took: 47.5597s
Batch 3900/3907, reward: 3.291, loss: -0.0146, took: 47.9990s
Mean epoch loss/reward: -0.0179, 3.2935, 3.2763, took: 1869.0423s (46.3725s / 100 batches)
Batch 0/3907, reward: 3.298, loss: 0.0243, took: 0.6438s
Batch 100/3907, reward: 3.294, loss: -0.0192, took: 49.1041s
Batch 200/3907, reward: 3.290, loss: -0.0161, took: 48.2316s
Batch 300/3907, reward: 3.294, loss: -0.0205, took: 47.7348s
Batch 400/3907, reward: 3.295, loss: -0.0179, took: 47.0811s
Batch 500/3907, reward: 3.291, loss: -0.0118, took: 45.4063s
Batch 600/3907, reward: 3.289, loss: -0.0174, took: 48.2130s
Batch 700/3907, reward: 3.290, loss: -0.0137, took: 44.7902s
Batch 800/3907, reward: 3.295, loss: -0.0215, took: 47.2901s
Batch 900/3907, reward: 3.293, loss: -0.0159, took: 47.2087s
Batch 1000/3907, reward: 3.291, loss: -0.0191, took: 47.9106s
Batch 1100/3907, reward: 3.293, loss: -0.0168, took: 46.9606s
Batch 1200/3907, reward: 3.295, loss: -0.0182, took: 47.8854s
Batch 1300/3907, reward: 3.293, loss: -0.0169, took: 46.9230s
Batch 1400/3907, reward: 3.292, loss: -0.0138, took: 47.6877s
Batch 1500/3907, reward: 3.288, loss: -0.0180, took: 48.4226s
Batch 1600/3907, reward: 3.290, loss: -0.0149, took: 48.3325s
Batch 1700/3907, reward: 3.293, loss: -0.0169, took: 49.1432s
Batch 1800/3907, reward: 3.292, loss: -0.0176, took: 45.9284s
Batch 1900/3907, reward: 3.293, loss: -0.0171, took: 48.0953s
Batch 2000/3907, reward: 3.289, loss: -0.0164, took: 47.4931s
Batch 2100/3907, reward: 3.294, loss: -0.0140, took: 48.0846s
Batch 2200/3907, reward: 3.289, loss: -0.0154, took: 49.2818s
Batch 2300/3907, reward: 3.296, loss: -0.0172, took: 48.5535s
Batch 2400/3907, reward: 3.295, loss: -0.0192, took: 48.2310s
Batch 2500/3907, reward: 3.292, loss: -0.0159, took: 49.0831s
Batch 2600/3907, reward: 3.301, loss: -0.0133, took: 47.7304s
Batch 2700/3907, reward: 3.291, loss: -0.0162, took: 47.9049s
Batch 2800/3907, reward: 3.291, loss: -0.0143, took: 48.1442s
Batch 2900/3907, reward: 3.288, loss: -0.0166, took: 45.2938s
Batch 3000/3907, reward: 3.291, loss: -0.0201, took: 47.6407s
Batch 3100/3907, reward: 3.292, loss: -0.0168, took: 47.7251s
Batch 3200/3907, reward: 3.286, loss: -0.0163, took: 48.2389s
Batch 3300/3907, reward: 3.291, loss: -0.0189, took: 48.3145s
Batch 3400/3907, reward: 3.290, loss: -0.0175, took: 48.2782s
Batch 3500/3907, reward: 3.294, loss: -0.0160, took: 47.2431s
Batch 3600/3907, reward: 3.293, loss: -0.0157, took: 47.1954s
Batch 3700/3907, reward: 3.291, loss: -0.0168, took: 47.7841s
Batch 3800/3907, reward: 3.291, loss: -0.0151, took: 47.7975s
Batch 3900/3907, reward: 3.295, loss: -0.0173, took: 47.8451s
Mean epoch loss/reward: -0.0167, 3.2920, 3.2726, took: 1874.7936s (46.5214s / 100 batches)
Batch 0/3907, reward: 3.297, loss: -0.0428, took: 0.9051s
Batch 100/3907, reward: 3.291, loss: -0.0154, took: 47.7184s
Batch 200/3907, reward: 3.290, loss: -0.0172, took: 48.8467s
Batch 300/3907, reward: 3.290, loss: -0.0154, took: 47.5733s
Batch 400/3907, reward: 3.293, loss: -0.0165, took: 45.7374s
Batch 500/3907, reward: 3.294, loss: -0.0152, took: 46.6078s
Batch 600/3907, reward: 3.292, loss: -0.0163, took: 47.4234s
Batch 700/3907, reward: 3.291, loss: -0.0180, took: 44.7180s
Batch 800/3907, reward: 3.294, loss: -0.0155, took: 46.5413s
Batch 900/3907, reward: 3.296, loss: -0.0143, took: 46.6776s
Batch 1000/3907, reward: 3.294, loss: -0.0155, took: 47.0444s
Batch 1100/3907, reward: 3.291, loss: -0.0181, took: 47.2341s
Batch 1200/3907, reward: 3.289, loss: -0.0134, took: 47.3373s
Batch 1300/3907, reward: 3.291, loss: -0.0165, took: 48.1793s
Batch 1400/3907, reward: 3.297, loss: -0.0134, took: 49.1310s
Batch 1500/3907, reward: 3.291, loss: -0.0176, took: 47.6832s
Batch 1600/3907, reward: 3.289, loss: -0.0149, took: 47.6285s
Batch 1700/3907, reward: 3.291, loss: -0.0155, took: 46.9868s
Batch 1800/3907, reward: 3.294, loss: -0.0204, took: 46.1921s
Batch 1900/3907, reward: 3.292, loss: -0.0173, took: 47.4958s
Batch 2000/3907, reward: 3.286, loss: -0.0197, took: 49.3715s
Batch 2100/3907, reward: 3.285, loss: -0.0178, took: 47.5002s
Batch 2200/3907, reward: 3.289, loss: -0.0165, took: 47.7593s
Batch 2300/3907, reward: 3.290, loss: -0.0143, took: 48.1108s
Batch 2400/3907, reward: 3.293, loss: -0.0147, took: 47.4597s
Batch 2500/3907, reward: 3.291, loss: -0.0176, took: 48.6593s
Batch 2600/3907, reward: 3.292, loss: -0.0154, took: 47.0977s
Batch 2700/3907, reward: 3.289, loss: -0.0173, took: 47.8060s
Batch 2800/3907, reward: 3.290, loss: -0.0183, took: 47.9536s
Batch 2900/3907, reward: 3.289, loss: -0.0171, took: 45.5273s
Batch 3000/3907, reward: 3.290, loss: -0.0181, took: 47.3592s
Batch 3100/3907, reward: 3.289, loss: -0.0200, took: 48.3741s
Batch 3200/3907, reward: 3.292, loss: -0.0161, took: 48.5246s
Batch 3300/3907, reward: 3.293, loss: -0.0165, took: 47.6428s
Batch 3400/3907, reward: 3.291, loss: -0.0139, took: 48.7605s
Batch 3500/3907, reward: 3.295, loss: -0.0184, took: 47.4030s
Batch 3600/3907, reward: 3.286, loss: -0.0152, took: 48.4553s
Batch 3700/3907, reward: 3.288, loss: -0.0126, took: 48.1679s
Batch 3800/3907, reward: 3.286, loss: -0.0148, took: 46.5699s
Batch 3900/3907, reward: 3.290, loss: -0.0143, took: 48.0887s
Mean epoch loss/reward: -0.0163, 3.2909, 3.2719, took: 1869.0735s (46.3563s / 100 batches)
Batch 0/3907, reward: 3.305, loss: -0.0079, took: 0.6625s
Batch 100/3907, reward: 3.288, loss: -0.0154, took: 48.3045s
Batch 200/3907, reward: 3.289, loss: -0.0166, took: 46.6120s
Batch 300/3907, reward: 3.289, loss: -0.0141, took: 48.6084s
Batch 400/3907, reward: 3.289, loss: -0.0145, took: 45.6018s
Batch 500/3907, reward: 3.294, loss: -0.0155, took: 46.5524s
Batch 600/3907, reward: 3.292, loss: -0.0136, took: 47.5449s
Batch 700/3907, reward: 3.289, loss: -0.0157, took: 43.8314s
Batch 800/3907, reward: 3.288, loss: -0.0170, took: 46.9312s
Batch 900/3907, reward: 3.290, loss: -0.0165, took: 48.1293s
Batch 1000/3907, reward: 3.290, loss: -0.0178, took: 48.0907s
Batch 1100/3907, reward: 3.286, loss: -0.0140, took: 47.8629s
Batch 1200/3907, reward: 3.286, loss: -0.0170, took: 47.9607s
Batch 1300/3907, reward: 3.285, loss: -0.0179, took: 47.3967s
Batch 1400/3907, reward: 3.287, loss: -0.0154, took: 47.2910s
Batch 1500/3907, reward: 3.288, loss: -0.0171, took: 47.0811s
Batch 1600/3907, reward: 3.287, loss: -0.0147, took: 47.6049s
Batch 1700/3907, reward: 3.289, loss: -0.0169, took: 47.9270s
Batch 1800/3907, reward: 3.288, loss: -0.0150, took: 44.7968s
Batch 1900/3907, reward: 3.292, loss: -0.0145, took: 49.2440s
Batch 2000/3907, reward: 3.284, loss: -0.0166, took: 47.5083s
Batch 2100/3907, reward: 3.292, loss: -0.0158, took: 48.7930s
Batch 2200/3907, reward: 3.287, loss: -0.0174, took: 48.3867s
Batch 2300/3907, reward: 3.289, loss: -0.0160, took: 48.2402s
Batch 2400/3907, reward: 3.290, loss: -0.0140, took: 48.0044s
Batch 2500/3907, reward: 3.288, loss: -0.0165, took: 47.4525s
Batch 2600/3907, reward: 3.290, loss: -0.0168, took: 47.3772s
Batch 2700/3907, reward: 3.292, loss: -0.0144, took: 47.4765s
Batch 2800/3907, reward: 3.290, loss: -0.0157, took: 47.4862s
Batch 2900/3907, reward: 3.293, loss: -0.0156, took: 44.0470s
Batch 3000/3907, reward: 3.290, loss: -0.0161, took: 48.3878s
Batch 3100/3907, reward: 3.293, loss: -0.0134, took: 47.4892s
Batch 3200/3907, reward: 3.291, loss: -0.0140, took: 48.3403s
Batch 3300/3907, reward: 3.290, loss: -0.0192, took: 47.5520s
Batch 3400/3907, reward: 3.288, loss: -0.0189, took: 46.3827s
Batch 3500/3907, reward: 3.287, loss: -0.0134, took: 48.4996s
Batch 3600/3907, reward: 3.290, loss: -0.0152, took: 48.8487s
Batch 3700/3907, reward: 3.286, loss: -0.0160, took: 47.1915s
Batch 3800/3907, reward: 3.288, loss: -0.0146, took: 48.5809s
Batch 3900/3907, reward: 3.287, loss: -0.0154, took: 48.5739s
Mean epoch loss/reward: -0.0158, 3.2890, 3.2719, took: 1866.9046s (46.3163s / 100 batches)
Batch 0/3907, reward: 3.278, loss: 0.0283, took: 0.6035s
Batch 100/3907, reward: 3.290, loss: -0.0150, took: 48.1896s
Batch 200/3907, reward: 3.290, loss: -0.0143, took: 46.3626s
Batch 300/3907, reward: 3.289, loss: -0.0167, took: 47.5537s
Batch 400/3907, reward: 3.286, loss: -0.0135, took: 46.0237s
Batch 500/3907, reward: 3.290, loss: -0.0113, took: 38.7750s
Batch 600/3907, reward: 3.286, loss: -0.0159, took: 39.1767s
Batch 700/3907, reward: 3.284, loss: -0.0176, took: 34.8229s
Batch 800/3907, reward: 3.287, loss: -0.0160, took: 40.1594s
Batch 900/3907, reward: 3.288, loss: -0.0140, took: 47.8524s
Batch 1000/3907, reward: 3.288, loss: -0.0144, took: 46.8015s
Batch 1100/3907, reward: 3.288, loss: -0.0121, took: 47.4711s
Batch 1200/3907, reward: 3.294, loss: -0.0153, took: 47.8556s
Batch 1300/3907, reward: 3.288, loss: -0.0116, took: 46.8856s
Batch 1400/3907, reward: 3.288, loss: -0.0185, took: 46.8634s
Batch 1500/3907, reward: 3.290, loss: -0.0140, took: 47.6073s
Batch 1600/3907, reward: 3.287, loss: -0.0190, took: 46.9346s
Batch 1700/3907, reward: 3.290, loss: -0.0173, took: 46.8527s
Batch 1800/3907, reward: 3.290, loss: -0.0173, took: 44.1643s
Batch 1900/3907, reward: 3.289, loss: -0.0143, took: 46.5971s
Batch 2000/3907, reward: 3.287, loss: -0.0156, took: 47.9970s
Batch 2100/3907, reward: 3.290, loss: -0.0131, took: 42.3056s
Batch 2200/3907, reward: 3.290, loss: -0.0170, took: 37.6179s
Batch 2300/3907, reward: 3.290, loss: -0.0173, took: 38.5029s
Batch 2400/3907, reward: 3.288, loss: -0.0161, took: 37.7087s
Batch 2500/3907, reward: 3.285, loss: -0.0135, took: 38.7378s
Batch 2600/3907, reward: 3.286, loss: -0.0166, took: 38.8726s
Batch 2700/3907, reward: 3.291, loss: -0.0128, took: 38.5960s
Batch 2800/3907, reward: 3.286, loss: -0.0156, took: 39.0485s
Batch 2900/3907, reward: 3.284, loss: -0.0171, took: 32.5580s
Batch 3000/3907, reward: 3.285, loss: -0.0165, took: 30.0772s
Batch 3100/3907, reward: 3.291, loss: -0.0143, took: 28.2243s
Batch 3200/3907, reward: 3.288, loss: -0.0143, took: 28.4841s
Batch 3300/3907, reward: 3.287, loss: -0.0164, took: 29.0307s
Batch 3400/3907, reward: 3.296, loss: -0.0111, took: 35.8272s
Batch 3500/3907, reward: 3.290, loss: -0.0200, took: 37.4936s
Batch 3600/3907, reward: 3.291, loss: -0.0172, took: 37.5974s
Batch 3700/3907, reward: 3.286, loss: -0.0162, took: 38.0152s
Batch 3800/3907, reward: 3.291, loss: -0.0134, took: 37.8636s
Batch 3900/3907, reward: 3.288, loss: -0.0151, took: 39.1255s
Mean epoch loss/reward: -0.0153, 3.2886, 3.2750, took: 1604.4259s (39.7809s / 100 batches)
Batch 0/3907, reward: 3.269, loss: -0.0165, took: 0.6784s
Batch 100/3907, reward: 3.288, loss: -0.0139, took: 38.2160s
Batch 200/3907, reward: 3.289, loss: -0.0167, took: 37.6490s
Batch 300/3907, reward: 3.285, loss: -0.0116, took: 39.5723s
Batch 400/3907, reward: 3.292, loss: -0.0157, took: 37.7540s
Batch 500/3907, reward: 3.291, loss: -0.0141, took: 37.7514s
Batch 600/3907, reward: 3.288, loss: -0.0158, took: 39.1079s
Batch 700/3907, reward: 3.286, loss: -0.0124, took: 29.9520s
Batch 800/3907, reward: 3.285, loss: -0.0163, took: 28.8359s
Batch 900/3907, reward: 3.285, loss: -0.0135, took: 29.1472s
Batch 1000/3907, reward: 3.287, loss: -0.0139, took: 31.2443s
Batch 1100/3907, reward: 3.284, loss: -0.0154, took: 28.9637s
Batch 1200/3907, reward: 3.290, loss: -0.0132, took: 29.1151s
Batch 1300/3907, reward: 3.294, loss: -0.0145, took: 29.3670s
Batch 1400/3907, reward: 3.285, loss: -0.0138, took: 29.3707s
Batch 1500/3907, reward: 3.288, loss: -0.0145, took: 29.8934s
Batch 1600/3907, reward: 3.289, loss: -0.0149, took: 28.7892s
Batch 1700/3907, reward: 3.285, loss: -0.0142, took: 29.2848s
Batch 1800/3907, reward: 3.291, loss: -0.0152, took: 28.5062s
Batch 1900/3907, reward: 3.287, loss: -0.0140, took: 20.4293s
Batch 2000/3907, reward: 3.286, loss: -0.0150, took: 23.3478s
Batch 2100/3907, reward: 3.287, loss: -0.0148, took: 21.8717s
Batch 2200/3907, reward: 3.287, loss: -0.0163, took: 20.5925s
Batch 2300/3907, reward: 3.288, loss: -0.0140, took: 20.5345s
Batch 2400/3907, reward: 3.289, loss: -0.0141, took: 21.0373s
Batch 2500/3907, reward: 3.290, loss: -0.0130, took: 29.4278s
Batch 2600/3907, reward: 3.292, loss: -0.0155, took: 28.7085s
Batch 2700/3907, reward: 3.292, loss: -0.0134, took: 28.5490s
Batch 2800/3907, reward: 3.289, loss: -0.0132, took: 30.2738s
Batch 2900/3907, reward: 3.287, loss: -0.0175, took: 30.0949s
Batch 3000/3907, reward: 3.288, loss: -0.0157, took: 28.0741s
Batch 3100/3907, reward: 3.293, loss: -0.0152, took: 28.6258s
Batch 3200/3907, reward: 3.292, loss: -0.0153, took: 28.4017s
Batch 3300/3907, reward: 3.284, loss: -0.0148, took: 29.2293s
Batch 3400/3907, reward: 3.291, loss: -0.0154, took: 28.6392s
Batch 3500/3907, reward: 3.291, loss: -0.0162, took: 27.7983s
Batch 3600/3907, reward: 3.286, loss: -0.0122, took: 26.7117s
Batch 3700/3907, reward: 3.284, loss: -0.0135, took: 28.8429s
Batch 3800/3907, reward: 3.291, loss: -0.0118, took: 20.5868s
Batch 3900/3907, reward: 3.293, loss: -0.0139, took: 21.3591s
Mean epoch loss/reward: -0.0144, 3.2885, 3.2706, took: 1138.4064s (28.1584s / 100 batches)
Average tour length for uniform: 4.370219969645711
Average tour length for shifted: 3.030314348285266
Average tour length for adversary: 2.864820895733234
