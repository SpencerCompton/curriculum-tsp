Current device: cuda
Batch 0/3907, reward: 11.164, loss: -660.5330, took: 0.4659s
Batch 100/3907, reward: 9.431, loss: -11.5720, took: 23.3076s
Batch 200/3907, reward: 7.404, loss: -0.7774, took: 21.7352s
Batch 300/3907, reward: 7.216, loss: -0.4762, took: 22.0238s
Batch 400/3907, reward: 7.139, loss: 0.4434, took: 22.7913s
Batch 500/3907, reward: 7.092, loss: 0.4503, took: 23.7413s
Batch 600/3907, reward: 7.026, loss: -0.3244, took: 20.8689s
Batch 700/3907, reward: 6.990, loss: 0.3235, took: 22.1923s
Batch 800/3907, reward: 6.939, loss: -0.0222, took: 20.7657s
Batch 900/3907, reward: 6.691, loss: 0.0849, took: 19.4853s
Batch 1000/3907, reward: 5.928, loss: 0.1006, took: 19.7058s
Batch 1100/3907, reward: 5.522, loss: 0.4887, took: 20.3008s
Batch 1200/3907, reward: 5.208, loss: -0.5012, took: 23.7882s
Batch 1300/3907, reward: 5.069, loss: -0.0967, took: 23.2209s
Batch 1400/3907, reward: 4.977, loss: -0.3366, took: 18.9057s
Batch 1500/3907, reward: 4.910, loss: -0.3471, took: 22.3792s
Batch 1600/3907, reward: 4.844, loss: -0.4332, took: 21.4486s
Batch 1700/3907, reward: 4.788, loss: 0.0064, took: 20.3109s
Batch 1800/3907, reward: 4.715, loss: -0.6422, took: 21.4256s
Batch 1900/3907, reward: 4.682, loss: -0.5787, took: 20.8014s
Batch 2000/3907, reward: 4.633, loss: -0.1527, took: 24.3998s
Batch 2100/3907, reward: 4.591, loss: 0.2492, took: 19.3171s
Batch 2200/3907, reward: 4.553, loss: -0.0703, took: 19.7263s
Batch 2300/3907, reward: 4.513, loss: -0.3547, took: 21.4550s
Batch 2400/3907, reward: 4.483, loss: -0.3198, took: 22.2379s
Batch 2500/3907, reward: 4.452, loss: 0.0724, took: 20.1769s
Batch 2600/3907, reward: 4.422, loss: -0.4349, took: 21.3072s
Batch 2700/3907, reward: 4.404, loss: -0.1847, took: 20.2503s
Batch 2800/3907, reward: 4.396, loss: 0.1765, took: 25.2659s
Batch 2900/3907, reward: 4.373, loss: -0.0356, took: 18.6936s
Batch 3000/3907, reward: 4.354, loss: -0.2803, took: 26.1095s
Batch 3100/3907, reward: 4.344, loss: -0.4855, took: 31.3503s
Batch 3200/3907, reward: 4.328, loss: -0.2296, took: 29.9305s
Batch 3300/3907, reward: 4.318, loss: -0.1674, took: 31.4840s
Batch 3400/3907, reward: 4.314, loss: -0.0578, took: 30.8687s
Batch 3500/3907, reward: 4.301, loss: -0.2787, took: 29.8086s
Batch 3600/3907, reward: 4.296, loss: -0.3135, took: 28.9490s
Batch 3700/3907, reward: 4.290, loss: -0.0757, took: 28.6629s
Batch 3800/3907, reward: 4.287, loss: -0.0810, took: 29.4851s
Batch 3900/3907, reward: 4.280, loss: -0.2251, took: 30.0460s
Mean epoch loss/reward: -0.6164, 5.2437, 4.1935, took: 932.1092s (22.9797s / 100 batches)
Batch 0/3907, reward: 4.256, loss: 0.6472, took: 0.4378s
Batch 100/3907, reward: 4.274, loss: -0.1896, took: 30.8691s
Batch 200/3907, reward: 4.264, loss: -0.2284, took: 30.2224s
Batch 300/3907, reward: 4.258, loss: -0.2388, took: 30.8759s
Batch 400/3907, reward: 4.254, loss: -0.2036, took: 30.7054s
Batch 500/3907, reward: 4.244, loss: -0.0625, took: 30.3013s
Batch 600/3907, reward: 4.240, loss: -0.1922, took: 29.8480s
Batch 700/3907, reward: 4.247, loss: -0.1589, took: 30.0792s
Batch 800/3907, reward: 4.241, loss: -0.2381, took: 30.5864s
Batch 900/3907, reward: 4.237, loss: -0.1092, took: 29.7775s
Batch 1000/3907, reward: 4.233, loss: -0.0559, took: 29.7825s
Batch 1100/3907, reward: 4.229, loss: -0.2154, took: 30.5730s
Batch 1200/3907, reward: 4.227, loss: -0.2189, took: 36.3909s
Batch 1300/3907, reward: 4.223, loss: -0.0577, took: 38.9729s
Batch 1400/3907, reward: 4.214, loss: -0.2680, took: 39.3520s
Batch 1500/3907, reward: 4.211, loss: -0.0635, took: 36.5564s
Batch 1600/3907, reward: 4.218, loss: -0.0770, took: 39.5707s
Batch 1700/3907, reward: 4.212, loss: -0.1337, took: 39.6446s
Batch 1800/3907, reward: 4.210, loss: -0.2057, took: 41.9527s
Batch 1900/3907, reward: 4.204, loss: -0.1322, took: 49.4933s
Batch 2000/3907, reward: 4.201, loss: -0.1476, took: 48.8508s
Batch 2100/3907, reward: 4.205, loss: -0.1764, took: 49.2520s
Batch 2200/3907, reward: 4.194, loss: -0.0746, took: 48.6037s
Batch 2300/3907, reward: 4.192, loss: -0.1943, took: 48.5844s
Batch 2400/3907, reward: 4.191, loss: -0.1170, took: 47.9713s
Batch 2500/3907, reward: 4.201, loss: -0.1106, took: 49.5532s
Batch 2600/3907, reward: 4.196, loss: -0.1039, took: 48.5205s
Batch 2700/3907, reward: 4.187, loss: -0.1097, took: 49.3913s
Batch 2800/3907, reward: 4.196, loss: -0.1145, took: 49.3257s
Batch 2900/3907, reward: 4.187, loss: -0.1022, took: 45.9314s
Batch 3000/3907, reward: 4.180, loss: -0.0424, took: 47.8253s
Batch 3100/3907, reward: 4.187, loss: -0.0874, took: 47.7524s
Batch 3200/3907, reward: 4.185, loss: -0.1322, took: 48.2736s
Batch 3300/3907, reward: 4.176, loss: -0.0834, took: 49.6342s
Batch 3400/3907, reward: 4.181, loss: -0.1521, took: 48.5811s
Batch 3500/3907, reward: 4.179, loss: -0.0941, took: 48.1934s
Batch 3600/3907, reward: 4.178, loss: -0.0884, took: 50.1963s
Batch 3700/3907, reward: 4.177, loss: -0.0812, took: 49.3899s
Batch 3800/3907, reward: 4.170, loss: -0.1317, took: 48.3244s
Batch 3900/3907, reward: 4.174, loss: -0.0827, took: 48.9522s
Mean epoch loss/reward: -0.1351, 4.2097, 4.1271, took: 1642.9272s (40.7275s / 100 batches)
Batch 0/3907, reward: 4.111, loss: 0.2695, took: 0.5461s
Batch 100/3907, reward: 4.174, loss: -0.0904, took: 48.9586s
Batch 200/3907, reward: 4.170, loss: -0.0750, took: 48.5376s
Batch 300/3907, reward: 4.168, loss: -0.1413, took: 47.0917s
Batch 400/3907, reward: 4.169, loss: -0.1236, took: 49.1663s
Batch 500/3907, reward: 4.165, loss: -0.1368, took: 48.6865s
Batch 600/3907, reward: 4.164, loss: -0.0983, took: 48.9962s
Batch 700/3907, reward: 4.166, loss: -0.0694, took: 48.9980s
Batch 800/3907, reward: 4.163, loss: -0.0206, took: 48.5964s
Batch 900/3907, reward: 4.158, loss: -0.0656, took: 47.3200s
Batch 1000/3907, reward: 4.155, loss: -0.0878, took: 48.5559s
Batch 1100/3907, reward: 4.156, loss: -0.0857, took: 46.0999s
Batch 1200/3907, reward: 4.162, loss: -0.0856, took: 48.4528s
Batch 1300/3907, reward: 4.155, loss: -0.1013, took: 48.2473s
Batch 1400/3907, reward: 4.150, loss: -0.0867, took: 49.7156s
Batch 1500/3907, reward: 4.150, loss: -0.1146, took: 47.7621s
Batch 1600/3907, reward: 4.153, loss: -0.0893, took: 48.5878s
Batch 1700/3907, reward: 4.152, loss: -0.0652, took: 48.3927s
Batch 1800/3907, reward: 4.145, loss: -0.1174, took: 45.7883s
Batch 1900/3907, reward: 4.146, loss: -0.0335, took: 48.2766s
Batch 2000/3907, reward: 4.147, loss: -0.0540, took: 48.2672s
Batch 2100/3907, reward: 4.141, loss: -0.0866, took: 48.6243s
Batch 2200/3907, reward: 4.140, loss: -0.0364, took: 49.0259s
Batch 2300/3907, reward: 4.152, loss: -0.0705, took: 47.2639s
Batch 2400/3907, reward: 4.145, loss: -0.0471, took: 47.1453s
Batch 2500/3907, reward: 4.142, loss: -0.0885, took: 49.7722s
Batch 2600/3907, reward: 4.141, loss: -0.0245, took: 48.8256s
Batch 2700/3907, reward: 4.133, loss: -0.0463, took: 48.5757s
Batch 2800/3907, reward: 4.135, loss: -0.0639, took: 47.1222s
Batch 2900/3907, reward: 4.135, loss: -0.0664, took: 45.8226s
Batch 3000/3907, reward: 4.131, loss: -0.0940, took: 47.6247s
Batch 3100/3907, reward: 4.131, loss: -0.0092, took: 47.7760s
Batch 3200/3907, reward: 4.131, loss: -0.0506, took: 48.7324s
Batch 3300/3907, reward: 4.132, loss: -0.0617, took: 48.6125s
Batch 3400/3907, reward: 4.134, loss: -0.0487, took: 47.2305s
Batch 3500/3907, reward: 4.132, loss: -0.0613, took: 47.4225s
Batch 3600/3907, reward: 4.132, loss: -0.0499, took: 48.5990s
Batch 3700/3907, reward: 4.133, loss: -0.0310, took: 48.6842s
Batch 3800/3907, reward: 4.129, loss: -0.0817, took: 48.0897s
Batch 3900/3907, reward: 4.126, loss: -0.0763, took: 48.1125s
Mean epoch loss/reward: -0.0723, 4.1472, 4.1096, took: 1892.8809s (46.9526s / 100 batches)
Batch 0/3907, reward: 4.138, loss: -0.9353, took: 0.6614s
Batch 100/3907, reward: 4.129, loss: -0.0952, took: 48.3173s
Batch 200/3907, reward: 4.125, loss: -0.1200, took: 48.6053s
Batch 300/3907, reward: 4.125, loss: -0.0320, took: 48.0450s
Batch 400/3907, reward: 4.121, loss: -0.0793, took: 49.1644s
Batch 500/3907, reward: 4.130, loss: -0.0644, took: 47.7390s
Batch 600/3907, reward: 4.128, loss: -0.0599, took: 48.0940s
Batch 700/3907, reward: 4.124, loss: -0.0849, took: 47.6727s
Batch 800/3907, reward: 4.119, loss: -0.0893, took: 47.5218s
Batch 900/3907, reward: 4.120, loss: -0.0858, took: 50.2256s
Batch 1000/3907, reward: 4.122, loss: -0.0734, took: 49.1883s
Batch 1100/3907, reward: 4.128, loss: -0.0471, took: 45.9670s
Batch 1200/3907, reward: 4.119, loss: -0.0604, took: 48.7856s
Batch 1300/3907, reward: 4.118, loss: -0.0540, took: 47.9226s
Batch 1400/3907, reward: 4.124, loss: -0.0791, took: 48.0314s
Batch 1500/3907, reward: 4.116, loss: -0.0493, took: 45.7550s
Batch 1600/3907, reward: 4.115, loss: -0.0663, took: 49.1765s
Batch 1700/3907, reward: 4.112, loss: -0.0604, took: 47.6543s
Batch 1800/3907, reward: 4.115, loss: -0.0766, took: 46.2953s
Batch 1900/3907, reward: 4.114, loss: -0.0693, took: 49.4323s
Batch 2000/3907, reward: 4.109, loss: -0.0297, took: 49.0215s
Batch 2100/3907, reward: 4.115, loss: -0.0469, took: 48.0730s
Batch 2200/3907, reward: 4.118, loss: -0.0831, took: 48.1016s
Batch 2300/3907, reward: 4.121, loss: -0.0649, took: 47.7406s
Batch 2400/3907, reward: 4.120, loss: -0.0725, took: 48.4976s
Batch 2500/3907, reward: 4.113, loss: -0.0644, took: 48.1887s
Batch 2600/3907, reward: 4.120, loss: -0.0464, took: 49.5223s
Batch 2700/3907, reward: 4.122, loss: -0.0560, took: 48.8081s
Batch 2800/3907, reward: 4.114, loss: -0.0865, took: 47.7920s
Batch 2900/3907, reward: 4.107, loss: -0.0677, took: 46.3163s
Batch 3000/3907, reward: 4.109, loss: -0.0889, took: 48.1307s
Batch 3100/3907, reward: 4.115, loss: -0.0408, took: 48.9614s
Batch 3200/3907, reward: 4.117, loss: -0.0743, took: 49.5957s
Batch 3300/3907, reward: 4.114, loss: -0.0770, took: 48.2993s
Batch 3400/3907, reward: 4.106, loss: -0.0496, took: 48.8325s
Batch 3500/3907, reward: 4.113, loss: -0.0526, took: 49.1118s
Batch 3600/3907, reward: 4.112, loss: -0.0494, took: 48.1838s
Batch 3700/3907, reward: 4.109, loss: -0.0779, took: 48.7636s
Batch 3800/3907, reward: 4.107, loss: -0.0423, took: 49.0879s
Batch 3900/3907, reward: 4.115, loss: -0.0377, took: 48.8034s
Mean epoch loss/reward: -0.0658, 4.1175, 4.1084, took: 1898.8622s (47.1022s / 100 batches)
Batch 0/3907, reward: 4.126, loss: -0.0707, took: 0.6189s
Batch 100/3907, reward: 4.124, loss: -0.0651, took: 49.3099s
Batch 200/3907, reward: 4.114, loss: -0.0437, took: 48.3355s
Batch 300/3907, reward: 4.110, loss: -0.0507, took: 47.3091s
Batch 400/3907, reward: 4.114, loss: -0.0825, took: 47.3294s
Batch 500/3907, reward: 4.109, loss: -0.0613, took: 48.9320s
Batch 600/3907, reward: 4.114, loss: -0.0882, took: 48.6327s
Batch 700/3907, reward: 4.111, loss: -0.0593, took: 48.6577s
Batch 800/3907, reward: 4.106, loss: -0.0529, took: 47.0616s
Batch 900/3907, reward: 4.107, loss: -0.0657, took: 46.8402s
Batch 1000/3907, reward: 4.108, loss: -0.0514, took: 48.4222s
Batch 1100/3907, reward: 4.106, loss: -0.0509, took: 44.8039s
Batch 1200/3907, reward: 4.105, loss: -0.0897, took: 47.2903s
Batch 1300/3907, reward: 4.107, loss: -0.0564, took: 49.2535s
Batch 1400/3907, reward: 4.103, loss: -0.0680, took: 48.5702s
Batch 1500/3907, reward: 4.110, loss: -0.0636, took: 46.3837s
Batch 1600/3907, reward: 4.105, loss: -0.0480, took: 49.0172s
Batch 1700/3907, reward: 4.110, loss: -0.0609, took: 47.3194s
Batch 1800/3907, reward: 4.108, loss: -0.0550, took: 48.0678s
Batch 1900/3907, reward: 4.105, loss: -0.0496, took: 48.1425s
Batch 2000/3907, reward: 4.105, loss: -0.0740, took: 49.4106s
Batch 2100/3907, reward: 4.104, loss: -0.0546, took: 49.2619s
Batch 2200/3907, reward: 4.105, loss: -0.0478, took: 48.8657s
Batch 2300/3907, reward: 4.112, loss: -0.0785, took: 47.6343s
Batch 2400/3907, reward: 4.099, loss: -0.0427, took: 48.7556s
Batch 2500/3907, reward: 4.102, loss: -0.0440, took: 47.7569s
Batch 2600/3907, reward: 4.098, loss: -0.0461, took: 48.2960s
Batch 2700/3907, reward: 4.105, loss: -0.0457, took: 47.5039s
Batch 2800/3907, reward: 4.100, loss: -0.0552, took: 48.1329s
Batch 2900/3907, reward: 4.103, loss: -0.0465, took: 45.8246s
Batch 3000/3907, reward: 4.099, loss: -0.0585, took: 47.4849s
Batch 3100/3907, reward: 4.098, loss: -0.0586, took: 48.9352s
Batch 3200/3907, reward: 4.101, loss: -0.0483, took: 48.4362s
Batch 3300/3907, reward: 4.104, loss: -0.0270, took: 46.4179s
Batch 3400/3907, reward: 4.105, loss: -0.0464, took: 48.5239s
Batch 3500/3907, reward: 4.100, loss: -0.0464, took: 49.2835s
Batch 3600/3907, reward: 4.102, loss: -0.0537, took: 49.3558s
Batch 3700/3907, reward: 4.100, loss: -0.0502, took: 48.3308s
Batch 3800/3907, reward: 4.106, loss: -0.0382, took: 47.2513s
Batch 3900/3907, reward: 4.097, loss: -0.0644, took: 47.9989s
Mean epoch loss/reward: -0.0561, 4.1056, 4.0912, took: 1887.8912s (46.8440s / 100 batches)
Batch 0/3907, reward: 4.081, loss: -0.0827, took: 0.6375s
Batch 100/3907, reward: 4.101, loss: -0.0628, took: 47.7446s
Batch 200/3907, reward: 4.097, loss: -0.0515, took: 47.3317s
Batch 300/3907, reward: 4.099, loss: -0.0544, took: 49.4779s
Batch 400/3907, reward: 4.104, loss: -0.0632, took: 48.0876s
Batch 500/3907, reward: 4.103, loss: -0.0405, took: 48.8664s
Batch 600/3907, reward: 4.100, loss: -0.0391, took: 49.6797s
Batch 700/3907, reward: 4.104, loss: -0.0379, took: 49.1602s
Batch 800/3907, reward: 4.101, loss: -0.0772, took: 46.5583s
Batch 900/3907, reward: 4.095, loss: -0.0525, took: 48.7756s
Batch 1000/3907, reward: 4.103, loss: -0.0581, took: 47.7177s
Batch 1100/3907, reward: 4.106, loss: -0.0386, took: 47.9015s
Batch 1200/3907, reward: 4.098, loss: -0.0297, took: 47.3201s
Batch 1300/3907, reward: 4.096, loss: -0.0502, took: 47.2294s
Batch 1400/3907, reward: 4.100, loss: -0.0476, took: 47.9609s
Batch 1500/3907, reward: 4.096, loss: -0.0376, took: 45.8452s
Batch 1600/3907, reward: 4.093, loss: -0.0540, took: 49.2486s
Batch 1700/3907, reward: 4.097, loss: -0.0224, took: 46.0375s
Batch 1800/3907, reward: 4.099, loss: -0.0539, took: 46.9956s
Batch 1900/3907, reward: 4.095, loss: -0.0637, took: 48.5311s
Batch 2000/3907, reward: 4.093, loss: -0.0608, took: 46.7684s
Batch 2100/3907, reward: 4.093, loss: -0.0443, took: 48.0672s
Batch 2200/3907, reward: 4.102, loss: -0.0422, took: 48.2416s
Batch 2300/3907, reward: 4.102, loss: -0.0514, took: 47.7304s
Batch 2400/3907, reward: 4.098, loss: -0.0519, took: 47.8280s
Batch 2500/3907, reward: 4.100, loss: -0.0473, took: 47.9600s
Batch 2600/3907, reward: 4.095, loss: -0.0398, took: 47.9323s
Batch 2700/3907, reward: 4.103, loss: -0.0567, took: 49.2944s
Batch 2800/3907, reward: 4.098, loss: -0.0477, took: 48.0672s
Batch 2900/3907, reward: 4.098, loss: -0.0401, took: 46.5978s
Batch 3000/3907, reward: 4.099, loss: -0.0599, took: 48.0324s
Batch 3100/3907, reward: 4.096, loss: -0.0460, took: 48.8615s
Batch 3200/3907, reward: 4.091, loss: -0.0413, took: 48.1933s
Batch 3300/3907, reward: 4.095, loss: -0.0497, took: 47.5301s
Batch 3400/3907, reward: 4.099, loss: -0.0493, took: 48.7471s
Batch 3500/3907, reward: 4.100, loss: -0.0467, took: 47.8863s
Batch 3600/3907, reward: 4.102, loss: -0.0593, took: 48.3132s
Batch 3700/3907, reward: 4.098, loss: -0.0530, took: 47.9352s
Batch 3800/3907, reward: 4.095, loss: -0.0527, took: 48.5154s
Batch 3900/3907, reward: 4.104, loss: -0.0264, took: 48.7794s
Mean epoch loss/reward: -0.0487, 4.0986, 4.0933, took: 1887.6083s (46.8097s / 100 batches)
Batch 0/3907, reward: 4.079, loss: -0.0259, took: 0.8073s
Batch 100/3907, reward: 4.099, loss: -0.0489, took: 48.8584s
Batch 200/3907, reward: 4.099, loss: -0.0493, took: 47.9236s
Batch 300/3907, reward: 4.094, loss: -0.0351, took: 46.7195s
Batch 400/3907, reward: 4.099, loss: -0.0200, took: 48.3989s
Batch 500/3907, reward: 4.091, loss: -0.0360, took: 50.6105s
Batch 600/3907, reward: 4.093, loss: -0.0579, took: 47.9510s
Batch 700/3907, reward: 4.089, loss: -0.0439, took: 47.1636s
Batch 800/3907, reward: 4.094, loss: -0.0435, took: 47.9993s
Batch 900/3907, reward: 4.090, loss: -0.0310, took: 47.3918s
Batch 1000/3907, reward: 4.103, loss: -0.0415, took: 47.1909s
Batch 1100/3907, reward: 4.092, loss: -0.0370, took: 45.9196s
Batch 1200/3907, reward: 4.090, loss: -0.0293, took: 47.4701s
Batch 1300/3907, reward: 4.094, loss: -0.0508, took: 48.9556s
Batch 1400/3907, reward: 4.099, loss: -0.0371, took: 47.1806s
Batch 1500/3907, reward: 4.103, loss: -0.0279, took: 46.3661s
Batch 1600/3907, reward: 4.096, loss: -0.0489, took: 48.0374s
Batch 1700/3907, reward: 4.095, loss: -0.0672, took: 46.7760s
Batch 1800/3907, reward: 4.090, loss: -0.0373, took: 48.3545s
Batch 1900/3907, reward: 4.093, loss: -0.0420, took: 47.2657s
Batch 2000/3907, reward: 4.090, loss: -0.0434, took: 48.3560s
Batch 2100/3907, reward: 4.090, loss: -0.0466, took: 47.7772s
Batch 2200/3907, reward: 4.089, loss: -0.0474, took: 48.4289s
Batch 2300/3907, reward: 4.093, loss: -0.0436, took: 48.3545s
Batch 2400/3907, reward: 4.086, loss: -0.0521, took: 48.0608s
Batch 2500/3907, reward: 4.092, loss: -0.0526, took: 48.5583s
Batch 2600/3907, reward: 4.098, loss: -0.0430, took: 48.0990s
Batch 2700/3907, reward: 4.093, loss: -0.0356, took: 49.0538s
Batch 2800/3907, reward: 4.091, loss: -0.0573, took: 47.4196s
Batch 2900/3907, reward: 4.095, loss: -0.0390, took: 45.7643s
Batch 3000/3907, reward: 4.092, loss: -0.0290, took: 47.9909s
Batch 3100/3907, reward: 4.095, loss: -0.0627, took: 46.8675s
Batch 3200/3907, reward: 4.092, loss: -0.0428, took: 47.5725s
Batch 3300/3907, reward: 4.088, loss: -0.0421, took: 48.9323s
Batch 3400/3907, reward: 4.090, loss: -0.0234, took: 48.3116s
Batch 3500/3907, reward: 4.091, loss: -0.0569, took: 47.5453s
Batch 3600/3907, reward: 4.091, loss: -0.0275, took: 49.1431s
Batch 3700/3907, reward: 4.093, loss: -0.0358, took: 47.1683s
Batch 3800/3907, reward: 4.100, loss: -0.0387, took: 47.0001s
Batch 3900/3907, reward: 4.092, loss: -0.0349, took: 48.8276s
Mean epoch loss/reward: -0.0422, 4.0936, 4.0961, took: 1880.7170s (46.6643s / 100 batches)
Batch 0/3907, reward: 4.086, loss: 0.2224, took: 0.6318s
Batch 100/3907, reward: 4.096, loss: -0.0352, took: 48.3404s
Batch 200/3907, reward: 4.093, loss: -0.0362, took: 48.0673s
Batch 300/3907, reward: 4.095, loss: -0.0327, took: 47.4102s
Batch 400/3907, reward: 4.093, loss: -0.0440, took: 48.1604s
Batch 500/3907, reward: 4.094, loss: -0.0402, took: 47.5927s
Batch 600/3907, reward: 4.092, loss: -0.0566, took: 47.7470s
Batch 700/3907, reward: 4.092, loss: -0.0472, took: 46.8809s
Batch 800/3907, reward: 4.084, loss: -0.0450, took: 47.7327s
Batch 900/3907, reward: 4.089, loss: -0.0483, took: 47.9451s
Batch 1000/3907, reward: 4.090, loss: -0.0504, took: 47.4629s
Batch 1100/3907, reward: 4.090, loss: -0.0398, took: 44.2959s
Batch 1200/3907, reward: 4.091, loss: -0.0430, took: 47.0073s
Batch 1300/3907, reward: 4.089, loss: -0.0395, took: 48.3549s
Batch 1400/3907, reward: 4.085, loss: -0.0390, took: 48.0064s
Batch 1500/3907, reward: 4.085, loss: -0.0571, took: 45.8026s
Batch 1600/3907, reward: 4.093, loss: -0.0464, took: 47.2132s
Batch 1700/3907, reward: 4.094, loss: -0.0434, took: 45.5072s
Batch 1800/3907, reward: 4.087, loss: -0.0424, took: 46.0655s
Batch 1900/3907, reward: 4.091, loss: -0.0373, took: 47.3872s
Batch 2000/3907, reward: 4.094, loss: -0.0407, took: 48.1800s
Batch 2100/3907, reward: 4.091, loss: -0.0544, took: 48.1888s
Batch 2200/3907, reward: 4.086, loss: -0.0438, took: 49.2237s
Batch 2300/3907, reward: 4.091, loss: -0.0488, took: 47.8036s
Batch 2400/3907, reward: 4.087, loss: -0.0390, took: 47.7826s
Batch 2500/3907, reward: 4.084, loss: -0.0291, took: 46.9664s
Batch 2600/3907, reward: 4.089, loss: -0.0461, took: 47.9691s
Batch 2700/3907, reward: 4.086, loss: -0.0404, took: 48.2815s
Batch 2800/3907, reward: 4.090, loss: -0.0354, took: 47.6728s
Batch 2900/3907, reward: 4.088, loss: -0.0372, took: 44.4157s
Batch 3000/3907, reward: 4.093, loss: -0.0354, took: 48.6498s
Batch 3100/3907, reward: 4.081, loss: -0.0339, took: 47.4205s
Batch 3200/3907, reward: 4.100, loss: -0.0433, took: 48.3148s
Batch 3300/3907, reward: 4.087, loss: -0.0263, took: 47.5663s
Batch 3400/3907, reward: 4.094, loss: -0.0331, took: 47.1636s
Batch 3500/3907, reward: 4.098, loss: -0.0443, took: 48.1111s
Batch 3600/3907, reward: 4.091, loss: -0.0285, took: 48.6481s
Batch 3700/3907, reward: 4.090, loss: -0.0406, took: 48.7247s
Batch 3800/3907, reward: 4.085, loss: -0.0452, took: 46.9190s
Batch 3900/3907, reward: 4.093, loss: -0.0423, took: 48.3710s
Mean epoch loss/reward: -0.0412, 4.0903, 4.0846, took: 1868.6530s (46.3496s / 100 batches)
Batch 0/3907, reward: 4.083, loss: 0.0710, took: 0.8393s
Batch 100/3907, reward: 4.086, loss: -0.0340, took: 47.4923s
Batch 200/3907, reward: 4.089, loss: -0.0505, took: 46.9491s
Batch 300/3907, reward: 4.086, loss: -0.0377, took: 48.4805s
Batch 400/3907, reward: 4.096, loss: -0.0444, took: 47.6013s
Batch 500/3907, reward: 4.095, loss: -0.0354, took: 47.3355s
Batch 600/3907, reward: 4.088, loss: -0.0398, took: 48.7505s
Batch 700/3907, reward: 4.086, loss: -0.0352, took: 47.6938s
Batch 800/3907, reward: 4.088, loss: -0.0320, took: 50.0904s
Batch 900/3907, reward: 4.085, loss: -0.0401, took: 48.1991s
Batch 1000/3907, reward: 4.084, loss: -0.0517, took: 48.0036s
Batch 1100/3907, reward: 4.087, loss: -0.0446, took: 46.3055s
Batch 1200/3907, reward: 4.089, loss: -0.0389, took: 48.0054s
Batch 1300/3907, reward: 4.087, loss: -0.0463, took: 48.5294s
Batch 1400/3907, reward: 4.086, loss: -0.0505, took: 48.0082s
Batch 1500/3907, reward: 4.084, loss: -0.0375, took: 45.1098s
Batch 1600/3907, reward: 4.082, loss: -0.0330, took: 48.5015s
Batch 1700/3907, reward: 4.082, loss: -0.0424, took: 46.9907s
Batch 1800/3907, reward: 4.085, loss: -0.0430, took: 45.6110s
Batch 1900/3907, reward: 4.084, loss: -0.0542, took: 49.3930s
Batch 2000/3907, reward: 4.086, loss: -0.0496, took: 48.6291s
Batch 2100/3907, reward: 4.086, loss: -0.0492, took: 48.4740s
Batch 2200/3907, reward: 4.089, loss: -0.0402, took: 47.9680s
Batch 2300/3907, reward: 4.090, loss: -0.0361, took: 47.8523s
Batch 2400/3907, reward: 4.087, loss: -0.0321, took: 49.2154s
Batch 2500/3907, reward: 4.088, loss: -0.0488, took: 46.6623s
Batch 2600/3907, reward: 4.086, loss: -0.0435, took: 47.8409s
Batch 2700/3907, reward: 4.083, loss: -0.0339, took: 47.8427s
Batch 2800/3907, reward: 4.090, loss: -0.0375, took: 49.4468s
Batch 2900/3907, reward: 4.093, loss: -0.0458, took: 44.6524s
Batch 3000/3907, reward: 4.086, loss: -0.0409, took: 48.2231s
Batch 3100/3907, reward: 4.085, loss: -0.0355, took: 48.2124s
Batch 3200/3907, reward: 4.082, loss: -0.0447, took: 48.4791s
Batch 3300/3907, reward: 4.079, loss: -0.0403, took: 47.4918s
Batch 3400/3907, reward: 4.084, loss: -0.0404, took: 48.0592s
Batch 3500/3907, reward: 4.086, loss: -0.0324, took: 47.8777s
Batch 3600/3907, reward: 4.083, loss: -0.0408, took: 48.6464s
Batch 3700/3907, reward: 4.080, loss: -0.0319, took: 48.2047s
Batch 3800/3907, reward: 4.086, loss: -0.0540, took: 48.4872s
Batch 3900/3907, reward: 4.086, loss: -0.0440, took: 48.2569s
Mean epoch loss/reward: -0.0413, 4.0861, 4.0792, took: 1882.5754s (46.7103s / 100 batches)
Batch 0/3907, reward: 4.063, loss: -0.2215, took: 0.6562s
Batch 100/3907, reward: 4.082, loss: -0.0249, took: 47.9164s
Batch 200/3907, reward: 4.083, loss: -0.0535, took: 48.0106s
Batch 300/3907, reward: 4.091, loss: -0.0450, took: 47.4976s
Batch 400/3907, reward: 4.086, loss: -0.0512, took: 47.6335s
Batch 500/3907, reward: 4.089, loss: -0.0392, took: 47.7391s
Batch 600/3907, reward: 4.092, loss: -0.0301, took: 46.7204s
Batch 700/3907, reward: 4.088, loss: -0.0527, took: 48.3987s
Batch 800/3907, reward: 4.087, loss: -0.0369, took: 48.3602s
Batch 900/3907, reward: 4.085, loss: -0.0379, took: 48.3250s
Batch 1000/3907, reward: 4.089, loss: -0.0416, took: 48.8782s
Batch 1100/3907, reward: 4.086, loss: -0.0289, took: 44.8790s
Batch 1200/3907, reward: 4.088, loss: -0.0380, took: 47.8204s
Batch 1300/3907, reward: 4.084, loss: -0.0371, took: 48.7262s
Batch 1400/3907, reward: 4.087, loss: -0.0324, took: 49.0165s
Batch 1500/3907, reward: 4.080, loss: -0.0431, took: 44.9415s
Batch 1600/3907, reward: 4.078, loss: -0.0311, took: 49.4935s
Batch 1700/3907, reward: 4.084, loss: -0.0254, took: 47.9846s
Batch 1800/3907, reward: 4.084, loss: -0.0360, took: 46.6739s
Batch 1900/3907, reward: 4.084, loss: -0.0256, took: 47.5273s
Batch 2000/3907, reward: 4.085, loss: -0.0333, took: 47.2142s
Batch 2100/3907, reward: 4.086, loss: -0.0406, took: 48.6556s
Batch 2200/3907, reward: 4.079, loss: -0.0388, took: 47.8088s
Batch 2300/3907, reward: 4.084, loss: -0.0330, took: 49.1108s
Batch 2400/3907, reward: 4.082, loss: -0.0381, took: 48.4585s
Batch 2500/3907, reward: 4.079, loss: -0.0377, took: 48.0946s
Batch 2600/3907, reward: 4.084, loss: -0.0307, took: 48.4325s
Batch 2700/3907, reward: 4.081, loss: -0.0403, took: 48.8333s
Batch 2800/3907, reward: 4.075, loss: -0.0419, took: 47.0429s
Batch 2900/3907, reward: 4.087, loss: -0.0368, took: 46.6035s
Batch 3000/3907, reward: 4.091, loss: -0.0276, took: 46.8337s
Batch 3100/3907, reward: 4.089, loss: -0.0276, took: 47.8909s
Batch 3200/3907, reward: 4.081, loss: -0.0279, took: 48.0716s
Batch 3300/3907, reward: 4.102, loss: -0.0350, took: 47.8768s
Batch 3400/3907, reward: 4.086, loss: -0.0339, took: 48.6087s
Batch 3500/3907, reward: 4.084, loss: -0.0409, took: 49.1679s
Batch 3600/3907, reward: 4.086, loss: -0.0417, took: 47.6111s
Batch 3700/3907, reward: 4.086, loss: -0.0330, took: 47.3813s
Batch 3800/3907, reward: 4.085, loss: -0.0303, took: 46.6879s
Batch 3900/3907, reward: 4.084, loss: -0.0488, took: 47.7638s
Mean epoch loss/reward: -0.0366, 4.0852, 4.0797, took: 1879.1807s (46.6337s / 100 batches)
Batch 0/3907, reward: 4.104, loss: -0.1525, took: 0.6003s
Batch 100/3907, reward: 4.082, loss: -0.0271, took: 49.0207s
Batch 200/3907, reward: 4.080, loss: -0.0408, took: 47.6129s
Batch 300/3907, reward: 4.086, loss: -0.0295, took: 47.8781s
Batch 400/3907, reward: 4.079, loss: -0.0179, took: 48.7386s
Batch 500/3907, reward: 4.083, loss: -0.0418, took: 48.5707s
Batch 600/3907, reward: 4.078, loss: -0.0338, took: 48.7847s
Batch 700/3907, reward: 4.088, loss: -0.0286, took: 47.2786s
Batch 800/3907, reward: 4.084, loss: -0.0338, took: 47.6032s
Batch 900/3907, reward: 4.084, loss: -0.0251, took: 47.7240s
Batch 1000/3907, reward: 4.081, loss: -0.0332, took: 48.0651s
Batch 1100/3907, reward: 4.077, loss: -0.0292, took: 46.5312s
Batch 1200/3907, reward: 4.086, loss: -0.0353, took: 47.3812s
Batch 1300/3907, reward: 4.080, loss: -0.0360, took: 48.6795s
Batch 1400/3907, reward: 4.084, loss: -0.0432, took: 48.3637s
Batch 1500/3907, reward: 4.083, loss: -0.0210, took: 45.0545s
Batch 1600/3907, reward: 4.080, loss: -0.0394, took: 48.9801s
Batch 1700/3907, reward: 4.082, loss: -0.0389, took: 44.5714s
Batch 1800/3907, reward: 4.083, loss: -0.0373, took: 45.6962s
Batch 1900/3907, reward: 4.079, loss: -0.0349, took: 46.5044s
Batch 2000/3907, reward: 4.078, loss: -0.0349, took: 48.0155s
Batch 2100/3907, reward: 4.078, loss: -0.0350, took: 47.4121s
Batch 2200/3907, reward: 4.085, loss: -0.0448, took: 47.3445s
Batch 2300/3907, reward: 4.077, loss: -0.0357, took: 47.0437s
Batch 2400/3907, reward: 4.081, loss: -0.0390, took: 48.3145s
Batch 2500/3907, reward: 4.084, loss: -0.0416, took: 47.4398s
Batch 2600/3907, reward: 4.082, loss: -0.0312, took: 47.1718s
Batch 2700/3907, reward: 4.078, loss: -0.0302, took: 48.9928s
Batch 2800/3907, reward: 4.084, loss: -0.0345, took: 48.0068s
Batch 2900/3907, reward: 4.082, loss: -0.0421, took: 46.3009s
Batch 3000/3907, reward: 4.083, loss: -0.0276, took: 47.8590s
Batch 3100/3907, reward: 4.081, loss: -0.0372, took: 48.7596s
Batch 3200/3907, reward: 4.079, loss: -0.0262, took: 49.1461s
Batch 3300/3907, reward: 4.083, loss: -0.0348, took: 48.1910s
Batch 3400/3907, reward: 4.082, loss: -0.0384, took: 47.2923s
Batch 3500/3907, reward: 4.079, loss: -0.0191, took: 47.1069s
Batch 3600/3907, reward: 4.084, loss: -0.0260, took: 49.0875s
Batch 3700/3907, reward: 4.078, loss: -0.0350, took: 46.9731s
Batch 3800/3907, reward: 4.086, loss: -0.0311, took: 46.9013s
Batch 3900/3907, reward: 4.082, loss: -0.0476, took: 46.9693s
Mean epoch loss/reward: -0.0338, 4.0817, 4.0835, took: 1871.6589s (46.4492s / 100 batches)
Batch 0/3907, reward: 4.066, loss: -0.0015, took: 0.5891s
Batch 100/3907, reward: 4.079, loss: -0.0266, took: 48.0637s
Batch 200/3907, reward: 4.089, loss: -0.0336, took: 47.4296s
Batch 300/3907, reward: 4.079, loss: -0.0316, took: 47.5410s
Batch 400/3907, reward: 4.078, loss: -0.0311, took: 46.9777s
Batch 500/3907, reward: 4.080, loss: -0.0374, took: 47.7479s
Batch 600/3907, reward: 4.086, loss: -0.0392, took: 47.1863s
Batch 700/3907, reward: 4.081, loss: -0.0359, took: 49.1357s
Batch 800/3907, reward: 4.082, loss: -0.0358, took: 48.4368s
Batch 900/3907, reward: 4.093, loss: -0.0380, took: 48.2069s
Batch 1000/3907, reward: 4.083, loss: -0.0332, took: 48.5634s
Batch 1100/3907, reward: 4.084, loss: -0.0316, took: 46.6251s
Batch 1200/3907, reward: 4.081, loss: -0.0317, took: 48.2851s
Batch 1300/3907, reward: 4.080, loss: -0.0321, took: 47.4867s
Batch 1400/3907, reward: 4.091, loss: -0.0212, took: 48.3065s
Batch 1500/3907, reward: 4.080, loss: -0.0323, took: 43.8401s
Batch 1600/3907, reward: 4.083, loss: -0.0249, took: 48.9665s
Batch 1700/3907, reward: 4.083, loss: -0.0410, took: 47.8736s
Batch 1800/3907, reward: 4.084, loss: -0.0405, took: 45.0345s
Batch 1900/3907, reward: 4.083, loss: -0.0297, took: 48.2872s
Batch 2000/3907, reward: 4.085, loss: -0.0477, took: 47.3243s
Batch 2100/3907, reward: 4.081, loss: -0.0358, took: 47.7098s
Batch 2200/3907, reward: 4.081, loss: -0.0269, took: 48.3364s
Batch 2300/3907, reward: 4.081, loss: -0.0312, took: 47.2087s
Batch 2400/3907, reward: 4.078, loss: -0.0319, took: 47.3943s
Batch 2500/3907, reward: 4.085, loss: -0.0353, took: 48.5994s
Batch 2600/3907, reward: 4.080, loss: -0.0347, took: 48.8864s
Batch 2700/3907, reward: 4.084, loss: -0.0226, took: 47.4851s
Batch 2800/3907, reward: 4.082, loss: -0.0244, took: 48.6780s
Batch 2900/3907, reward: 4.085, loss: -0.0250, took: 44.5226s
Batch 3000/3907, reward: 4.089, loss: -0.0276, took: 48.4651s
Batch 3100/3907, reward: 4.079, loss: -0.0279, took: 48.3696s
Batch 3200/3907, reward: 4.078, loss: -0.0313, took: 48.5813s
Batch 3300/3907, reward: 4.082, loss: -0.0365, took: 47.8993s
Batch 3400/3907, reward: 4.080, loss: -0.0381, took: 47.4691s
Batch 3500/3907, reward: 4.082, loss: -0.0452, took: 48.6868s
Batch 3600/3907, reward: 4.080, loss: -0.0227, took: 47.0069s
Batch 3700/3907, reward: 4.080, loss: -0.0287, took: 47.9735s
Batch 3800/3907, reward: 4.084, loss: -0.0255, took: 46.6950s
Batch 3900/3907, reward: 4.082, loss: -0.0364, took: 46.7651s
Mean epoch loss/reward: -0.0324, 4.0825, 4.0795, took: 1873.1256s (46.4660s / 100 batches)
Batch 0/3907, reward: 4.098, loss: 0.0121, took: 0.5207s
Batch 100/3907, reward: 4.082, loss: -0.0261, took: 48.1042s
Batch 200/3907, reward: 4.088, loss: -0.0376, took: 47.6469s
Batch 300/3907, reward: 4.079, loss: -0.0291, took: 47.5571s
Batch 400/3907, reward: 4.078, loss: -0.0311, took: 47.9290s
Batch 500/3907, reward: 4.079, loss: -0.0274, took: 47.6385s
Batch 600/3907, reward: 4.079, loss: -0.0351, took: 47.9085s
Batch 700/3907, reward: 4.080, loss: -0.0259, took: 48.2467s
Batch 800/3907, reward: 4.081, loss: -0.0265, took: 47.7968s
Batch 900/3907, reward: 4.079, loss: -0.0361, took: 48.1485s
Batch 1000/3907, reward: 4.076, loss: -0.0342, took: 46.8666s
Batch 1100/3907, reward: 4.076, loss: -0.0362, took: 46.6655s
Batch 1200/3907, reward: 4.083, loss: -0.0453, took: 47.9687s
Batch 1300/3907, reward: 4.082, loss: -0.0290, took: 47.5649s
Batch 1400/3907, reward: 4.081, loss: -0.0244, took: 47.0442s
Batch 1500/3907, reward: 4.085, loss: -0.0334, took: 44.8180s
Batch 1600/3907, reward: 4.082, loss: -0.0332, took: 48.5440s
Batch 1700/3907, reward: 4.083, loss: -0.0284, took: 46.9524s
Batch 1800/3907, reward: 4.085, loss: -0.0260, took: 45.7962s
Batch 1900/3907, reward: 4.078, loss: -0.0267, took: 47.5284s
Batch 2000/3907, reward: 4.085, loss: -0.0418, took: 48.5823s
Batch 2100/3907, reward: 4.080, loss: -0.0195, took: 49.3443s
Batch 2200/3907, reward: 4.080, loss: -0.0255, took: 47.7952s
Batch 2300/3907, reward: 4.076, loss: -0.0314, took: 48.3597s
Batch 2400/3907, reward: 4.083, loss: -0.0338, took: 48.3058s
Batch 2500/3907, reward: 4.074, loss: -0.0389, took: 48.6419s
Batch 2600/3907, reward: 4.077, loss: -0.0379, took: 47.0082s
Batch 2700/3907, reward: 4.078, loss: -0.0370, took: 48.8939s
Batch 2800/3907, reward: 4.078, loss: -0.0178, took: 45.9504s
Batch 2900/3907, reward: 4.084, loss: -0.0295, took: 47.4721s
Batch 3000/3907, reward: 4.080, loss: -0.0272, took: 47.0349s
Batch 3100/3907, reward: 4.085, loss: -0.0347, took: 48.8637s
Batch 3200/3907, reward: 4.091, loss: -0.0358, took: 47.8794s
Batch 3300/3907, reward: 4.080, loss: -0.0245, took: 47.7064s
Batch 3400/3907, reward: 4.079, loss: -0.0333, took: 48.5867s
Batch 3500/3907, reward: 4.081, loss: -0.0292, took: 48.4399s
Batch 3600/3907, reward: 4.081, loss: -0.0307, took: 48.4903s
Batch 3700/3907, reward: 4.078, loss: -0.0287, took: 48.4266s
Batch 3800/3907, reward: 4.076, loss: -0.0222, took: 48.4364s
Batch 3900/3907, reward: 4.080, loss: -0.0272, took: 48.1391s
Mean epoch loss/reward: -0.0307, 4.0806, 4.0782, took: 1878.4034s (46.5901s / 100 batches)
Batch 0/3907, reward: 4.101, loss: -0.1890, took: 0.6261s
Batch 100/3907, reward: 4.080, loss: -0.0336, took: 48.6704s
Batch 200/3907, reward: 4.082, loss: -0.0423, took: 47.2375s
Batch 300/3907, reward: 4.080, loss: -0.0444, took: 47.3667s
Batch 400/3907, reward: 4.082, loss: -0.0318, took: 47.9189s
Batch 500/3907, reward: 4.078, loss: -0.0318, took: 47.3078s
Batch 600/3907, reward: 4.075, loss: -0.0296, took: 47.5202s
Batch 700/3907, reward: 4.077, loss: -0.0390, took: 48.1929s
Batch 800/3907, reward: 4.075, loss: -0.0270, took: 47.6053s
Batch 900/3907, reward: 4.079, loss: -0.0276, took: 47.2139s
Batch 1000/3907, reward: 4.077, loss: -0.0370, took: 46.6895s
Batch 1100/3907, reward: 4.077, loss: -0.0255, took: 45.7371s
Batch 1200/3907, reward: 4.079, loss: -0.0272, took: 47.8997s
Batch 1300/3907, reward: 4.081, loss: -0.0253, took: 48.0718s
Batch 1400/3907, reward: 4.078, loss: -0.0367, took: 47.7324s
Batch 1500/3907, reward: 4.077, loss: -0.0277, took: 45.7437s
Batch 1600/3907, reward: 4.078, loss: -0.0231, took: 48.5095s
Batch 1700/3907, reward: 4.083, loss: -0.0282, took: 46.5820s
Batch 1800/3907, reward: 4.079, loss: -0.0234, took: 47.7496s
Batch 1900/3907, reward: 4.075, loss: -0.0191, took: 47.5961s
Batch 2000/3907, reward: 4.077, loss: -0.0312, took: 48.1891s
Batch 2100/3907, reward: 4.074, loss: -0.0338, took: 49.1559s
Batch 2200/3907, reward: 4.080, loss: -0.0259, took: 46.3471s
Batch 2300/3907, reward: 4.078, loss: -0.0245, took: 47.4049s
Batch 2400/3907, reward: 4.076, loss: -0.0286, took: 48.0744s
Batch 2500/3907, reward: 4.079, loss: -0.0188, took: 48.3330s
Batch 2600/3907, reward: 4.070, loss: -0.0233, took: 48.5756s
Batch 2700/3907, reward: 4.072, loss: -0.0305, took: 47.9003s
Batch 2800/3907, reward: 4.079, loss: -0.0249, took: 44.5648s
Batch 2900/3907, reward: 4.077, loss: -0.0332, took: 48.4473s
Batch 3000/3907, reward: 4.072, loss: -0.0213, took: 47.1607s
Batch 3100/3907, reward: 4.076, loss: -0.0310, took: 47.6279s
Batch 3200/3907, reward: 4.076, loss: -0.0311, took: 47.6432s
Batch 3300/3907, reward: 4.074, loss: -0.0262, took: 48.5687s
Batch 3400/3907, reward: 4.077, loss: -0.0347, took: 48.9368s
Batch 3500/3907, reward: 4.074, loss: -0.0266, took: 47.5545s
Batch 3600/3907, reward: 4.078, loss: -0.0316, took: 47.1054s
Batch 3700/3907, reward: 4.076, loss: -0.0324, took: 47.8227s
Batch 3800/3907, reward: 4.080, loss: -0.0342, took: 48.0712s
Batch 3900/3907, reward: 4.080, loss: -0.0356, took: 48.2657s
Mean epoch loss/reward: -0.0297, 4.0773, 4.0723, took: 1871.7987s (46.4430s / 100 batches)
Batch 0/3907, reward: 4.052, loss: -0.0236, took: 0.6661s
Batch 100/3907, reward: 4.078, loss: -0.0226, took: 47.9792s
Batch 200/3907, reward: 4.076, loss: -0.0174, took: 47.7710s
Batch 300/3907, reward: 4.078, loss: -0.0363, took: 47.4866s
Batch 400/3907, reward: 4.078, loss: -0.0294, took: 48.0197s
Batch 500/3907, reward: 4.079, loss: -0.0342, took: 48.0816s
Batch 600/3907, reward: 4.075, loss: -0.0346, took: 47.4171s
Batch 700/3907, reward: 4.077, loss: -0.0305, took: 47.7846s
Batch 800/3907, reward: 4.077, loss: -0.0196, took: 48.0782s
Batch 900/3907, reward: 4.074, loss: -0.0222, took: 47.6075s
Batch 1000/3907, reward: 4.075, loss: -0.0284, took: 46.9706s
Batch 1100/3907, reward: 4.075, loss: -0.0221, took: 46.1484s
Batch 1200/3907, reward: 4.074, loss: -0.0238, took: 48.9619s
Batch 1300/3907, reward: 4.076, loss: -0.0284, took: 47.9951s
Batch 1400/3907, reward: 4.075, loss: -0.0289, took: 48.4679s
Batch 1500/3907, reward: 4.076, loss: -0.0265, took: 45.6801s
Batch 1600/3907, reward: 4.073, loss: -0.0278, took: 47.7179s
Batch 1700/3907, reward: 4.073, loss: -0.0183, took: 43.8237s
Batch 1800/3907, reward: 4.070, loss: -0.0247, took: 49.1187s
Batch 1900/3907, reward: 4.074, loss: -0.0346, took: 47.5448s
Batch 2000/3907, reward: 4.077, loss: -0.0356, took: 46.9786s
Batch 2100/3907, reward: 4.078, loss: -0.0240, took: 47.6853s
Batch 2200/3907, reward: 4.076, loss: -0.0320, took: 47.0571s
Batch 2300/3907, reward: 4.080, loss: -0.0193, took: 47.4335s
Batch 2400/3907, reward: 4.075, loss: -0.0167, took: 49.1880s
Batch 2500/3907, reward: 4.074, loss: -0.0192, took: 47.7167s
Batch 2600/3907, reward: 4.074, loss: -0.0223, took: 47.2255s
Batch 2700/3907, reward: 4.075, loss: -0.0316, took: 46.3908s
Batch 2800/3907, reward: 4.077, loss: -0.0250, took: 45.3268s
Batch 2900/3907, reward: 4.071, loss: -0.0247, took: 47.1030s
Batch 3000/3907, reward: 4.073, loss: -0.0239, took: 46.9839s
Batch 3100/3907, reward: 4.073, loss: -0.0262, took: 48.8473s
Batch 3200/3907, reward: 4.073, loss: -0.0241, took: 48.3966s
Batch 3300/3907, reward: 4.083, loss: -0.0304, took: 47.2881s
Batch 3400/3907, reward: 4.078, loss: -0.0287, took: 48.4778s
Batch 3500/3907, reward: 4.074, loss: -0.0339, took: 47.4433s
Batch 3600/3907, reward: 4.078, loss: -0.0321, took: 48.3980s
Batch 3700/3907, reward: 4.073, loss: -0.0177, took: 47.8883s
Batch 3800/3907, reward: 4.078, loss: -0.0357, took: 48.6585s
Batch 3900/3907, reward: 4.074, loss: -0.0262, took: 47.0396s
Mean epoch loss/reward: -0.0267, 4.0756, 4.0651, took: 1869.4467s (46.3712s / 100 batches)
Batch 0/3907, reward: 4.095, loss: 0.0105, took: 0.6620s
Batch 100/3907, reward: 4.070, loss: -0.0357, took: 47.6935s
Batch 200/3907, reward: 4.069, loss: -0.0280, took: 48.2910s
Batch 300/3907, reward: 4.078, loss: -0.0307, took: 46.5940s
Batch 400/3907, reward: 4.074, loss: -0.0296, took: 48.0294s
Batch 500/3907, reward: 4.070, loss: -0.0227, took: 47.5164s
Batch 600/3907, reward: 4.077, loss: -0.0219, took: 47.3482s
Batch 700/3907, reward: 4.078, loss: -0.0267, took: 47.5032s
Batch 800/3907, reward: 4.077, loss: -0.0280, took: 48.5411s
Batch 900/3907, reward: 4.071, loss: -0.0238, took: 47.4180s
Batch 1000/3907, reward: 4.069, loss: -0.0348, took: 48.5090s
Batch 1100/3907, reward: 4.073, loss: -0.0393, took: 45.1434s
Batch 1200/3907, reward: 4.076, loss: -0.0298, took: 47.6189s
Batch 1300/3907, reward: 4.080, loss: -0.0203, took: 48.3033s
Batch 1400/3907, reward: 4.069, loss: -0.0208, took: 47.1066s
Batch 1500/3907, reward: 4.072, loss: -0.0204, took: 43.5638s
Batch 1600/3907, reward: 4.076, loss: -0.0370, took: 48.0415s
Batch 1700/3907, reward: 4.077, loss: -0.0402, took: 44.8647s
Batch 1800/3907, reward: 4.075, loss: -0.0235, took: 49.4184s
Batch 1900/3907, reward: 4.075, loss: -0.0159, took: 48.1631s
Batch 2000/3907, reward: 4.078, loss: -0.0345, took: 46.3932s
Batch 2100/3907, reward: 4.076, loss: -0.0210, took: 47.6700s
Batch 2200/3907, reward: 4.078, loss: -0.0258, took: 47.9239s
Batch 2300/3907, reward: 4.078, loss: -0.0315, took: 48.1106s
Batch 2400/3907, reward: 4.076, loss: -0.0246, took: 46.8680s
Batch 2500/3907, reward: 4.073, loss: -0.0253, took: 46.9743s
Batch 2600/3907, reward: 4.082, loss: -0.0285, took: 49.1229s
Batch 2700/3907, reward: 4.076, loss: -0.0376, took: 47.9914s
Batch 2800/3907, reward: 4.073, loss: -0.0328, took: 46.8265s
Batch 2900/3907, reward: 4.078, loss: -0.0371, took: 46.0172s
Batch 3000/3907, reward: 4.082, loss: -0.0388, took: 47.1461s
Batch 3100/3907, reward: 4.072, loss: -0.0260, took: 46.8125s
Batch 3200/3907, reward: 4.079, loss: -0.0290, took: 48.6532s
Batch 3300/3907, reward: 4.074, loss: -0.0260, took: 47.7028s
Batch 3400/3907, reward: 4.076, loss: -0.0325, took: 47.1931s
Batch 3500/3907, reward: 4.077, loss: -0.0272, took: 49.0098s
Batch 3600/3907, reward: 4.076, loss: -0.0325, took: 47.7966s
Batch 3700/3907, reward: 4.074, loss: -0.0326, took: 48.7481s
Batch 3800/3907, reward: 4.069, loss: -0.0245, took: 47.0505s
Batch 3900/3907, reward: 4.078, loss: -0.0241, took: 48.1374s
Mean epoch loss/reward: -0.0288, 4.0752, 4.0728, took: 1866.6924s (46.3119s / 100 batches)
Batch 0/3907, reward: 4.075, loss: -0.0000, took: 0.8108s
Batch 100/3907, reward: 4.083, loss: -0.0356, took: 48.7055s
Batch 200/3907, reward: 4.073, loss: -0.0417, took: 48.5863s
Batch 300/3907, reward: 4.079, loss: -0.0392, took: 47.1498s
Batch 400/3907, reward: 4.072, loss: -0.0247, took: 47.5190s
Batch 500/3907, reward: 4.075, loss: -0.0269, took: 47.3007s
Batch 600/3907, reward: 4.076, loss: -0.0224, took: 47.6322s
Batch 700/3907, reward: 4.068, loss: -0.0263, took: 47.0904s
Batch 800/3907, reward: 4.071, loss: -0.0407, took: 47.0814s
Batch 900/3907, reward: 4.071, loss: -0.0277, took: 47.7817s
Batch 1000/3907, reward: 4.074, loss: -0.0261, took: 46.1888s
Batch 1100/3907, reward: 4.073, loss: -0.0288, took: 44.5851s
Batch 1200/3907, reward: 4.074, loss: -0.0270, took: 48.2418s
Batch 1300/3907, reward: 4.077, loss: -0.0353, took: 48.1765s
Batch 1400/3907, reward: 4.074, loss: -0.0496, took: 48.0998s
Batch 1500/3907, reward: 4.068, loss: -0.0210, took: 44.3729s
Batch 1600/3907, reward: 4.070, loss: -0.0331, took: 48.3227s
Batch 1700/3907, reward: 4.076, loss: -0.0308, took: 45.3214s
Batch 1800/3907, reward: 4.081, loss: -0.0202, took: 46.4986s
Batch 1900/3907, reward: 4.074, loss: -0.0304, took: 47.6429s
Batch 2000/3907, reward: 4.070, loss: -0.0233, took: 48.1635s
Batch 2100/3907, reward: 4.073, loss: -0.0253, took: 47.9535s
Batch 2200/3907, reward: 4.076, loss: -0.0165, took: 47.7412s
Batch 2300/3907, reward: 4.072, loss: -0.0313, took: 48.8091s
Batch 2400/3907, reward: 4.084, loss: -0.0282, took: 48.9541s
Batch 2500/3907, reward: 4.075, loss: -0.0298, took: 47.4368s
Batch 2600/3907, reward: 4.073, loss: -0.0239, took: 47.4382s
Batch 2700/3907, reward: 4.077, loss: -0.0331, took: 48.0270s
Batch 2800/3907, reward: 4.074, loss: -0.0258, took: 45.1548s
Batch 2900/3907, reward: 4.072, loss: -0.0264, took: 48.5023s
Batch 3000/3907, reward: 4.071, loss: -0.0342, took: 46.7538s
Batch 3100/3907, reward: 4.067, loss: -0.0108, took: 47.1044s
Batch 3200/3907, reward: 4.071, loss: -0.0236, took: 48.0928s
Batch 3300/3907, reward: 4.076, loss: -0.0277, took: 47.4216s
Batch 3400/3907, reward: 4.077, loss: -0.0258, took: 47.4859s
Batch 3500/3907, reward: 4.074, loss: -0.0236, took: 48.9516s
Batch 3600/3907, reward: 4.077, loss: -0.0299, took: 47.0775s
Batch 3700/3907, reward: 4.072, loss: -0.0219, took: 48.1703s
Batch 3800/3907, reward: 4.069, loss: -0.0245, took: 48.0483s
Batch 3900/3907, reward: 4.075, loss: -0.0316, took: 48.4948s
Mean epoch loss/reward: -0.0283, 4.0740, 4.0710, took: 1867.1229s (46.3222s / 100 batches)
Batch 0/3907, reward: 4.048, loss: -0.1485, took: 0.6076s
Batch 100/3907, reward: 4.072, loss: -0.0310, took: 48.3802s
Batch 200/3907, reward: 4.072, loss: -0.0266, took: 48.6436s
Batch 300/3907, reward: 4.075, loss: -0.0343, took: 46.8879s
Batch 400/3907, reward: 4.074, loss: -0.0286, took: 47.2776s
Batch 500/3907, reward: 4.069, loss: -0.0299, took: 49.1471s
Batch 600/3907, reward: 4.075, loss: -0.0319, took: 47.6864s
Batch 700/3907, reward: 4.075, loss: -0.0206, took: 46.9720s
Batch 800/3907, reward: 4.076, loss: -0.0289, took: 48.7637s
Batch 900/3907, reward: 4.071, loss: -0.0317, took: 48.5138s
Batch 1000/3907, reward: 4.080, loss: -0.0212, took: 47.9054s
Batch 1100/3907, reward: 4.083, loss: -0.0239, took: 44.8855s
Batch 1200/3907, reward: 4.074, loss: -0.0288, took: 48.0881s
Batch 1300/3907, reward: 4.072, loss: -0.0222, took: 47.3511s
Batch 1400/3907, reward: 4.072, loss: -0.0345, took: 48.3300s
Batch 1500/3907, reward: 4.081, loss: -0.0300, took: 47.0301s
Batch 1600/3907, reward: 4.077, loss: -0.0287, took: 46.2630s
Batch 1700/3907, reward: 4.070, loss: -0.0229, took: 44.8881s
Batch 1800/3907, reward: 4.078, loss: -0.0294, took: 47.3384s
Batch 1900/3907, reward: 4.069, loss: -0.0279, took: 47.8653s
Batch 2000/3907, reward: 4.075, loss: -0.0261, took: 48.3621s
Batch 2100/3907, reward: 4.072, loss: -0.0264, took: 48.7899s
Batch 2200/3907, reward: 4.070, loss: -0.0294, took: 47.7094s
Batch 2300/3907, reward: 4.067, loss: -0.0176, took: 47.3486s
Batch 2400/3907, reward: 4.070, loss: -0.0318, took: 47.6872s
Batch 2500/3907, reward: 4.074, loss: -0.0286, took: 47.8140s
Batch 2600/3907, reward: 4.071, loss: -0.0339, took: 47.3483s
Batch 2700/3907, reward: 4.072, loss: -0.0260, took: 46.9126s
Batch 2800/3907, reward: 4.073, loss: -0.0297, took: 44.4226s
Batch 2900/3907, reward: 4.077, loss: -0.0270, took: 46.6013s
Batch 3000/3907, reward: 4.071, loss: -0.0301, took: 47.7582s
Batch 3100/3907, reward: 4.076, loss: -0.0259, took: 47.3750s
Batch 3200/3907, reward: 4.073, loss: -0.0353, took: 47.1545s
Batch 3300/3907, reward: 4.071, loss: -0.0307, took: 47.4067s
Batch 3400/3907, reward: 4.069, loss: -0.0225, took: 47.5802s
Batch 3500/3907, reward: 4.074, loss: -0.0267, took: 46.9912s
Batch 3600/3907, reward: 4.068, loss: -0.0292, took: 48.1136s
Batch 3700/3907, reward: 4.067, loss: -0.0250, took: 47.9844s
Batch 3800/3907, reward: 4.069, loss: -0.0272, took: 48.5103s
Batch 3900/3907, reward: 4.069, loss: -0.0277, took: 48.2503s
Mean epoch loss/reward: -0.0280, 4.0729, 4.0736, took: 1866.7581s (46.3236s / 100 batches)
Batch 0/3907, reward: 4.080, loss: -0.0767, took: 0.6207s
Batch 100/3907, reward: 4.073, loss: -0.0150, took: 47.1617s
Batch 200/3907, reward: 4.072, loss: -0.0230, took: 48.1640s
Batch 300/3907, reward: 4.072, loss: -0.0275, took: 48.0980s
Batch 400/3907, reward: 4.071, loss: -0.0487, took: 47.6538s
Batch 500/3907, reward: 4.075, loss: -0.0156, took: 47.2201s
Batch 600/3907, reward: 4.073, loss: -0.0287, took: 47.3173s
Batch 700/3907, reward: 4.068, loss: -0.0330, took: 47.2202s
Batch 800/3907, reward: 4.073, loss: -0.0242, took: 48.1829s
Batch 900/3907, reward: 4.081, loss: -0.0378, took: 47.9589s
Batch 1000/3907, reward: 4.075, loss: -0.0278, took: 47.5900s
Batch 1100/3907, reward: 4.074, loss: -0.0250, took: 45.0240s
Batch 1200/3907, reward: 4.072, loss: -0.0348, took: 47.5403s
Batch 1300/3907, reward: 4.071, loss: -0.0205, took: 49.1083s
Batch 1400/3907, reward: 4.066, loss: -0.0284, took: 48.8604s
Batch 1500/3907, reward: 4.077, loss: -0.0182, took: 45.7833s
Batch 1600/3907, reward: 4.073, loss: -0.0273, took: 48.5917s
Batch 1700/3907, reward: 4.069, loss: -0.0322, took: 44.3021s
Batch 1800/3907, reward: 4.071, loss: -0.0315, took: 48.1682s
Batch 1900/3907, reward: 4.077, loss: -0.0373, took: 48.5114s
Batch 2000/3907, reward: 4.067, loss: -0.0205, took: 47.3080s
Batch 2100/3907, reward: 4.073, loss: -0.0324, took: 47.8767s
Batch 2200/3907, reward: 4.075, loss: -0.0266, took: 48.0243s
Batch 2300/3907, reward: 4.074, loss: -0.0240, took: 47.4111s
Batch 2400/3907, reward: 4.075, loss: -0.0339, took: 46.7997s
Batch 2500/3907, reward: 4.079, loss: -0.0234, took: 47.7562s
Batch 2600/3907, reward: 4.068, loss: -0.0287, took: 47.2242s
Batch 2700/3907, reward: 4.068, loss: -0.0329, took: 46.5537s
Batch 2800/3907, reward: 4.066, loss: -0.0349, took: 47.3953s
Batch 2900/3907, reward: 4.073, loss: -0.0238, took: 44.9943s
Batch 3000/3907, reward: 4.070, loss: -0.0290, took: 47.1340s
Batch 3100/3907, reward: 4.069, loss: -0.0263, took: 47.5468s
Batch 3200/3907, reward: 4.074, loss: -0.0266, took: 47.5319s
Batch 3300/3907, reward: 4.072, loss: -0.0177, took: 47.1035s
Batch 3400/3907, reward: 4.078, loss: -0.0366, took: 47.3147s
Batch 3500/3907, reward: 4.071, loss: -0.0257, took: 47.4030s
Batch 3600/3907, reward: 4.074, loss: -0.0327, took: 47.4836s
Batch 3700/3907, reward: 4.073, loss: -0.0312, took: 47.8949s
Batch 3800/3907, reward: 4.075, loss: -0.0307, took: 47.0345s
Batch 3900/3907, reward: 4.069, loss: -0.0317, took: 48.0480s
Mean epoch loss/reward: -0.0283, 4.0724, 4.0656, took: 1862.8859s (46.2229s / 100 batches)
Batch 0/3907, reward: 4.025, loss: -0.0320, took: 0.9256s
Batch 100/3907, reward: 4.074, loss: -0.0305, took: 47.9584s
Batch 200/3907, reward: 4.072, loss: -0.0323, took: 47.4057s
Batch 300/3907, reward: 4.070, loss: -0.0293, took: 47.5627s
Batch 400/3907, reward: 4.069, loss: -0.0172, took: 47.4859s
Batch 500/3907, reward: 4.073, loss: -0.0349, took: 47.4160s
Batch 600/3907, reward: 4.071, loss: -0.0308, took: 48.0043s
Batch 700/3907, reward: 4.079, loss: -0.0344, took: 46.8154s
Batch 800/3907, reward: 4.071, loss: -0.0198, took: 47.2141s
Batch 900/3907, reward: 4.071, loss: -0.0266, took: 47.3547s
Batch 1000/3907, reward: 4.073, loss: -0.0296, took: 46.7534s
Batch 1100/3907, reward: 4.067, loss: -0.0296, took: 44.6647s
Batch 1200/3907, reward: 4.070, loss: -0.0234, took: 48.9159s
Batch 1300/3907, reward: 4.071, loss: -0.0127, took: 47.5459s
Batch 1400/3907, reward: 4.065, loss: -0.0209, took: 47.7341s
Batch 1500/3907, reward: 4.074, loss: -0.0129, took: 43.7046s
Batch 1600/3907, reward: 4.071, loss: -0.0219, took: 37.4031s
Batch 1700/3907, reward: 4.072, loss: -0.0181, took: 37.1099s
Batch 1800/3907, reward: 4.069, loss: -0.0298, took: 34.3925s
Batch 1900/3907, reward: 4.068, loss: -0.0223, took: 42.1898s
Batch 2000/3907, reward: 4.066, loss: -0.0320, took: 46.9100s
Batch 2100/3907, reward: 4.071, loss: -0.0301, took: 47.1952s
Batch 2200/3907, reward: 4.073, loss: -0.0291, took: 46.9476s
Batch 2300/3907, reward: 4.071, loss: -0.0237, took: 47.2473s
Batch 2400/3907, reward: 4.072, loss: -0.0180, took: 47.1252s
Batch 2500/3907, reward: 4.076, loss: -0.0352, took: 46.8228s
Batch 2600/3907, reward: 4.073, loss: -0.0319, took: 47.1617s
Batch 2700/3907, reward: 4.073, loss: -0.0257, took: 47.5658s
Batch 2800/3907, reward: 4.068, loss: -0.0205, took: 47.3375s
Batch 2900/3907, reward: 4.070, loss: -0.0320, took: 44.9492s
Batch 3000/3907, reward: 4.069, loss: -0.0242, took: 46.5746s
Batch 3100/3907, reward: 4.071, loss: -0.0213, took: 47.0236s
Batch 3200/3907, reward: 4.065, loss: -0.0265, took: 42.8027s
Batch 3300/3907, reward: 4.073, loss: -0.0298, took: 39.5653s
Batch 3400/3907, reward: 4.067, loss: -0.0256, took: 39.6945s
Batch 3500/3907, reward: 4.072, loss: -0.0259, took: 39.0368s
Batch 3600/3907, reward: 4.071, loss: -0.0255, took: 38.5149s
Batch 3700/3907, reward: 4.073, loss: -0.0342, took: 38.7954s
Batch 3800/3907, reward: 4.075, loss: -0.0257, took: 38.7371s
Batch 3900/3907, reward: 4.069, loss: -0.0184, took: 38.6828s
Mean epoch loss/reward: -0.0260, 4.0709, 4.0723, took: 1751.1971s (43.4312s / 100 batches)
Average tour length for uniform: 4.027769453665287
Average tour length for shifted: 3.327155515120589
Average tour length for adversary: 2.870332129507502
