Current device: cuda
Batch 0/3907, reward: 14.055, loss: -780.7357, took: 0.5401s
Batch 100/3907, reward: 5.879, loss: -16.1920, took: 38.9792s
Batch 200/3907, reward: 2.898, loss: -0.6252, took: 38.7774s
Batch 300/3907, reward: 2.896, loss: 1.0032, took: 36.7245s
Batch 400/3907, reward: 2.895, loss: 3.6617, took: 37.9446s
Batch 500/3907, reward: 2.895, loss: 0.7332, took: 38.7564s
Batch 600/3907, reward: 2.894, loss: 0.0886, took: 38.6646s
Batch 700/3907, reward: 2.894, loss: 0.9695, took: 38.7951s
Batch 800/3907, reward: 2.895, loss: 0.4468, took: 38.6929s
Batch 900/3907, reward: 2.894, loss: 0.5217, took: 35.7886s
Batch 1000/3907, reward: 2.894, loss: 1.7124, took: 38.6490s
Batch 1100/3907, reward: 2.894, loss: 1.8138, took: 38.6563s
Batch 1200/3907, reward: 2.894, loss: -0.6094, took: 38.7767s
Batch 1300/3907, reward: 2.894, loss: 0.0925, took: 38.5386s
Batch 1400/3907, reward: 2.894, loss: 0.7186, took: 38.8658s
Batch 1500/3907, reward: 2.894, loss: 1.2391, took: 38.7048s
Batch 1600/3907, reward: 2.894, loss: 1.5345, took: 38.5885s
Batch 1700/3907, reward: 2.894, loss: 1.8727, took: 38.7584s
Batch 1800/3907, reward: 2.894, loss: 1.3353, took: 38.9367s
Batch 1900/3907, reward: 2.894, loss: -0.8999, took: 38.8031s
Batch 2000/3907, reward: 2.894, loss: -0.3787, took: 38.8458s
Batch 2100/3907, reward: 2.894, loss: 0.1200, took: 38.6963s
Batch 2200/3907, reward: 2.894, loss: 0.3174, took: 38.7591s
Batch 2300/3907, reward: 2.894, loss: -0.4920, took: 38.6742s
Batch 2400/3907, reward: 2.894, loss: 0.2170, took: 38.5992s
Batch 2500/3907, reward: 2.894, loss: 0.9095, took: 38.8618s
Batch 2600/3907, reward: 2.894, loss: 1.5740, took: 38.5655s
Batch 2700/3907, reward: 2.894, loss: 1.2017, took: 38.5190s
Batch 2800/3907, reward: 2.894, loss: 0.1959, took: 38.5051s
Batch 2900/3907, reward: 2.894, loss: 0.8775, took: 38.5293s
Batch 3000/3907, reward: 2.894, loss: 1.6437, took: 38.6106s
Batch 3100/3907, reward: 2.894, loss: 0.6769, took: 38.6505s
Batch 3200/3907, reward: 2.894, loss: -0.5156, took: 38.7162s
Batch 3300/3907, reward: 2.894, loss: 0.3489, took: 35.7054s
Batch 3400/3907, reward: 2.894, loss: 1.0328, took: 38.4457s
Batch 3500/3907, reward: 2.894, loss: 1.6661, took: 38.5982s
Batch 3600/3907, reward: 2.894, loss: 0.3704, took: 38.7261s
Batch 3700/3907, reward: 2.894, loss: -1.0200, took: 38.6789s
Batch 3800/3907, reward: 2.894, loss: -0.3658, took: 38.7419s
Batch 3900/3907, reward: 2.894, loss: 0.2464, took: 38.7377s
Mean epoch loss/reward: 0.0069, 2.9735, 2.8671, took: 1512.5736s (37.5277s / 100 batches)
Batch 0/3907, reward: 2.895, loss: -1.7742, took: 0.5248s
Batch 100/3907, reward: 2.894, loss: 0.8669, took: 38.8496s
Batch 200/3907, reward: 2.894, loss: 1.2383, took: 38.7576s
Batch 300/3907, reward: 2.894, loss: 1.2986, took: 37.2093s
Batch 400/3907, reward: 2.894, loss: 0.7952, took: 37.8125s
Batch 500/3907, reward: 2.894, loss: -0.8192, took: 38.6479s
Batch 600/3907, reward: 2.894, loss: -0.1036, took: 38.7098s
Batch 700/3907, reward: 2.894, loss: -0.7839, took: 38.6470s
Batch 800/3907, reward: 2.894, loss: 1.2208, took: 38.7736s
Batch 900/3907, reward: 2.894, loss: 1.3803, took: 35.9404s
Batch 1000/3907, reward: 2.894, loss: 1.6130, took: 38.7512s
Batch 1100/3907, reward: 2.894, loss: 0.4787, took: 38.8237s
Batch 1200/3907, reward: 2.894, loss: 0.1057, took: 38.8155s
Batch 1300/3907, reward: 2.894, loss: -0.1221, took: 38.6214s
Batch 1400/3907, reward: 2.894, loss: 0.3574, took: 38.8357s
Batch 1500/3907, reward: 2.894, loss: 0.0025, took: 38.6841s
Batch 1600/3907, reward: 2.894, loss: 0.6978, took: 38.8357s
Batch 1700/3907, reward: 2.894, loss: 0.0217, took: 38.8256s
Batch 1800/3907, reward: 2.894, loss: -0.2258, took: 38.7772s
Batch 1900/3907, reward: 2.894, loss: -0.1038, took: 38.8082s
Batch 2000/3907, reward: 2.894, loss: 0.3400, took: 38.7677s
Batch 2100/3907, reward: 2.894, loss: 1.1892, took: 38.6314s
Batch 2200/3907, reward: 2.894, loss: 1.3224, took: 38.6129s
Batch 2300/3907, reward: 2.894, loss: -0.0339, took: 38.6477s
Batch 2400/3907, reward: 2.894, loss: -0.3877, took: 38.4714s
Batch 2500/3907, reward: 2.894, loss: -0.5482, took: 38.4984s
Batch 2600/3907, reward: 2.894, loss: 0.3391, took: 38.5024s
Batch 2700/3907, reward: 2.894, loss: 0.7783, took: 38.6447s
Batch 2800/3907, reward: 2.894, loss: 0.0617, took: 38.4209s
Batch 2900/3907, reward: 2.894, loss: -0.7507, took: 38.4638s
Batch 3000/3907, reward: 2.894, loss: 0.2498, took: 38.7041s
Batch 3100/3907, reward: 2.894, loss: 0.6450, took: 38.5732s
Batch 3200/3907, reward: 2.894, loss: 0.1047, took: 38.4841s
Batch 3300/3907, reward: 2.894, loss: -0.9259, took: 35.7071s
Batch 3400/3907, reward: 2.894, loss: 0.0506, took: 38.5414s
Batch 3500/3907, reward: 2.893, loss: 0.2731, took: 38.5755s
Batch 3600/3907, reward: 2.894, loss: 0.6047, took: 38.6697s
Batch 3700/3907, reward: 2.893, loss: 0.4682, took: 38.6116s
Batch 3800/3907, reward: 2.893, loss: 0.8745, took: 38.6116s
Batch 3900/3907, reward: 2.893, loss: -0.0179, took: 38.7002s
Mean epoch loss/reward: 0.3211, 2.8938, 2.8669, took: 1512.0350s (37.5123s / 100 batches)
Batch 0/3907, reward: 2.893, loss: 2.2822, took: 0.5182s
Batch 100/3907, reward: 2.893, loss: 0.6903, took: 38.9839s
Batch 200/3907, reward: 2.893, loss: 0.0187, took: 38.5519s
Batch 300/3907, reward: 2.893, loss: -0.3045, took: 36.9127s
Batch 400/3907, reward: 2.893, loss: -0.2812, took: 37.7243s
Batch 500/3907, reward: 2.893, loss: 0.1469, took: 38.5922s
Batch 600/3907, reward: 2.893, loss: 0.1443, took: 38.6947s
Batch 700/3907, reward: 2.893, loss: 0.0719, took: 38.5381s
Batch 800/3907, reward: 2.892, loss: 0.3590, took: 38.5608s
Batch 900/3907, reward: 2.892, loss: 0.4402, took: 35.6004s
Batch 1000/3907, reward: 2.892, loss: -0.6819, took: 38.5105s
Batch 1100/3907, reward: 2.891, loss: -0.2443, took: 38.5642s
Batch 1200/3907, reward: 2.890, loss: 0.0751, took: 38.6175s
Batch 1300/3907, reward: 2.889, loss: 0.4285, took: 38.4467s
Batch 1400/3907, reward: 2.888, loss: 0.2926, took: 38.4616s
Batch 1500/3907, reward: 2.887, loss: -0.2423, took: 38.5377s
Batch 1600/3907, reward: 2.885, loss: -0.2408, took: 38.5158s
Batch 1700/3907, reward: 2.884, loss: -0.0601, took: 38.5287s
Batch 1800/3907, reward: 2.883, loss: 0.1309, took: 38.1753s
Batch 1900/3907, reward: 2.882, loss: 0.2622, took: 38.3104s
Batch 2000/3907, reward: 2.882, loss: -0.0107, took: 38.3987s
Batch 2100/3907, reward: 2.881, loss: 0.1019, took: 38.5696s
Batch 2200/3907, reward: 2.881, loss: 0.2683, took: 38.3552s
Batch 2300/3907, reward: 2.881, loss: -0.0101, took: 38.4286s
Batch 2400/3907, reward: 2.880, loss: -0.0395, took: 38.3299s
Batch 2500/3907, reward: 2.880, loss: -0.0134, took: 38.4966s
Batch 2600/3907, reward: 2.880, loss: 0.0253, took: 38.6079s
Batch 2700/3907, reward: 2.880, loss: 0.0387, took: 38.4008s
Batch 2800/3907, reward: 2.880, loss: 0.0882, took: 38.4760s
Batch 2900/3907, reward: 2.893, loss: 0.4118, took: 38.2947s
Batch 3000/3907, reward: 2.894, loss: 0.6667, took: 38.1367s
Batch 3100/3907, reward: 2.894, loss: -0.0836, took: 38.2533s
Batch 3200/3907, reward: 2.894, loss: 0.1163, took: 37.9620s
Batch 3300/3907, reward: 2.894, loss: 0.5435, took: 35.0007s
Batch 3400/3907, reward: 2.894, loss: -0.0363, took: 38.2279s
Batch 3500/3907, reward: 2.894, loss: 0.2231, took: 38.2278s
Batch 3600/3907, reward: 2.894, loss: 0.1094, took: 38.1179s
Batch 3700/3907, reward: 2.894, loss: -0.1635, took: 38.2755s
Batch 3800/3907, reward: 2.894, loss: -0.0035, took: 38.0351s
Batch 3900/3907, reward: 2.894, loss: -0.0056, took: 38.1045s
Mean epoch loss/reward: 0.0835, 2.8889, 2.8954, took: 1501.5046s (37.2511s / 100 batches)
Batch 0/3907, reward: 2.894, loss: -0.7989, took: 0.5544s
Batch 100/3907, reward: 2.894, loss: 0.1818, took: 38.2824s
Batch 200/3907, reward: 2.894, loss: -0.0360, took: 38.0200s
Batch 300/3907, reward: 2.894, loss: -0.1562, took: 36.2772s
Batch 400/3907, reward: 2.894, loss: -0.1510, took: 37.2592s
Batch 500/3907, reward: 2.894, loss: -0.0353, took: 38.1435s
Batch 600/3907, reward: 2.894, loss: 0.1690, took: 38.1484s
Batch 700/3907, reward: 2.894, loss: 0.1393, took: 38.3615s
Batch 800/3907, reward: 2.894, loss: 0.1957, took: 38.1382s
Batch 900/3907, reward: 2.894, loss: 0.1084, took: 35.1584s
Batch 1000/3907, reward: 2.894, loss: -0.0052, took: 38.3516s
Batch 1100/3907, reward: 2.894, loss: -0.0247, took: 38.1547s
Batch 1200/3907, reward: 2.894, loss: -0.0170, took: 38.1928s
Batch 1300/3907, reward: 2.894, loss: 0.0638, took: 38.2944s
Batch 1400/3907, reward: 2.894, loss: -0.0584, took: 38.0667s
Batch 1500/3907, reward: 2.894, loss: -0.0030, took: 38.1766s
Batch 1600/3907, reward: 2.894, loss: -0.3293, took: 38.1431s
Batch 1700/3907, reward: 2.894, loss: -0.0778, took: 38.2461s
Batch 1800/3907, reward: 2.894, loss: 0.0657, took: 38.3300s
Batch 1900/3907, reward: 2.894, loss: 0.0847, took: 38.1611s
Batch 2000/3907, reward: 2.894, loss: 0.2571, took: 38.3621s
Batch 2100/3907, reward: 2.894, loss: 0.3536, took: 38.4282s
Batch 2200/3907, reward: 2.894, loss: 0.0056, took: 38.2250s
Batch 2300/3907, reward: 2.894, loss: -0.0097, took: 38.2824s
Batch 2400/3907, reward: 2.894, loss: -0.2130, took: 38.2011s
Batch 2500/3907, reward: 2.894, loss: -0.1077, took: 38.1393s
Batch 2600/3907, reward: 2.894, loss: 0.3403, took: 38.1180s
Batch 2700/3907, reward: 2.894, loss: 0.0275, took: 37.9454s
Batch 2800/3907, reward: 2.894, loss: 0.0823, took: 38.0307s
Batch 2900/3907, reward: 2.894, loss: 0.1336, took: 38.1304s
Batch 3000/3907, reward: 2.894, loss: -0.0508, took: 38.1263s
Batch 3100/3907, reward: 2.894, loss: 0.0413, took: 37.8901s
Batch 3200/3907, reward: 2.894, loss: 0.0018, took: 38.2385s
Batch 3300/3907, reward: 2.894, loss: -0.0004, took: 35.0101s
Batch 3400/3907, reward: 2.894, loss: -0.0021, took: 38.2245s
Batch 3500/3907, reward: 2.894, loss: 0.0038, took: 38.4355s
Batch 3600/3907, reward: 2.894, loss: -0.0555, took: 38.3796s
Batch 3700/3907, reward: 2.894, loss: 0.1030, took: 38.3910s
Batch 3800/3907, reward: 2.894, loss: -0.0786, took: 38.4412s
Batch 3900/3907, reward: 2.894, loss: 0.4553, took: 38.5325s
Mean epoch loss/reward: 0.0370, 2.8940, 2.8954, took: 1493.4114s (37.0498s / 100 batches)
Batch 0/3907, reward: 2.893, loss: -1.0975, took: 0.5255s
Batch 100/3907, reward: 2.894, loss: -0.0010, took: 38.2651s
Batch 200/3907, reward: 2.894, loss: 0.1565, took: 38.2006s
Batch 300/3907, reward: 2.894, loss: 0.0131, took: 36.3709s
Batch 400/3907, reward: 2.894, loss: -0.1092, took: 36.8513s
Batch 500/3907, reward: 2.894, loss: -0.0043, took: 38.1235s
Batch 600/3907, reward: 2.894, loss: -0.0326, took: 37.9503s
Batch 700/3907, reward: 2.894, loss: -0.0048, took: 37.9855s
Batch 800/3907, reward: 2.894, loss: 0.2272, took: 38.1256s
Batch 900/3907, reward: 2.894, loss: -0.0790, took: 35.4642s
Batch 1000/3907, reward: 2.894, loss: 0.0264, took: 38.1469s
Batch 1100/3907, reward: 2.894, loss: -0.0111, took: 38.0141s
Batch 1200/3907, reward: 2.894, loss: -0.0167, took: 38.0952s
Batch 1300/3907, reward: 2.894, loss: 0.1844, took: 38.2397s
Batch 1400/3907, reward: 2.894, loss: 0.0051, took: 38.3873s
Batch 1500/3907, reward: 2.894, loss: 0.0378, took: 38.3057s
Batch 1600/3907, reward: 2.894, loss: -0.0004, took: 38.3510s
Batch 1700/3907, reward: 2.894, loss: 0.0065, took: 38.1713s
Batch 1800/3907, reward: 2.894, loss: -0.0052, took: 38.2164s
Batch 1900/3907, reward: 2.894, loss: 0.1931, took: 38.1829s
Batch 2000/3907, reward: 2.894, loss: -0.0570, took: 38.2640s
Batch 2100/3907, reward: 2.894, loss: 0.0317, took: 38.1013s
Batch 2200/3907, reward: 2.894, loss: 0.0044, took: 38.1735s
Batch 2300/3907, reward: 2.894, loss: -0.0023, took: 38.0728s
Batch 2400/3907, reward: 2.894, loss: -0.0614, took: 38.0739s
Batch 2500/3907, reward: 2.894, loss: 0.3458, took: 38.0206s
Batch 2600/3907, reward: 2.894, loss: 0.1562, took: 37.9626s
Batch 2700/3907, reward: 2.894, loss: 0.1424, took: 38.1786s
Batch 2800/3907, reward: 2.894, loss: -0.0019, took: 38.1009s
Batch 2900/3907, reward: 2.894, loss: -0.0802, took: 38.1271s
Batch 3000/3907, reward: 2.894, loss: -0.0041, took: 37.9974s
Batch 3100/3907, reward: 2.894, loss: -0.0037, took: 38.1693s
Batch 3200/3907, reward: 2.894, loss: 0.0012, took: 38.1242s
Batch 3300/3907, reward: 2.894, loss: 0.2751, took: 35.3578s
Batch 3400/3907, reward: 2.894, loss: 0.0188, took: 37.9344s
Batch 3500/3907, reward: 2.894, loss: 0.1005, took: 38.0513s
Batch 3600/3907, reward: 2.894, loss: 0.0015, took: 38.1698s
Batch 3700/3907, reward: 2.894, loss: -0.0013, took: 38.1127s
Batch 3800/3907, reward: 2.894, loss: -0.0016, took: 38.1540s
Batch 3900/3907, reward: 2.894, loss: -0.0000, took: 38.0521s
Mean epoch loss/reward: 0.0368, 2.8940, 2.8954, took: 1490.6110s (36.9793s / 100 batches)
Batch 0/3907, reward: 2.894, loss: -0.1438, took: 0.5232s
Batch 100/3907, reward: 2.894, loss: -0.0224, took: 38.2912s
Batch 200/3907, reward: 2.894, loss: -0.0009, took: 38.0300s
Batch 300/3907, reward: 2.894, loss: -0.0562, took: 36.4433s
Batch 400/3907, reward: 2.894, loss: -0.2446, took: 36.8309s
Batch 500/3907, reward: 2.894, loss: 0.0823, took: 38.0606s
Batch 600/3907, reward: 2.894, loss: -0.0407, took: 38.1156s
Batch 700/3907, reward: 2.894, loss: -0.0048, took: 38.1860s
Batch 800/3907, reward: 2.894, loss: 0.0015, took: 38.0827s
Batch 900/3907, reward: 2.894, loss: 0.0028, took: 34.9883s
Batch 1000/3907, reward: 2.894, loss: -0.0071, took: 38.1439s
Batch 1100/3907, reward: 2.894, loss: -0.1853, took: 38.1138s
Batch 1200/3907, reward: 2.894, loss: 0.2414, took: 38.1873s
Batch 1300/3907, reward: 2.894, loss: 0.1708, took: 38.1966s
Batch 1400/3907, reward: 2.894, loss: -0.1002, took: 38.0273s
Batch 1500/3907, reward: 2.894, loss: 0.1809, took: 38.2157s
Batch 1600/3907, reward: 2.894, loss: 0.0012, took: 38.1279s
Batch 1700/3907, reward: 2.894, loss: 0.0242, took: 38.1784s
Batch 1800/3907, reward: 2.894, loss: -0.0110, took: 38.0620s
Batch 1900/3907, reward: 2.894, loss: -0.0606, took: 37.9718s
Batch 2000/3907, reward: 2.894, loss: 0.0919, took: 38.1040s
Batch 2100/3907, reward: 2.894, loss: -0.0058, took: 38.2335s
Batch 2200/3907, reward: 2.894, loss: 0.0003, took: 38.2037s
Batch 2300/3907, reward: 2.894, loss: -0.0005, took: 38.3347s
Batch 2400/3907, reward: 2.894, loss: -0.0007, took: 38.2830s
Batch 2500/3907, reward: 2.894, loss: -0.0006, took: 38.1984s
Batch 2600/3907, reward: 2.894, loss: -0.0013, took: 38.1885s
Batch 2700/3907, reward: 2.894, loss: 0.0031, took: 38.0814s
Batch 2800/3907, reward: 2.894, loss: -0.0011, took: 38.1717s
Batch 2900/3907, reward: 2.894, loss: -0.0030, took: 38.2571s
Batch 3000/3907, reward: 2.894, loss: -0.0212, took: 38.0345s
Batch 3100/3907, reward: 2.894, loss: -0.0090, took: 38.1840s
Batch 3200/3907, reward: 2.894, loss: -0.0027, took: 38.2961s
Batch 3300/3907, reward: 2.894, loss: -0.0040, took: 35.0725s
Batch 3400/3907, reward: 2.894, loss: 0.0044, took: 37.9916s
Batch 3500/3907, reward: 2.894, loss: 0.0070, took: 38.1108s
Batch 3600/3907, reward: 2.894, loss: 0.1323, took: 38.1214s
Batch 3700/3907, reward: 2.894, loss: -0.0700, took: 38.1308s
Batch 3800/3907, reward: 2.894, loss: -0.0043, took: 38.0249s
Batch 3900/3907, reward: 2.894, loss: -0.0146, took: 38.1409s
Mean epoch loss/reward: 0.0018, 2.8940, 2.8954, took: 1490.3292s (36.9735s / 100 batches)
Batch 0/3907, reward: 2.894, loss: -0.0129, took: 0.5249s
Batch 100/3907, reward: 2.894, loss: -0.0056, took: 38.1718s
Batch 200/3907, reward: 2.894, loss: 0.0010, took: 38.1111s
Batch 300/3907, reward: 2.894, loss: 0.0002, took: 36.8804s
Batch 400/3907, reward: 2.894, loss: 0.0018, took: 36.4964s
Batch 500/3907, reward: 2.894, loss: -0.0519, took: 38.2525s
Batch 600/3907, reward: 2.894, loss: -0.1258, took: 38.1889s
Batch 700/3907, reward: 2.894, loss: 0.0018, took: 38.1164s
Batch 800/3907, reward: 2.894, loss: -0.0016, took: 38.0719s
Batch 900/3907, reward: 2.894, loss: 0.1185, took: 35.2441s
Batch 1000/3907, reward: 2.894, loss: 0.0542, took: 38.1007s
Batch 1100/3907, reward: 2.894, loss: 0.1019, took: 38.0436s
Batch 1200/3907, reward: 2.894, loss: -0.1183, took: 38.0135s
Batch 1300/3907, reward: 2.894, loss: 0.1544, took: 37.9668s
Batch 1400/3907, reward: 2.894, loss: 0.0011, took: 38.1775s
Batch 1500/3907, reward: 2.894, loss: -0.0002, took: 37.8965s
Batch 1600/3907, reward: 2.894, loss: 0.0034, took: 38.0846s
Batch 1700/3907, reward: 2.894, loss: 0.0110, took: 38.3214s
Batch 1800/3907, reward: 2.894, loss: 0.1296, took: 38.0091s
Batch 1900/3907, reward: 2.894, loss: 0.1231, took: 38.0538s
Batch 2000/3907, reward: 2.894, loss: 0.0861, took: 38.0827s
Batch 2100/3907, reward: 2.894, loss: 0.0933, took: 38.1854s
Batch 2200/3907, reward: 2.894, loss: 0.0026, took: 38.2114s
Batch 2300/3907, reward: 2.894, loss: -0.0007, took: 38.3320s
Batch 2400/3907, reward: 2.894, loss: 0.0032, took: 38.1006s
Batch 2500/3907, reward: 2.894, loss: -0.0020, took: 38.4006s
Batch 2600/3907, reward: 2.894, loss: 0.0040, took: 37.9510s
Batch 2700/3907, reward: 2.894, loss: -0.0013, took: 37.9299s
Batch 2800/3907, reward: 2.894, loss: -0.0047, took: 37.9920s
Batch 2900/3907, reward: 2.894, loss: 0.0011, took: 38.0024s
Batch 3000/3907, reward: 2.894, loss: -0.0008, took: 38.1290s
Batch 3100/3907, reward: 2.894, loss: -0.0042, took: 37.9641s
Batch 3200/3907, reward: 2.894, loss: 0.0029, took: 38.0352s
Batch 3300/3907, reward: 2.894, loss: -0.0011, took: 35.0368s
Batch 3400/3907, reward: 2.894, loss: -0.0014, took: 37.9622s
Batch 3500/3907, reward: 2.894, loss: 0.0086, took: 37.8429s
Batch 3600/3907, reward: 2.894, loss: 0.2327, took: 37.9539s
Batch 3700/3907, reward: 2.894, loss: 0.0261, took: 38.0810s
Batch 3800/3907, reward: 2.894, loss: 0.0009, took: 38.1375s
Batch 3900/3907, reward: 2.894, loss: 0.0001, took: 38.1913s
Mean epoch loss/reward: 0.0215, 2.8940, 2.8954, took: 1488.6313s (36.9312s / 100 batches)
Batch 0/3907, reward: 2.895, loss: 0.0061, took: 0.5526s
Batch 100/3907, reward: 2.894, loss: -0.0009, took: 38.4967s
Batch 200/3907, reward: 2.894, loss: 0.0003, took: 38.1591s
Batch 300/3907, reward: 2.894, loss: -0.0346, took: 37.1758s
Batch 400/3907, reward: 2.894, loss: -0.1662, took: 35.9072s
Batch 500/3907, reward: 2.894, loss: 0.3085, took: 38.0007s
Batch 600/3907, reward: 2.894, loss: -0.0050, took: 38.0660s
Batch 700/3907, reward: 2.894, loss: -0.0012, took: 37.8835s
Batch 800/3907, reward: 2.894, loss: 0.0008, took: 37.8940s
Batch 900/3907, reward: 2.894, loss: -0.0018, took: 35.1700s
Batch 1000/3907, reward: 2.894, loss: -0.0019, took: 38.1323s
Batch 1100/3907, reward: 2.894, loss: -0.0007, took: 38.0738s
Batch 1200/3907, reward: 2.894, loss: 0.0029, took: 38.1212s
Batch 1300/3907, reward: 2.894, loss: -0.0040, took: 37.9694s
Batch 1400/3907, reward: 2.894, loss: 0.0026, took: 38.1156s
Batch 1500/3907, reward: 2.894, loss: 0.0027, took: 38.1996s
Batch 1600/3907, reward: 2.894, loss: -0.0082, took: 38.2666s
Batch 1700/3907, reward: 2.894, loss: 0.0566, took: 37.9681s
Batch 1800/3907, reward: 2.894, loss: -0.0902, took: 38.1021s
Batch 1900/3907, reward: 2.894, loss: 0.0979, took: 38.1116s
Batch 2000/3907, reward: 2.894, loss: 0.1438, took: 38.0421s
Batch 2100/3907, reward: 2.894, loss: 0.2212, took: 38.1896s
Batch 2200/3907, reward: 2.894, loss: 0.2261, took: 38.0063s
Batch 2300/3907, reward: 2.894, loss: -0.0165, took: 38.0146s
Batch 2400/3907, reward: 2.894, loss: 0.1135, took: 38.0250s
Batch 2500/3907, reward: 2.894, loss: 0.0606, took: 38.0042s
Batch 2600/3907, reward: 2.894, loss: 0.0220, took: 38.1615s
Batch 2700/3907, reward: 2.894, loss: 0.0005, took: 38.0555s
Batch 2800/3907, reward: 2.894, loss: -0.0009, took: 38.1739s
Batch 2900/3907, reward: 2.894, loss: 0.0010, took: 38.1077s
Batch 3000/3907, reward: 2.894, loss: -0.0001, took: 37.9701s
Batch 3100/3907, reward: 2.894, loss: -0.0002, took: 38.1827s
Batch 3200/3907, reward: 2.894, loss: 0.0002, took: 37.9822s
Batch 3300/3907, reward: 2.894, loss: 0.0003, took: 35.0636s
Batch 3400/3907, reward: 2.894, loss: -0.0015, took: 38.0104s
Batch 3500/3907, reward: 2.894, loss: -0.0006, took: 37.9012s
Batch 3600/3907, reward: 2.894, loss: 0.0013, took: 38.0201s
Batch 3700/3907, reward: 2.894, loss: -0.0013, took: 38.1178s
Batch 3800/3907, reward: 2.894, loss: -0.0002, took: 38.1849s
Batch 3900/3907, reward: 2.894, loss: 0.0070, took: 38.1431s
Mean epoch loss/reward: 0.0238, 2.8940, 2.8954, took: 1488.1759s (36.9181s / 100 batches)
Batch 0/3907, reward: 2.894, loss: 0.0803, took: 0.5245s
Batch 100/3907, reward: 2.894, loss: 0.0120, took: 38.4862s
Batch 200/3907, reward: 2.894, loss: 0.0023, took: 38.2079s
Batch 300/3907, reward: 2.894, loss: 0.0030, took: 37.8425s
Batch 400/3907, reward: 2.894, loss: -0.0035, took: 35.8921s
Batch 500/3907, reward: 2.894, loss: 0.0041, took: 38.1635s
Batch 600/3907, reward: 2.894, loss: -0.0451, took: 38.2103s
Batch 700/3907, reward: 2.894, loss: 0.0345, took: 37.9982s
Batch 800/3907, reward: 2.894, loss: -0.0029, took: 38.0947s
Batch 900/3907, reward: 2.894, loss: 0.0027, took: 35.5107s
Batch 1000/3907, reward: 2.894, loss: 0.0005, took: 38.1213s
Batch 1100/3907, reward: 2.894, loss: 0.0000, took: 38.0838s
Batch 1200/3907, reward: 2.894, loss: -0.0026, took: 38.0664s
Batch 1300/3907, reward: 2.894, loss: -0.0013, took: 37.9947s
Batch 1400/3907, reward: 2.894, loss: 0.0029, took: 38.2579s
Batch 1500/3907, reward: 2.894, loss: 0.0330, took: 37.9873s
Batch 1600/3907, reward: 2.894, loss: 0.0273, took: 38.0499s
Batch 1700/3907, reward: 2.894, loss: 0.0144, took: 38.0859s
Batch 1800/3907, reward: 2.894, loss: -0.0014, took: 38.1679s
Batch 1900/3907, reward: 2.894, loss: 0.0003, took: 38.0559s
Batch 2000/3907, reward: 2.894, loss: -0.0004, took: 38.0963s
Batch 2100/3907, reward: 2.894, loss: 0.0000, took: 38.1788s
Batch 2200/3907, reward: 2.894, loss: 0.0023, took: 38.1272s
Batch 2300/3907, reward: 2.894, loss: -0.0009, took: 38.0628s
Batch 2400/3907, reward: 2.894, loss: 0.0248, took: 37.9481s
Batch 2500/3907, reward: 2.894, loss: -0.0048, took: 38.0973s
Batch 2600/3907, reward: 2.894, loss: 0.0024, took: 37.9903s
Batch 2700/3907, reward: 2.894, loss: -0.0029, took: 38.1282s
Batch 2800/3907, reward: 2.894, loss: 0.0257, took: 38.1120s
Batch 2900/3907, reward: 2.894, loss: -0.0003, took: 37.9188s
Batch 3000/3907, reward: 2.894, loss: 0.0109, took: 37.9221s
Batch 3100/3907, reward: 2.894, loss: 0.0003, took: 37.9830s
Batch 3200/3907, reward: 2.894, loss: -0.0044, took: 37.9419s
Batch 3300/3907, reward: 2.894, loss: 0.0019, took: 34.8932s
Batch 3400/3907, reward: 2.894, loss: -0.0028, took: 38.1868s
Batch 3500/3907, reward: 2.894, loss: 0.0001, took: 38.2781s
Batch 3600/3907, reward: 2.894, loss: -0.2337, took: 38.1450s
Batch 3700/3907, reward: 2.894, loss: 0.1486, took: 38.0953s
Batch 3800/3907, reward: 2.894, loss: -0.0055, took: 38.3219s
Batch 3900/3907, reward: 2.894, loss: -0.0015, took: 38.2756s
Mean epoch loss/reward: 0.0010, 2.8940, 2.8954, took: 1489.9487s (36.9626s / 100 batches)
Batch 0/3907, reward: 2.894, loss: 0.2350, took: 0.5450s
Batch 100/3907, reward: 2.894, loss: -0.0003, took: 38.2398s
Batch 200/3907, reward: 2.894, loss: -0.0004, took: 37.9698s
Batch 300/3907, reward: 2.894, loss: 0.0017, took: 37.7090s
Batch 400/3907, reward: 2.894, loss: -0.0015, took: 35.6685s
Batch 500/3907, reward: 2.894, loss: -0.0016, took: 37.8389s
Batch 600/3907, reward: 2.894, loss: 0.0017, took: 38.1316s
Batch 700/3907, reward: 2.894, loss: -0.0018, took: 38.0654s
Batch 800/3907, reward: 2.894, loss: 0.1034, took: 38.0679s
Batch 900/3907, reward: 2.894, loss: -0.1397, took: 35.1871s
Batch 1000/3907, reward: 2.894, loss: 0.0391, took: 38.1343s
Batch 1100/3907, reward: 2.894, loss: 0.0021, took: 38.1057s
Batch 1200/3907, reward: 2.894, loss: 0.0006, took: 38.1033s
Batch 1300/3907, reward: 2.894, loss: 0.0005, took: 38.1377s
Batch 1400/3907, reward: 2.894, loss: -0.0049, took: 38.2771s
Batch 1500/3907, reward: 2.894, loss: 0.0034, took: 38.3211s
Batch 1600/3907, reward: 2.894, loss: 0.0005, took: 38.1422s
Batch 1700/3907, reward: 2.894, loss: -0.0002, took: 38.2152s
Batch 1800/3907, reward: 2.894, loss: -0.0018, took: 38.4042s
Batch 1900/3907, reward: 2.894, loss: 0.0006, took: 38.1787s
Batch 2000/3907, reward: 2.894, loss: 0.0014, took: 38.1947s
Batch 2100/3907, reward: 2.894, loss: 0.0127, took: 38.1489s
Batch 2200/3907, reward: 2.894, loss: 0.0341, took: 38.0915s
Batch 2300/3907, reward: 2.894, loss: 0.0693, took: 38.1180s
Batch 2400/3907, reward: 2.894, loss: 0.0235, took: 38.0466s
Batch 2500/3907, reward: 2.894, loss: 0.0113, took: 38.0174s
Batch 2600/3907, reward: 2.894, loss: -0.0016, took: 38.0592s
Batch 2700/3907, reward: 2.894, loss: 0.0021, took: 37.7876s
Batch 2800/3907, reward: 2.894, loss: -0.0345, took: 37.8257s
Batch 2900/3907, reward: 2.894, loss: 0.0035, took: 38.0506s
Batch 3000/3907, reward: 2.894, loss: 0.0009, took: 37.9284s
Batch 3100/3907, reward: 2.894, loss: 0.0001, took: 37.9345s
Batch 3200/3907, reward: 2.894, loss: -0.0003, took: 38.0121s
Batch 3300/3907, reward: 2.894, loss: 0.0000, took: 35.1790s
Batch 3400/3907, reward: 2.894, loss: 0.0006, took: 38.0203s
Batch 3500/3907, reward: 2.894, loss: -0.0291, took: 38.1401s
Batch 3600/3907, reward: 2.894, loss: -0.0963, took: 38.1572s
Batch 3700/3907, reward: 2.894, loss: -0.0003, took: 38.2534s
Batch 3800/3907, reward: 2.894, loss: -0.0043, took: 38.1416s
Batch 3900/3907, reward: 2.894, loss: 0.0272, took: 38.1380s
Mean epoch loss/reward: 0.0006, 2.8940, 2.8954, took: 1489.0680s (36.9422s / 100 batches)
Batch 0/3907, reward: 2.894, loss: -1.8706, took: 0.5606s
Batch 100/3907, reward: 2.894, loss: -0.1037, took: 38.2941s
Batch 200/3907, reward: 2.894, loss: 0.0012, took: 38.0658s
Batch 300/3907, reward: 2.894, loss: -0.0004, took: 37.9599s
Batch 400/3907, reward: 2.894, loss: 0.0001, took: 35.1402s
Batch 500/3907, reward: 2.894, loss: -0.0006, took: 38.0012s
Batch 600/3907, reward: 2.894, loss: -0.0024, took: 37.8098s
Batch 700/3907, reward: 2.894, loss: 0.0334, took: 37.8623s
Batch 800/3907, reward: 2.894, loss: 0.0036, took: 37.8170s
Batch 900/3907, reward: 2.894, loss: -0.0052, took: 35.0889s
Batch 1000/3907, reward: 2.894, loss: 0.0011, took: 38.0893s
Batch 1100/3907, reward: 2.894, loss: -0.0392, took: 37.9999s
Batch 1200/3907, reward: 2.894, loss: -0.1853, took: 37.9700s
Batch 1300/3907, reward: 2.894, loss: -0.0000, took: 37.9592s
Batch 1400/3907, reward: 2.894, loss: 0.0026, took: 38.0003s
Batch 1500/3907, reward: 2.894, loss: 0.0015, took: 37.9501s
Batch 1600/3907, reward: 2.894, loss: -0.0078, took: 37.9746s
Batch 1700/3907, reward: 2.894, loss: 0.0092, took: 38.1303s
Batch 1800/3907, reward: 2.894, loss: 0.0010, took: 37.9762s
Batch 1900/3907, reward: 2.894, loss: 0.0119, took: 38.0181s
Batch 2000/3907, reward: 2.894, loss: -0.0040, took: 37.9261s
Batch 2100/3907, reward: 2.894, loss: 0.0023, took: 38.0895s
Batch 2200/3907, reward: 2.894, loss: 0.0019, took: 38.0151s
Batch 2300/3907, reward: 2.894, loss: 0.0282, took: 37.9933s
Batch 2400/3907, reward: 2.894, loss: -0.0764, took: 37.8810s
Batch 2500/3907, reward: 2.894, loss: -0.0207, took: 37.9244s
Batch 2600/3907, reward: 2.894, loss: 0.0014, took: 37.8692s
Batch 2700/3907, reward: 2.894, loss: 0.0005, took: 38.1330s
Batch 2800/3907, reward: 2.894, loss: 0.0009, took: 38.0807s
Batch 2900/3907, reward: 2.894, loss: 0.0063, took: 37.9380s
Batch 3000/3907, reward: 2.894, loss: 0.0028, took: 38.0172s
Batch 3100/3907, reward: 2.894, loss: -0.0009, took: 37.9618s
Batch 3200/3907, reward: 2.894, loss: -0.0003, took: 38.0435s
Batch 3300/3907, reward: 2.894, loss: -0.0011, took: 34.9249s
Batch 3400/3907, reward: 2.894, loss: 0.0065, took: 38.1797s
Batch 3500/3907, reward: 2.894, loss: -0.0059, took: 37.9826s
Batch 3600/3907, reward: 2.894, loss: 0.0031, took: 38.0143s
Batch 3700/3907, reward: 2.894, loss: -0.0019, took: 38.1677s
Batch 3800/3907, reward: 2.894, loss: 0.0037, took: 38.0730s
Batch 3900/3907, reward: 2.894, loss: 0.0090, took: 37.9891s
Mean epoch loss/reward: -0.0088, 2.8940, 2.8954, took: 1485.2127s (36.8468s / 100 batches)
Batch 0/3907, reward: 2.892, loss: -0.0952, took: 0.4559s
Batch 100/3907, reward: 2.894, loss: 0.0006, took: 38.0260s
Batch 200/3907, reward: 2.894, loss: 0.0018, took: 37.8663s
Batch 300/3907, reward: 2.894, loss: -0.0025, took: 37.8566s
Batch 400/3907, reward: 2.894, loss: -0.0027, took: 35.1131s
Batch 500/3907, reward: 2.894, loss: 0.0076, took: 37.9046s
Batch 600/3907, reward: 2.894, loss: -0.0003, took: 37.9260s
Batch 700/3907, reward: 2.894, loss: -0.0023, took: 37.9822s
Batch 800/3907, reward: 2.894, loss: 0.0036, took: 38.1183s
Batch 900/3907, reward: 2.894, loss: -0.1050, took: 34.9904s
Batch 1000/3907, reward: 2.894, loss: -0.0319, took: 38.0648s
Batch 1100/3907, reward: 2.894, loss: 0.0223, took: 38.0545s
Batch 1200/3907, reward: 2.894, loss: 0.0129, took: 38.0749s
Batch 1300/3907, reward: 2.894, loss: -0.0013, took: 38.1424s
Batch 1400/3907, reward: 2.894, loss: -0.0012, took: 38.0675s
Batch 1500/3907, reward: 2.894, loss: 0.0013, took: 38.1087s
Batch 1600/3907, reward: 2.894, loss: -0.0005, took: 38.0062s
Batch 1700/3907, reward: 2.894, loss: -0.0008, took: 38.0012s
Batch 1800/3907, reward: 2.894, loss: 0.0006, took: 37.9995s
Batch 1900/3907, reward: 2.894, loss: -0.0023, took: 37.9688s
Batch 2000/3907, reward: 2.894, loss: -0.0004, took: 37.7408s
Batch 2100/3907, reward: 2.894, loss: 0.0015, took: 37.8900s
Batch 2200/3907, reward: 2.894, loss: -0.0004, took: 37.8314s
Batch 2300/3907, reward: 2.894, loss: -0.0027, took: 37.9501s
Batch 2400/3907, reward: 2.894, loss: -0.0827, took: 37.9673s
Batch 2500/3907, reward: 2.894, loss: 0.1093, took: 37.8179s
Batch 2600/3907, reward: 2.894, loss: 0.1837, took: 38.0267s
Batch 2700/3907, reward: 2.894, loss: 0.0994, took: 38.0012s
Batch 2800/3907, reward: 2.894, loss: 0.1353, took: 37.8885s
Batch 2900/3907, reward: 2.894, loss: -0.0131, took: 38.0056s
Batch 3000/3907, reward: 2.894, loss: 0.0002, took: 38.0707s
Batch 3100/3907, reward: 2.894, loss: 0.0006, took: 37.9267s
Batch 3200/3907, reward: 2.894, loss: 0.0003, took: 38.0424s
Batch 3300/3907, reward: 2.894, loss: -0.0016, took: 34.8664s
Batch 3400/3907, reward: 2.894, loss: -0.0001, took: 37.9354s
Batch 3500/3907, reward: 2.894, loss: 0.0023, took: 37.9295s
Batch 3600/3907, reward: 2.894, loss: -0.0011, took: 38.0207s
Batch 3700/3907, reward: 2.894, loss: -0.0006, took: 37.7773s
Batch 3800/3907, reward: 2.894, loss: 0.0009, took: 37.9588s
Batch 3900/3907, reward: 2.894, loss: 0.0002, took: 37.9179s
Mean epoch loss/reward: 0.0085, 2.8940, 2.8954, took: 1483.6672s (36.8073s / 100 batches)
Batch 0/3907, reward: 2.895, loss: -0.3188, took: 0.5127s
Batch 100/3907, reward: 2.894, loss: 0.0042, took: 38.2116s
Batch 200/3907, reward: 2.894, loss: -0.0019, took: 37.8568s
Batch 300/3907, reward: 2.894, loss: -0.0015, took: 38.0859s
Batch 400/3907, reward: 2.894, loss: -0.0007, took: 34.8018s
Batch 500/3907, reward: 2.894, loss: 0.0005, took: 37.9020s
Batch 600/3907, reward: 2.894, loss: 0.0027, took: 38.0922s
Batch 700/3907, reward: 2.894, loss: -0.0023, took: 37.9388s
Batch 800/3907, reward: 2.894, loss: 0.0001, took: 37.8369s
Batch 900/3907, reward: 2.894, loss: -0.0011, took: 35.0002s
Batch 1000/3907, reward: 2.894, loss: -0.0017, took: 38.0891s
Batch 1100/3907, reward: 2.894, loss: 0.0002, took: 37.9956s
Batch 1200/3907, reward: 2.894, loss: -0.0011, took: 38.0213s
Batch 1300/3907, reward: 2.894, loss: -0.0004, took: 38.0744s
Batch 1400/3907, reward: 2.894, loss: 0.0127, took: 38.1129s
Batch 1500/3907, reward: 2.894, loss: -0.0018, took: 37.9977s
Batch 1600/3907, reward: 2.894, loss: 0.0009, took: 38.0876s
Batch 1700/3907, reward: 2.894, loss: -0.0016, took: 38.0500s
Batch 1800/3907, reward: 2.894, loss: -0.0002, took: 37.9770s
Batch 1900/3907, reward: 2.894, loss: 0.0012, took: 38.0428s
Batch 2000/3907, reward: 2.894, loss: -0.0023, took: 37.8977s
Batch 2100/3907, reward: 2.894, loss: 0.0027, took: 37.9155s
Batch 2200/3907, reward: 2.894, loss: -0.0013, took: 37.9861s
Batch 2300/3907, reward: 2.894, loss: 0.0011, took: 37.8779s
Batch 2400/3907, reward: 2.894, loss: -0.0032, took: 37.7875s
Batch 2500/3907, reward: 2.894, loss: 0.0017, took: 37.9029s
Batch 2600/3907, reward: 2.894, loss: 0.0005, took: 37.7183s
Batch 2700/3907, reward: 2.894, loss: -0.0008, took: 37.8963s
Batch 2800/3907, reward: 2.894, loss: -0.0041, took: 37.8996s
Batch 2900/3907, reward: 2.894, loss: 0.0014, took: 37.8049s
Batch 3000/3907, reward: 2.894, loss: -0.0003, took: 37.7210s
Batch 3100/3907, reward: 2.894, loss: 0.0010, took: 37.8765s
Batch 3200/3907, reward: 2.894, loss: 0.0015, took: 37.8173s
Batch 3300/3907, reward: 2.894, loss: -0.0076, took: 34.8317s
Batch 3400/3907, reward: 2.894, loss: -0.1423, took: 37.8660s
Batch 3500/3907, reward: 2.894, loss: 0.0751, took: 37.8086s
Batch 3600/3907, reward: 2.894, loss: 0.0015, took: 37.9131s
Batch 3700/3907, reward: 2.894, loss: -0.0004, took: 37.9525s
Batch 3800/3907, reward: 2.894, loss: 0.0247, took: 38.0475s
Batch 3900/3907, reward: 2.894, loss: 0.0307, took: 38.0858s
Mean epoch loss/reward: -0.0004, 2.8940, 2.8954, took: 1482.3421s (36.7824s / 100 batches)
Batch 0/3907, reward: 2.894, loss: -0.2404, took: 0.5412s
Batch 100/3907, reward: 2.894, loss: -0.0007, took: 38.3068s
Batch 200/3907, reward: 2.894, loss: 0.0015, took: 38.1108s
Batch 300/3907, reward: 2.894, loss: -0.0009, took: 38.1363s
Batch 400/3907, reward: 2.894, loss: -0.0009, took: 35.2346s
Batch 500/3907, reward: 2.894, loss: -0.0003, took: 38.1790s
Batch 600/3907, reward: 2.894, loss: 0.0015, took: 37.9212s
Batch 700/3907, reward: 2.894, loss: -0.0031, took: 37.9720s
Batch 800/3907, reward: 2.894, loss: 0.0034, took: 37.9222s
Batch 900/3907, reward: 2.894, loss: -0.0012, took: 35.0706s
Batch 1000/3907, reward: 2.894, loss: -0.0011, took: 37.9336s
Batch 1100/3907, reward: 2.894, loss: -0.0024, took: 37.8471s
Batch 1200/3907, reward: 2.894, loss: 0.0031, took: 37.8957s
Batch 1300/3907, reward: 2.894, loss: -0.0007, took: 38.0025s
Batch 1400/3907, reward: 2.894, loss: -0.0003, took: 38.0107s
Batch 1500/3907, reward: 2.894, loss: 0.0020, took: 38.1478s
Batch 1600/3907, reward: 2.894, loss: -0.0093, took: 38.1701s
Batch 1700/3907, reward: 2.894, loss: -0.0016, took: 38.1144s
Batch 1800/3907, reward: 2.894, loss: 0.0009, took: 38.1944s
Batch 1900/3907, reward: 2.894, loss: -0.0018, took: 38.0501s
Batch 2000/3907, reward: 2.894, loss: 0.0023, took: 38.0744s
Batch 2100/3907, reward: 2.894, loss: -0.0021, took: 38.1716s
Batch 2200/3907, reward: 2.894, loss: -0.0027, took: 37.8545s
Batch 2300/3907, reward: 2.894, loss: -0.1596, took: 37.8159s
Batch 2400/3907, reward: 2.894, loss: -0.2371, took: 37.9817s
Batch 2500/3907, reward: 2.894, loss: 0.0240, took: 37.9981s
Batch 2600/3907, reward: 2.894, loss: -0.0005, took: 37.8938s
Batch 2700/3907, reward: 2.894, loss: 0.0010, took: 37.8585s
Batch 2800/3907, reward: 2.894, loss: -0.0006, took: 38.0094s
Batch 2900/3907, reward: 2.894, loss: 0.0021, took: 37.9993s
Batch 3000/3907, reward: 2.894, loss: -0.0016, took: 38.0133s
Batch 3100/3907, reward: 2.894, loss: -0.0020, took: 38.0331s
Batch 3200/3907, reward: 2.894, loss: 0.0011, took: 38.0639s
Batch 3300/3907, reward: 2.894, loss: -0.0028, took: 35.3346s
Batch 3400/3907, reward: 2.894, loss: 0.0006, took: 38.0667s
Batch 3500/3907, reward: 2.894, loss: -0.0001, took: 38.0779s
Batch 3600/3907, reward: 2.894, loss: -0.0036, took: 38.1612s
Batch 3700/3907, reward: 2.894, loss: 0.0026, took: 38.2200s
Batch 3800/3907, reward: 2.894, loss: -0.0014, took: 38.1162s
Batch 3900/3907, reward: 2.894, loss: 0.0003, took: 38.1137s
Mean epoch loss/reward: -0.0101, 2.8940, 2.8954, took: 1486.7954s (36.8905s / 100 batches)
Batch 0/3907, reward: 2.893, loss: 0.3211, took: 0.5207s
Batch 100/3907, reward: 2.894, loss: -0.0010, took: 38.0408s
Batch 200/3907, reward: 2.894, loss: -0.0035, took: 37.9025s
Batch 300/3907, reward: 2.894, loss: 0.0001, took: 37.9263s
Batch 400/3907, reward: 2.894, loss: 0.0002, took: 35.0589s
Batch 500/3907, reward: 2.894, loss: 0.0033, took: 37.8609s
Batch 600/3907, reward: 2.894, loss: -0.0046, took: 37.9987s
Batch 700/3907, reward: 2.894, loss: 0.0263, took: 37.8094s
Batch 800/3907, reward: 2.894, loss: -0.0953, took: 37.8773s
Batch 900/3907, reward: 2.894, loss: 0.1256, took: 35.1291s
Batch 1000/3907, reward: 2.894, loss: -0.0029, took: 37.9954s
Batch 1100/3907, reward: 2.894, loss: 0.0012, took: 37.9705s
Batch 1200/3907, reward: 2.894, loss: -0.0017, took: 38.1940s
Batch 1300/3907, reward: 2.894, loss: 0.0033, took: 37.9514s
Batch 1400/3907, reward: 2.894, loss: -0.0030, took: 38.2276s
Batch 1500/3907, reward: 2.894, loss: -0.0004, took: 38.0557s
Batch 1600/3907, reward: 2.894, loss: 0.0003, took: 38.1098s
Batch 1700/3907, reward: 2.894, loss: -0.0009, took: 37.9782s
Batch 1800/3907, reward: 2.894, loss: 0.0011, took: 38.0135s
Batch 1900/3907, reward: 2.894, loss: -0.0008, took: 38.0257s
Batch 2000/3907, reward: 2.894, loss: 0.0019, took: 37.9143s
Batch 2100/3907, reward: 2.894, loss: 0.0004, took: 37.9461s
Batch 2200/3907, reward: 2.894, loss: -0.0011, took: 37.9643s
Batch 2300/3907, reward: 2.894, loss: 0.0655, took: 37.8915s
Batch 2400/3907, reward: 2.894, loss: -0.0377, took: 37.8497s
Batch 2500/3907, reward: 2.894, loss: -0.0004, took: 37.9892s
Batch 2600/3907, reward: 2.894, loss: -0.0016, took: 38.0147s
Batch 2700/3907, reward: 2.894, loss: 0.0032, took: 38.1579s
Batch 2800/3907, reward: 2.894, loss: -0.0026, took: 38.0759s
Batch 2900/3907, reward: 2.894, loss: 0.0004, took: 38.1564s
Batch 3000/3907, reward: 2.894, loss: 0.0009, took: 38.0456s
Batch 3100/3907, reward: 2.894, loss: -0.0020, took: 38.2413s
Batch 3200/3907, reward: 2.894, loss: 0.0040, took: 37.8952s
Batch 3300/3907, reward: 2.894, loss: -0.0040, took: 35.1616s
Batch 3400/3907, reward: 2.894, loss: -0.0027, took: 37.9743s
Batch 3500/3907, reward: 2.894, loss: 0.0022, took: 37.9532s
Batch 3600/3907, reward: 2.894, loss: -0.0017, took: 37.8913s
Batch 3700/3907, reward: 2.894, loss: -0.0007, took: 37.9909s
Batch 3800/3907, reward: 2.894, loss: 0.0009, took: 37.9208s
Batch 3900/3907, reward: 2.894, loss: 0.0000, took: 37.8907s
Mean epoch loss/reward: 0.0019, 2.8940, 2.8954, took: 1484.6925s (36.8393s / 100 batches)
Batch 0/3907, reward: 2.894, loss: -0.3054, took: 0.5579s
Batch 100/3907, reward: 2.894, loss: -0.0004, took: 38.0550s
Batch 200/3907, reward: 2.894, loss: 0.0022, took: 37.8025s
Batch 300/3907, reward: 2.894, loss: 0.0009, took: 38.0224s
Batch 400/3907, reward: 2.894, loss: -0.0049, took: 35.0326s
Batch 500/3907, reward: 2.894, loss: 0.0018, took: 37.9623s
Batch 600/3907, reward: 2.894, loss: -0.0021, took: 38.1081s
Batch 700/3907, reward: 2.894, loss: -0.0032, took: 37.9195s
Batch 800/3907, reward: 2.894, loss: 0.0001, took: 37.7526s
Batch 900/3907, reward: 2.894, loss: 0.0034, took: 35.1000s
Batch 1000/3907, reward: 2.894, loss: 0.0661, took: 38.0354s
Batch 1100/3907, reward: 2.894, loss: -0.0039, took: 38.0700s
Batch 1200/3907, reward: 2.894, loss: -0.0120, took: 38.1879s
Batch 1300/3907, reward: 2.894, loss: 0.0016, took: 37.9807s
Batch 1400/3907, reward: 2.894, loss: -0.0005, took: 38.2246s
Batch 1500/3907, reward: 2.894, loss: 0.0003, took: 38.2667s
Batch 1600/3907, reward: 2.894, loss: -0.0001, took: 37.9256s
Batch 1700/3907, reward: 2.894, loss: -0.0010, took: 38.2997s
Batch 1800/3907, reward: 2.894, loss: -0.0006, took: 37.9635s
Batch 1900/3907, reward: 2.894, loss: 0.0013, took: 37.9628s
Batch 2000/3907, reward: 2.894, loss: -0.0001, took: 37.9521s
Batch 2100/3907, reward: 2.894, loss: -0.0014, took: 37.9728s
Batch 2200/3907, reward: 2.894, loss: 0.0026, took: 37.9471s
Batch 2300/3907, reward: 2.894, loss: -0.0024, took: 37.9199s
Batch 2400/3907, reward: 2.894, loss: 0.0011, took: 38.0192s
Batch 2500/3907, reward: 2.894, loss: -0.0019, took: 37.9560s
Batch 2600/3907, reward: 2.894, loss: 0.0022, took: 38.1328s
Batch 2700/3907, reward: 2.894, loss: 0.0001, took: 37.9039s
Batch 2800/3907, reward: 2.894, loss: -0.0028, took: 38.1307s
Batch 2900/3907, reward: 2.894, loss: -0.0003, took: 38.1415s
Batch 3000/3907, reward: 2.894, loss: -0.0019, took: 38.1215s
Batch 3100/3907, reward: 2.894, loss: 0.0054, took: 38.0038s
Batch 3200/3907, reward: 2.894, loss: -0.0003, took: 38.0748s
Batch 3300/3907, reward: 2.894, loss: 0.0025, took: 34.9158s
Batch 3400/3907, reward: 2.894, loss: 0.0007, took: 37.9942s
Batch 3500/3907, reward: 2.894, loss: -0.0017, took: 37.9781s
Batch 3600/3907, reward: 2.894, loss: -0.0015, took: 37.9836s
Batch 3700/3907, reward: 2.894, loss: -0.0002, took: 38.0568s
Batch 3800/3907, reward: 2.894, loss: 0.0005, took: 37.9202s
Batch 3900/3907, reward: 2.894, loss: -0.0385, took: 37.9524s
Mean epoch loss/reward: 0.0001, 2.8940, 2.8954, took: 1485.6634s (36.8577s / 100 batches)
Batch 0/3907, reward: 2.894, loss: -0.9589, took: 0.5120s
Batch 100/3907, reward: 2.894, loss: 0.0121, took: 38.1301s
Batch 200/3907, reward: 2.894, loss: 0.0023, took: 37.9436s
Batch 300/3907, reward: 2.894, loss: 0.0002, took: 37.7727s
Batch 400/3907, reward: 2.894, loss: -0.0001, took: 35.2773s
Batch 500/3907, reward: 2.894, loss: -0.0013, took: 37.8834s
Batch 600/3907, reward: 2.894, loss: 0.0008, took: 37.8369s
Batch 700/3907, reward: 2.894, loss: -0.0005, took: 37.9006s
Batch 800/3907, reward: 2.894, loss: 0.0000, took: 37.9381s
Batch 900/3907, reward: 2.894, loss: -0.0023, took: 35.2960s
Batch 1000/3907, reward: 2.894, loss: 0.0007, took: 37.8782s
Batch 1100/3907, reward: 2.894, loss: 0.0017, took: 37.9658s
Batch 1200/3907, reward: 2.894, loss: 0.0359, took: 37.8063s
Batch 1300/3907, reward: 2.894, loss: 0.0029, took: 38.0584s
Batch 1400/3907, reward: 2.894, loss: -0.0019, took: 38.0906s
Batch 1500/3907, reward: 2.894, loss: 0.0016, took: 37.8720s
Batch 1600/3907, reward: 2.894, loss: 0.0000, took: 37.8849s
Batch 1700/3907, reward: 2.894, loss: -0.0020, took: 38.0394s
Batch 1800/3907, reward: 2.894, loss: -0.0006, took: 38.0321s
Batch 1900/3907, reward: 2.894, loss: 0.0645, took: 38.0827s
Batch 2000/3907, reward: 2.894, loss: 0.2937, took: 37.9880s
Batch 2100/3907, reward: 2.894, loss: -0.0050, took: 37.9223s
Batch 2200/3907, reward: 2.894, loss: -0.0002, took: 37.9397s
Batch 2300/3907, reward: 2.894, loss: 0.0000, took: 37.9936s
Batch 2400/3907, reward: 2.894, loss: -0.0015, took: 37.9366s
Batch 2500/3907, reward: 2.894, loss: 0.0010, took: 38.1260s
Batch 2600/3907, reward: 2.894, loss: 0.0011, took: 37.8132s
Batch 2700/3907, reward: 2.894, loss: -0.0017, took: 38.0189s
Batch 2800/3907, reward: 2.894, loss: 0.0037, took: 38.0712s
Batch 2900/3907, reward: 2.894, loss: -0.0018, took: 37.9670s
Batch 3000/3907, reward: 2.894, loss: -0.0031, took: 37.8212s
Batch 3100/3907, reward: 2.894, loss: 0.0017, took: 37.8793s
Batch 3200/3907, reward: 2.894, loss: 0.0012, took: 37.9656s
Batch 3300/3907, reward: 2.894, loss: -0.0006, took: 35.0869s
Batch 3400/3907, reward: 2.894, loss: -0.0016, took: 38.0873s
Batch 3500/3907, reward: 2.894, loss: 0.0018, took: 37.7858s
Batch 3600/3907, reward: 2.894, loss: -0.0013, took: 37.9829s
Batch 3700/3907, reward: 2.894, loss: -0.0005, took: 38.0172s
Batch 3800/3907, reward: 2.894, loss: 0.0019, took: 38.1021s
Batch 3900/3907, reward: 2.894, loss: -0.0118, took: 37.8284s
Mean epoch loss/reward: 0.0101, 2.8940, 2.8954, took: 1483.7383s (36.8134s / 100 batches)
Batch 0/3907, reward: 2.895, loss: -1.0363, took: 0.5153s
Batch 100/3907, reward: 2.894, loss: 0.0066, took: 38.0041s
Batch 200/3907, reward: 2.894, loss: -0.0002, took: 38.1947s
Batch 300/3907, reward: 2.894, loss: -0.0006, took: 37.8466s
Batch 400/3907, reward: 2.894, loss: 0.0011, took: 34.9747s
Batch 500/3907, reward: 2.894, loss: -0.0011, took: 37.9358s
Batch 600/3907, reward: 2.894, loss: 0.0012, took: 37.9313s
Batch 700/3907, reward: 2.894, loss: -0.0051, took: 37.9965s
Batch 800/3907, reward: 2.894, loss: 0.0000, took: 37.8738s
Batch 900/3907, reward: 2.894, loss: 0.0007, took: 34.9353s
Batch 1000/3907, reward: 2.894, loss: -0.0009, took: 37.8058s
Batch 1100/3907, reward: 2.894, loss: 0.0014, took: 38.0861s
Batch 1200/3907, reward: 2.894, loss: -0.0021, took: 38.2748s
Batch 1300/3907, reward: 2.894, loss: -0.0032, took: 37.8067s
Batch 1400/3907, reward: 2.894, loss: -0.0136, took: 37.9142s
Batch 1500/3907, reward: 2.894, loss: 0.0003, took: 37.9278s
Batch 1600/3907, reward: 2.894, loss: -0.0013, took: 37.9312s
Batch 1700/3907, reward: 2.894, loss: 0.0038, took: 37.9827s
Batch 1800/3907, reward: 2.894, loss: -0.0052, took: 37.9348s
Batch 1900/3907, reward: 2.894, loss: 0.0011, took: 38.0996s
Batch 2000/3907, reward: 2.894, loss: -0.0008, took: 37.9844s
Batch 2100/3907, reward: 2.894, loss: 0.0005, took: 38.1301s
Batch 2200/3907, reward: 2.894, loss: 0.0005, took: 38.0223s
Batch 2300/3907, reward: 2.894, loss: 0.0007, took: 38.0420s
Batch 2400/3907, reward: 2.894, loss: 0.0005, took: 38.0007s
Batch 2500/3907, reward: 2.894, loss: -0.0020, took: 37.9752s
Batch 2600/3907, reward: 2.894, loss: 0.0013, took: 37.9717s
Batch 2700/3907, reward: 2.894, loss: -0.0010, took: 37.8532s
Batch 2800/3907, reward: 2.894, loss: 0.0052, took: 38.0029s
Batch 2900/3907, reward: 2.894, loss: 0.0131, took: 37.9346s
Batch 3000/3907, reward: 2.894, loss: 0.0121, took: 37.9115s
Batch 3100/3907, reward: 2.894, loss: -0.0047, took: 37.7417s
Batch 3200/3907, reward: 2.894, loss: 0.0009, took: 38.1313s
Batch 3300/3907, reward: 2.894, loss: -0.0021, took: 35.2642s
Batch 3400/3907, reward: 2.894, loss: 0.0021, took: 37.8112s
Batch 3500/3907, reward: 2.894, loss: 0.0014, took: 37.8736s
Batch 3600/3907, reward: 2.894, loss: 0.0004, took: 37.8453s
Batch 3700/3907, reward: 2.894, loss: -0.0004, took: 37.9844s
Batch 3800/3907, reward: 2.894, loss: -0.0003, took: 37.7565s
Batch 3900/3907, reward: 2.894, loss: -0.0010, took: 37.9961s
Mean epoch loss/reward: -0.0000, 2.8940, 2.8954, took: 1483.3434s (36.8051s / 100 batches)
Batch 0/3907, reward: 2.895, loss: 0.1188, took: 0.5210s
Batch 100/3907, reward: 2.894, loss: 0.0011, took: 37.9600s
Batch 200/3907, reward: 2.894, loss: 0.0695, took: 37.7672s
Batch 300/3907, reward: 2.894, loss: -0.0186, took: 37.8943s
Batch 400/3907, reward: 2.894, loss: -0.0729, took: 31.4150s
Batch 500/3907, reward: 2.894, loss: 0.0059, took: 28.8259s
Batch 600/3907, reward: 2.894, loss: -0.0003, took: 28.8669s
Batch 700/3907, reward: 2.894, loss: 0.0014, took: 29.0990s
Batch 800/3907, reward: 2.894, loss: 0.0003, took: 33.2911s
Batch 900/3907, reward: 2.894, loss: -0.0012, took: 34.8108s
Batch 1000/3907, reward: 2.894, loss: 0.0003, took: 37.8108s
Batch 1100/3907, reward: 2.894, loss: 0.0022, took: 37.8782s
Batch 1200/3907, reward: 2.894, loss: -0.0026, took: 37.9086s
Batch 1300/3907, reward: 2.894, loss: -0.0009, took: 37.7877s
Batch 1400/3907, reward: 2.894, loss: 0.0016, took: 37.8996s
Batch 1500/3907, reward: 2.894, loss: -0.0009, took: 37.8467s
Batch 1600/3907, reward: 2.894, loss: 0.0004, took: 37.7805s
Batch 1700/3907, reward: 2.894, loss: 0.0000, took: 37.8716s
Batch 1800/3907, reward: 2.894, loss: -0.0020, took: 37.7299s
Batch 1900/3907, reward: 2.894, loss: 0.0015, took: 37.6959s
Batch 2000/3907, reward: 2.894, loss: -0.0002, took: 36.4599s
Batch 2100/3907, reward: 2.894, loss: -0.0007, took: 28.9822s
Batch 2200/3907, reward: 2.894, loss: -0.0284, took: 29.0380s
Batch 2300/3907, reward: 2.894, loss: -0.0509, took: 28.7269s
Batch 2400/3907, reward: 2.894, loss: -0.0112, took: 28.7016s
Batch 2500/3907, reward: 2.894, loss: -0.0005, took: 29.0248s
Batch 2600/3907, reward: 2.894, loss: 0.0015, took: 29.0476s
Batch 2700/3907, reward: 2.894, loss: -0.0000, took: 28.7719s
Batch 2800/3907, reward: 2.894, loss: 0.0006, took: 28.9292s
Batch 2900/3907, reward: 2.894, loss: -0.0005, took: 28.5576s
Batch 3000/3907, reward: 2.894, loss: 0.0009, took: 28.9311s
Batch 3100/3907, reward: 2.894, loss: -0.0015, took: 28.7289s
Batch 3200/3907, reward: 2.894, loss: -0.0006, took: 28.5305s
Batch 3300/3907, reward: 2.894, loss: 0.0016, took: 24.6731s
Batch 3400/3907, reward: 2.894, loss: -0.0017, took: 28.8449s
Batch 3500/3907, reward: 2.894, loss: 0.0005, took: 28.6215s
Batch 3600/3907, reward: 2.894, loss: 0.0003, took: 28.5892s
Batch 3700/3907, reward: 2.894, loss: 0.0006, took: 29.2436s
Batch 3800/3907, reward: 2.894, loss: 0.0001, took: 29.1142s
Batch 3900/3907, reward: 2.894, loss: 0.0025, took: 29.3703s
Mean epoch loss/reward: -0.0026, 2.8940, 2.8954, took: 1270.0413s (31.4887s / 100 batches)
Batch 0/3907, reward: 2.893, loss: -0.4867, took: 0.4152s
Batch 100/3907, reward: 2.894, loss: 0.0005, took: 29.2049s
Batch 200/3907, reward: 2.894, loss: 0.0023, took: 29.2041s
Batch 300/3907, reward: 2.894, loss: -0.0015, took: 28.9976s
Batch 400/3907, reward: 2.894, loss: -0.0010, took: 28.9919s
Batch 500/3907, reward: 2.894, loss: 0.0007, took: 28.7528s
Batch 600/3907, reward: 2.894, loss: -0.0010, took: 28.8865s
Batch 700/3907, reward: 2.894, loss: 0.0008, took: 29.0096s
Batch 800/3907, reward: 2.894, loss: -0.0013, took: 29.0837s
Batch 900/3907, reward: 2.894, loss: -0.0005, took: 23.9135s
Batch 1000/3907, reward: 2.894, loss: -0.0006, took: 20.0182s
Batch 1100/3907, reward: 2.894, loss: 0.0030, took: 19.7210s
Batch 1200/3907, reward: 2.894, loss: -0.0010, took: 20.5265s
Batch 1300/3907, reward: 2.894, loss: -0.0006, took: 19.7665s
Batch 1400/3907, reward: 2.894, loss: -0.0005, took: 19.9552s
Batch 1500/3907, reward: 2.894, loss: 0.0002, took: 27.1518s
Batch 1600/3907, reward: 2.894, loss: -0.0003, took: 28.6851s
Batch 1700/3907, reward: 2.894, loss: 0.0014, took: 28.6069s
Batch 1800/3907, reward: 2.894, loss: -0.0021, took: 28.5887s
Batch 1900/3907, reward: 2.894, loss: 0.0028, took: 28.6303s
Batch 2000/3907, reward: 2.894, loss: -0.0564, took: 28.6307s
Batch 2100/3907, reward: 2.894, loss: -0.0327, took: 28.7994s
Batch 2200/3907, reward: 2.894, loss: 0.0015, took: 28.5625s
Batch 2300/3907, reward: 2.894, loss: 0.0002, took: 28.7080s
Batch 2400/3907, reward: 2.894, loss: -0.0007, took: 28.6961s
Batch 2500/3907, reward: 2.894, loss: -0.0015, took: 28.7091s
Batch 2600/3907, reward: 2.894, loss: 0.0005, took: 28.6769s
Batch 2700/3907, reward: 2.894, loss: 0.0020, took: 24.8274s
Batch 2800/3907, reward: 2.894, loss: -0.0008, took: 19.8285s
Batch 2900/3907, reward: 2.894, loss: -0.0009, took: 19.9119s
Batch 3000/3907, reward: 2.894, loss: -0.0006, took: 19.7692s
Batch 3100/3907, reward: 2.894, loss: -0.0008, took: 20.3080s
Batch 3200/3907, reward: 2.894, loss: 0.0010, took: 20.0432s
Batch 3300/3907, reward: 2.894, loss: -0.0001, took: 15.5655s
Batch 3400/3907, reward: 2.894, loss: 0.0003, took: 11.4360s
Batch 3500/3907, reward: 2.894, loss: -0.0004, took: 11.6917s
Batch 3600/3907, reward: 2.894, loss: 0.0008, took: 11.2659s
Batch 3700/3907, reward: 2.894, loss: -0.0017, took: 11.3106s
Batch 3800/3907, reward: 2.894, loss: -0.0001, took: 11.2364s
Batch 3900/3907, reward: 2.894, loss: 0.0014, took: 11.4328s
Mean epoch loss/reward: -0.0023, 2.8940, 2.8954, took: 916.6581s (22.6880s / 100 batches)
Average tour length for uniform: 7.4763628897655945
Average tour length for shifted: 3.790098773081467
Average tour length for adversary: 2.8939858909027945
