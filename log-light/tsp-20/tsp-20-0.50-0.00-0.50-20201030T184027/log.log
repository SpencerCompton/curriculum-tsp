Current device: cuda
Batch 0/3907, reward: 13.533, loss: -759.9521, took: 0.3473s
Batch 100/3907, reward: 10.371, loss: -19.2539, took: 20.7245s
Batch 200/3907, reward: 5.645, loss: -0.0889, took: 20.6679s
Batch 300/3907, reward: 5.503, loss: 1.0055, took: 20.5369s
Batch 400/3907, reward: 5.407, loss: 1.2754, took: 20.4257s
Batch 500/3907, reward: 5.344, loss: 1.3085, took: 20.5213s
Batch 600/3907, reward: 5.321, loss: 1.6407, took: 20.1652s
Batch 700/3907, reward: 5.294, loss: 1.2494, took: 20.4246s
Batch 800/3907, reward: 5.300, loss: 1.6916, took: 20.3398s
Batch 900/3907, reward: 5.296, loss: 1.5282, took: 20.6093s
Batch 1000/3907, reward: 5.273, loss: 0.4389, took: 20.6383s
Batch 1100/3907, reward: 5.271, loss: 1.8966, took: 20.4274s
Batch 1200/3907, reward: 5.270, loss: 0.4159, took: 20.6767s
Batch 1300/3907, reward: 5.253, loss: 0.0595, took: 20.5119s
Batch 1400/3907, reward: 5.158, loss: 0.3310, took: 20.4601s
Batch 1500/3907, reward: 4.956, loss: 0.2085, took: 20.5210s
Batch 1600/3907, reward: 4.888, loss: 0.8121, took: 20.6013s
Batch 1700/3907, reward: 4.812, loss: -0.4173, took: 20.6232s
Batch 1800/3907, reward: 4.763, loss: -0.0775, took: 20.8564s
Batch 1900/3907, reward: 4.525, loss: 0.0890, took: 20.2084s
Batch 2000/3907, reward: 4.334, loss: -0.3190, took: 20.4842s
Batch 2100/3907, reward: 4.259, loss: -0.2076, took: 20.3588s
Batch 2200/3907, reward: 4.212, loss: -0.2911, took: 20.5014s
Batch 2300/3907, reward: 4.173, loss: 0.1543, took: 20.8439s
Batch 2400/3907, reward: 4.142, loss: 0.2630, took: 21.0647s
Batch 2500/3907, reward: 4.111, loss: 0.0098, took: 29.0703s
Batch 2600/3907, reward: 4.093, loss: -0.2704, took: 29.2474s
Batch 2700/3907, reward: 4.071, loss: 0.2480, took: 29.3076s
Batch 2800/3907, reward: 4.047, loss: 0.0976, took: 29.6549s
Batch 2900/3907, reward: 4.034, loss: -0.0802, took: 29.3044s
Batch 3000/3907, reward: 4.017, loss: -0.3289, took: 29.2466s
Batch 3100/3907, reward: 4.008, loss: -0.1758, took: 30.0498s
Batch 3200/3907, reward: 3.993, loss: -0.2069, took: 38.9032s
Batch 3300/3907, reward: 3.985, loss: -0.1131, took: 38.8687s
Batch 3400/3907, reward: 3.971, loss: 0.2586, took: 36.4093s
Batch 3500/3907, reward: 3.966, loss: -0.1958, took: 38.1782s
Batch 3600/3907, reward: 3.959, loss: 0.0127, took: 38.6795s
Batch 3700/3907, reward: 3.959, loss: 0.0235, took: 38.7124s
Batch 3800/3907, reward: 3.949, loss: -0.1496, took: 38.7573s
Batch 3900/3907, reward: 3.943, loss: -0.0925, took: 38.7633s
Mean epoch loss/reward: -0.3818, 4.7414, 3.8662, took: 1018.5631s (25.1673s / 100 batches)
Batch 0/3907, reward: 3.915, loss: -1.6775, took: 0.5756s
Batch 100/3907, reward: 3.936, loss: -0.0196, took: 38.7077s
Batch 200/3907, reward: 3.936, loss: -0.2480, took: 38.7241s
Batch 300/3907, reward: 3.930, loss: -0.5293, took: 38.6787s
Batch 400/3907, reward: 3.930, loss: 0.0844, took: 38.5360s
Batch 500/3907, reward: 3.920, loss: -0.3170, took: 38.6696s
Batch 600/3907, reward: 3.916, loss: 0.2754, took: 38.6819s
Batch 700/3907, reward: 3.925, loss: 0.0311, took: 38.6352s
Batch 800/3907, reward: 3.912, loss: 0.0750, took: 38.7497s
Batch 900/3907, reward: 3.912, loss: 0.0379, took: 38.7803s
Batch 1000/3907, reward: 3.905, loss: -0.2478, took: 38.9072s
Batch 1100/3907, reward: 3.904, loss: -0.0141, took: 38.8818s
Batch 1200/3907, reward: 3.909, loss: -0.0602, took: 38.7575s
Batch 1300/3907, reward: 3.904, loss: -0.1220, took: 38.7682s
Batch 1400/3907, reward: 3.899, loss: -0.4438, took: 38.8719s
Batch 1500/3907, reward: 3.901, loss: -0.2186, took: 38.6770s
Batch 1600/3907, reward: 3.895, loss: 0.1439, took: 38.6782s
Batch 1700/3907, reward: 3.894, loss: -0.0163, took: 38.5462s
Batch 1800/3907, reward: 3.893, loss: 0.1835, took: 38.5489s
Batch 1900/3907, reward: 3.895, loss: 0.0212, took: 38.6683s
Batch 2000/3907, reward: 3.891, loss: -0.0984, took: 38.5416s
Batch 2100/3907, reward: 3.890, loss: -0.0009, took: 38.5348s
Batch 2200/3907, reward: 3.886, loss: -0.2905, took: 38.6325s
Batch 2300/3907, reward: 3.882, loss: 0.0352, took: 38.5934s
Batch 2400/3907, reward: 3.885, loss: 0.0612, took: 35.5340s
Batch 2500/3907, reward: 3.882, loss: -0.1119, took: 38.5656s
Batch 2600/3907, reward: 3.879, loss: -0.0957, took: 38.5662s
Batch 2700/3907, reward: 3.884, loss: -0.0714, took: 38.9235s
Batch 2800/3907, reward: 3.883, loss: -0.0921, took: 38.7506s
Batch 2900/3907, reward: 3.875, loss: -0.0989, took: 38.7499s
Batch 3000/3907, reward: 3.879, loss: 0.2520, took: 38.7997s
Batch 3100/3907, reward: 3.878, loss: 0.1434, took: 35.7401s
Batch 3200/3907, reward: 3.884, loss: -0.3097, took: 38.7923s
Batch 3300/3907, reward: 3.875, loss: -0.0246, took: 38.7738s
Batch 3400/3907, reward: 3.875, loss: 0.2055, took: 36.5027s
Batch 3500/3907, reward: 3.870, loss: 0.2307, took: 38.0814s
Batch 3600/3907, reward: 3.870, loss: 0.0137, took: 38.8340s
Batch 3700/3907, reward: 3.871, loss: 0.1826, took: 38.7419s
Batch 3800/3907, reward: 3.871, loss: 0.1774, took: 38.7726s
Batch 3900/3907, reward: 3.870, loss: -0.0745, took: 38.8299s
Mean epoch loss/reward: -0.0360, 3.8947, 3.8409, took: 1512.9745s (37.5326s / 100 batches)
Batch 0/3907, reward: 3.848, loss: 4.0966, took: 0.5219s
Batch 100/3907, reward: 3.866, loss: -0.0614, took: 38.8875s
Batch 200/3907, reward: 3.865, loss: 0.0594, took: 38.7144s
Batch 300/3907, reward: 3.873, loss: -0.0438, took: 38.8392s
Batch 400/3907, reward: 3.869, loss: -0.3967, took: 38.7323s
Batch 500/3907, reward: 3.874, loss: -0.0213, took: 38.6496s
Batch 600/3907, reward: 3.862, loss: -0.0370, took: 38.7641s
Batch 700/3907, reward: 3.865, loss: 0.0236, took: 38.8001s
Batch 800/3907, reward: 3.867, loss: -0.1232, took: 38.7792s
Batch 900/3907, reward: 3.868, loss: 0.0501, took: 38.7788s
Batch 1000/3907, reward: 3.854, loss: -0.0634, took: 38.7679s
Batch 1100/3907, reward: 3.862, loss: -0.0693, took: 38.7484s
Batch 1200/3907, reward: 3.858, loss: 0.0243, took: 38.7039s
Batch 1300/3907, reward: 3.857, loss: -0.0365, took: 38.6656s
Batch 1400/3907, reward: 3.862, loss: 0.0205, took: 38.6282s
Batch 1500/3907, reward: 3.862, loss: 0.0525, took: 38.5941s
Batch 1600/3907, reward: 3.855, loss: 0.0713, took: 38.5401s
Batch 1700/3907, reward: 3.855, loss: 0.0602, took: 38.5116s
Batch 1800/3907, reward: 3.851, loss: -0.0683, took: 38.6057s
Batch 1900/3907, reward: 3.857, loss: -0.0260, took: 38.5497s
Batch 2000/3907, reward: 3.855, loss: -0.0359, took: 38.6363s
Batch 2100/3907, reward: 3.853, loss: -0.1027, took: 38.6471s
Batch 2200/3907, reward: 3.855, loss: -0.1438, took: 38.6432s
Batch 2300/3907, reward: 3.852, loss: -0.0021, took: 38.6867s
Batch 2400/3907, reward: 3.858, loss: 0.1017, took: 35.3637s
Batch 2500/3907, reward: 3.848, loss: 0.0365, took: 38.4707s
Batch 2600/3907, reward: 3.851, loss: 0.0121, took: 38.5310s
Batch 2700/3907, reward: 3.847, loss: -0.0693, took: 38.5163s
Batch 2800/3907, reward: 3.848, loss: -0.0191, took: 38.6714s
Batch 2900/3907, reward: 3.854, loss: 0.2597, took: 38.6122s
Batch 3000/3907, reward: 3.850, loss: 0.1417, took: 38.5270s
Batch 3100/3907, reward: 3.849, loss: -0.0689, took: 35.8146s
Batch 3200/3907, reward: 3.852, loss: -0.0334, took: 38.6039s
Batch 3300/3907, reward: 3.851, loss: -0.0506, took: 38.6158s
Batch 3400/3907, reward: 3.852, loss: -0.1057, took: 36.3505s
Batch 3500/3907, reward: 3.850, loss: -0.0466, took: 37.8538s
Batch 3600/3907, reward: 3.849, loss: 0.0255, took: 38.5642s
Batch 3700/3907, reward: 3.850, loss: 0.0531, took: 38.6008s
Batch 3800/3907, reward: 3.844, loss: -0.0243, took: 38.5375s
Batch 3900/3907, reward: 3.849, loss: 0.1336, took: 38.5646s
Mean epoch loss/reward: -0.0119, 3.8564, 3.8266, took: 1510.0013s (37.4648s / 100 batches)
Batch 0/3907, reward: 3.816, loss: -0.3009, took: 0.5090s
Batch 100/3907, reward: 3.850, loss: -0.0385, took: 38.5379s
Batch 200/3907, reward: 3.848, loss: -0.1215, took: 38.4942s
Batch 300/3907, reward: 3.846, loss: -0.2343, took: 38.5464s
Batch 400/3907, reward: 3.846, loss: 0.0057, took: 38.7146s
Batch 500/3907, reward: 3.849, loss: -0.0580, took: 38.4143s
Batch 600/3907, reward: 3.847, loss: -0.0049, took: 38.6572s
Batch 700/3907, reward: 3.845, loss: -0.0236, took: 38.6100s
Batch 800/3907, reward: 3.845, loss: -0.0928, took: 38.4411s
Batch 900/3907, reward: 3.846, loss: -0.0224, took: 38.4180s
Batch 1000/3907, reward: 3.843, loss: -0.0121, took: 38.2449s
Batch 1100/3907, reward: 3.846, loss: 0.0386, took: 38.2683s
Batch 1200/3907, reward: 3.842, loss: -0.1215, took: 38.4291s
Batch 1300/3907, reward: 3.844, loss: 0.0182, took: 38.5075s
Batch 1400/3907, reward: 3.845, loss: -0.0261, took: 38.4518s
Batch 1500/3907, reward: 3.843, loss: 0.0959, took: 38.3886s
Batch 1600/3907, reward: 3.843, loss: 0.0471, took: 38.5430s
Batch 1700/3907, reward: 3.839, loss: -0.0222, took: 38.4562s
Batch 1800/3907, reward: 3.845, loss: 0.0904, took: 38.4745s
Batch 1900/3907, reward: 3.843, loss: 0.0507, took: 38.4229s
Batch 2000/3907, reward: 3.843, loss: -0.0662, took: 38.3296s
Batch 2100/3907, reward: 3.847, loss: 0.0076, took: 38.3449s
Batch 2200/3907, reward: 3.839, loss: -0.0659, took: 38.1422s
Batch 2300/3907, reward: 3.844, loss: -0.0904, took: 38.0610s
Batch 2400/3907, reward: 3.840, loss: 0.0060, took: 35.3225s
Batch 2500/3907, reward: 3.841, loss: -0.0129, took: 38.1551s
Batch 2600/3907, reward: 3.839, loss: 0.0490, took: 38.2237s
Batch 2700/3907, reward: 3.849, loss: -0.0082, took: 38.1112s
Batch 2800/3907, reward: 3.843, loss: -0.1290, took: 38.2127s
Batch 2900/3907, reward: 3.844, loss: 0.0040, took: 38.3533s
Batch 3000/3907, reward: 3.842, loss: -0.0074, took: 38.0048s
Batch 3100/3907, reward: 3.839, loss: 0.0061, took: 35.4899s
Batch 3200/3907, reward: 3.839, loss: -0.0990, took: 38.3258s
Batch 3300/3907, reward: 3.842, loss: 0.0043, took: 38.0014s
Batch 3400/3907, reward: 3.846, loss: -0.0245, took: 35.8587s
Batch 3500/3907, reward: 3.846, loss: -0.0510, took: 37.6404s
Batch 3600/3907, reward: 3.843, loss: 0.0278, took: 38.1612s
Batch 3700/3907, reward: 3.835, loss: 0.0280, took: 38.2447s
Batch 3800/3907, reward: 3.841, loss: -0.0287, took: 38.3243s
Batch 3900/3907, reward: 3.837, loss: -0.0665, took: 38.1549s
Mean epoch loss/reward: -0.0254, 3.8435, 3.8246, took: 1498.7888s (37.1748s / 100 batches)
Batch 0/3907, reward: 3.863, loss: 1.2597, took: 0.5560s
Batch 100/3907, reward: 3.836, loss: -0.0003, took: 38.4715s
Batch 200/3907, reward: 3.839, loss: -0.0145, took: 38.4075s
Batch 300/3907, reward: 3.834, loss: 0.0158, took: 38.2980s
Batch 400/3907, reward: 3.835, loss: -0.0313, took: 38.1598s
Batch 500/3907, reward: 3.840, loss: -0.0133, took: 38.1785s
Batch 600/3907, reward: 3.832, loss: 0.0564, took: 38.2417s
Batch 700/3907, reward: 3.840, loss: 0.0502, took: 37.9840s
Batch 800/3907, reward: 3.832, loss: -0.0995, took: 38.2604s
Batch 900/3907, reward: 3.835, loss: 0.0416, took: 38.3844s
Batch 1000/3907, reward: 3.837, loss: -0.0446, took: 38.3335s
Batch 1100/3907, reward: 3.868, loss: -0.0550, took: 38.2525s
Batch 1200/3907, reward: 3.843, loss: 0.0234, took: 38.3409s
Batch 1300/3907, reward: 3.839, loss: -0.0151, took: 38.1221s
Batch 1400/3907, reward: 3.840, loss: -0.0095, took: 38.3209s
Batch 1500/3907, reward: 3.842, loss: -0.0636, took: 38.2287s
Batch 1600/3907, reward: 3.841, loss: -0.0079, took: 38.0695s
Batch 1700/3907, reward: 3.838, loss: -0.0246, took: 38.2378s
Batch 1800/3907, reward: 3.839, loss: -0.0087, took: 38.0718s
Batch 1900/3907, reward: 3.845, loss: -0.0853, took: 37.9050s
Batch 2000/3907, reward: 3.843, loss: -0.0259, took: 38.1849s
Batch 2100/3907, reward: 3.841, loss: -0.0499, took: 38.1176s
Batch 2200/3907, reward: 3.837, loss: 0.0150, took: 38.0492s
Batch 2300/3907, reward: 3.866, loss: -0.0379, took: 38.1692s
Batch 2400/3907, reward: 3.854, loss: -0.0556, took: 35.4583s
Batch 2500/3907, reward: 3.843, loss: 0.0244, took: 38.2164s
Batch 2600/3907, reward: 3.847, loss: -0.0186, took: 38.2453s
Batch 2700/3907, reward: 3.844, loss: -0.0006, took: 38.4724s
Batch 2800/3907, reward: 3.841, loss: -0.0254, took: 38.3203s
Batch 2900/3907, reward: 3.835, loss: -0.0387, took: 38.2561s
Batch 3000/3907, reward: 3.834, loss: -0.0110, took: 38.4523s
Batch 3100/3907, reward: 3.834, loss: -0.0540, took: 35.7672s
Batch 3200/3907, reward: 3.838, loss: -0.0440, took: 38.3386s
Batch 3300/3907, reward: 3.837, loss: 0.0957, took: 38.1043s
Batch 3400/3907, reward: 3.836, loss: 0.0147, took: 35.7154s
Batch 3500/3907, reward: 3.832, loss: -0.0321, took: 37.6086s
Batch 3600/3907, reward: 3.833, loss: -0.0827, took: 38.2069s
Batch 3700/3907, reward: 3.837, loss: -0.0008, took: 38.0490s
Batch 3800/3907, reward: 3.834, loss: -0.0519, took: 38.1244s
Batch 3900/3907, reward: 3.835, loss: 0.0368, took: 38.2328s
Mean epoch loss/reward: -0.0162, 3.8399, 3.8229, took: 1494.4756s (37.0729s / 100 batches)
Batch 0/3907, reward: 3.865, loss: -0.1057, took: 0.5299s
Batch 100/3907, reward: 3.833, loss: -0.0221, took: 38.1561s
Batch 200/3907, reward: 3.832, loss: -0.0588, took: 38.1305s
Batch 300/3907, reward: 3.834, loss: 0.0387, took: 38.0770s
Batch 400/3907, reward: 3.832, loss: -0.0563, took: 38.1072s
Batch 500/3907, reward: 3.829, loss: -0.0053, took: 38.3071s
Batch 600/3907, reward: 3.837, loss: -0.0315, took: 38.2079s
Batch 700/3907, reward: 3.831, loss: 0.0491, took: 38.3922s
Batch 800/3907, reward: 3.833, loss: 0.0147, took: 38.2148s
Batch 900/3907, reward: 3.836, loss: -0.0604, took: 38.2123s
Batch 1000/3907, reward: 3.832, loss: -0.0295, took: 38.3710s
Batch 1100/3907, reward: 3.830, loss: -0.0998, took: 38.1141s
Batch 1200/3907, reward: 3.834, loss: -0.0400, took: 38.2794s
Batch 1300/3907, reward: 3.832, loss: 0.0034, took: 38.1219s
Batch 1400/3907, reward: 3.834, loss: -0.0207, took: 38.0543s
Batch 1500/3907, reward: 3.829, loss: -0.0316, took: 38.1502s
Batch 1600/3907, reward: 3.831, loss: -0.0394, took: 38.0178s
Batch 1700/3907, reward: 3.834, loss: 0.0101, took: 37.9906s
Batch 1800/3907, reward: 3.835, loss: 0.0486, took: 38.0409s
Batch 1900/3907, reward: 3.829, loss: 0.0112, took: 38.0808s
Batch 2000/3907, reward: 3.833, loss: 0.0161, took: 38.1162s
Batch 2100/3907, reward: 3.832, loss: -0.0254, took: 38.2694s
Batch 2200/3907, reward: 3.829, loss: -0.0269, took: 38.1976s
Batch 2300/3907, reward: 3.831, loss: -0.0389, took: 38.2040s
Batch 2400/3907, reward: 3.828, loss: -0.0419, took: 35.3570s
Batch 2500/3907, reward: 3.831, loss: -0.0536, took: 38.0538s
Batch 2600/3907, reward: 3.836, loss: -0.0053, took: 38.2328s
Batch 2700/3907, reward: 3.831, loss: -0.0535, took: 38.1945s
Batch 2800/3907, reward: 3.829, loss: -0.0423, took: 38.0840s
Batch 2900/3907, reward: 3.827, loss: 0.0061, took: 38.2414s
Batch 3000/3907, reward: 3.834, loss: -0.0735, took: 37.9913s
Batch 3100/3907, reward: 3.835, loss: -0.0355, took: 35.3488s
Batch 3200/3907, reward: 3.832, loss: -0.0226, took: 38.2209s
Batch 3300/3907, reward: 3.829, loss: 0.0165, took: 38.0815s
Batch 3400/3907, reward: 3.830, loss: 0.0087, took: 35.3960s
Batch 3500/3907, reward: 3.840, loss: 0.0075, took: 38.1686s
Batch 3600/3907, reward: 3.842, loss: -0.0769, took: 38.1660s
Batch 3700/3907, reward: 3.836, loss: -0.0078, took: 38.0918s
Batch 3800/3907, reward: 3.830, loss: -0.0623, took: 38.0325s
Batch 3900/3907, reward: 3.833, loss: 0.0065, took: 38.1440s
Mean epoch loss/reward: -0.0213, 3.8324, 3.8195, took: 1491.8532s (37.0037s / 100 batches)
Batch 0/3907, reward: 3.827, loss: 0.3869, took: 0.5331s
Batch 100/3907, reward: 3.827, loss: -0.0404, took: 38.1863s
Batch 200/3907, reward: 3.832, loss: 0.0600, took: 38.1259s
Batch 300/3907, reward: 3.829, loss: 0.0187, took: 38.1676s
Batch 400/3907, reward: 3.831, loss: -0.0284, took: 38.0672s
Batch 500/3907, reward: 3.826, loss: 0.0033, took: 38.0123s
Batch 600/3907, reward: 3.827, loss: -0.0441, took: 38.1835s
Batch 700/3907, reward: 3.830, loss: -0.0107, took: 38.0785s
Batch 800/3907, reward: 3.825, loss: 0.0014, took: 38.1860s
Batch 900/3907, reward: 3.828, loss: -0.0570, took: 38.1453s
Batch 1000/3907, reward: 3.830, loss: -0.0456, took: 37.9881s
Batch 1100/3907, reward: 3.830, loss: -0.0977, took: 37.9836s
Batch 1200/3907, reward: 3.834, loss: 0.0004, took: 38.3150s
Batch 1300/3907, reward: 3.830, loss: -0.0471, took: 38.2960s
Batch 1400/3907, reward: 3.830, loss: -0.0110, took: 38.1684s
Batch 1500/3907, reward: 3.823, loss: -0.0225, took: 38.3602s
Batch 1600/3907, reward: 3.828, loss: -0.0073, took: 38.2060s
Batch 1700/3907, reward: 3.827, loss: -0.0369, took: 38.1855s
Batch 1800/3907, reward: 3.829, loss: 0.0013, took: 38.2297s
Batch 1900/3907, reward: 3.830, loss: -0.0214, took: 38.1966s
Batch 2000/3907, reward: 3.828, loss: -0.0284, took: 38.2733s
Batch 2100/3907, reward: 3.832, loss: -0.0852, took: 38.2020s
Batch 2200/3907, reward: 3.830, loss: 0.0105, took: 38.2017s
Batch 2300/3907, reward: 3.831, loss: -0.0352, took: 38.4758s
Batch 2400/3907, reward: 3.835, loss: -0.0606, took: 34.9991s
Batch 2500/3907, reward: 3.828, loss: 0.0062, took: 38.2193s
Batch 2600/3907, reward: 3.828, loss: -0.0207, took: 37.9891s
Batch 2700/3907, reward: 3.826, loss: -0.0028, took: 38.0625s
Batch 2800/3907, reward: 3.846, loss: 0.0377, took: 38.2228s
Batch 2900/3907, reward: 3.848, loss: -0.0784, took: 37.9970s
Batch 3000/3907, reward: 3.836, loss: -0.0043, took: 38.0401s
Batch 3100/3907, reward: 3.828, loss: -0.0527, took: 35.4019s
Batch 3200/3907, reward: 3.826, loss: 0.0081, took: 37.9974s
Batch 3300/3907, reward: 3.825, loss: -0.0157, took: 38.0139s
Batch 3400/3907, reward: 3.825, loss: -0.0697, took: 35.5786s
Batch 3500/3907, reward: 3.830, loss: 0.0289, took: 37.8103s
Batch 3600/3907, reward: 3.830, loss: -0.0231, took: 38.1280s
Batch 3700/3907, reward: 3.833, loss: 0.0285, took: 37.9909s
Batch 3800/3907, reward: 3.836, loss: -0.0438, took: 38.1437s
Batch 3900/3907, reward: 3.832, loss: 0.0200, took: 38.1847s
Mean epoch loss/reward: -0.0193, 3.8303, 3.8243, took: 1491.1025s (36.9887s / 100 batches)
Batch 0/3907, reward: 3.856, loss: 0.0916, took: 0.5656s
Batch 100/3907, reward: 3.829, loss: -0.0167, took: 38.2310s
Batch 200/3907, reward: 3.831, loss: 0.0570, took: 38.1218s
Batch 300/3907, reward: 3.831, loss: -0.0544, took: 38.0034s
Batch 400/3907, reward: 3.832, loss: -0.0081, took: 37.8641s
Batch 500/3907, reward: 3.833, loss: -0.0303, took: 38.1252s
Batch 600/3907, reward: 3.831, loss: -0.0581, took: 37.9802s
Batch 700/3907, reward: 3.827, loss: -0.0279, took: 38.0269s
Batch 800/3907, reward: 3.829, loss: -0.0174, took: 38.0875s
Batch 900/3907, reward: 3.828, loss: 0.0310, took: 37.9954s
Batch 1000/3907, reward: 3.825, loss: -0.0193, took: 38.2325s
Batch 1100/3907, reward: 3.822, loss: -0.0619, took: 37.9250s
Batch 1200/3907, reward: 3.826, loss: -0.0591, took: 38.0355s
Batch 1300/3907, reward: 3.826, loss: -0.0374, took: 38.0511s
Batch 1400/3907, reward: 3.821, loss: 0.0218, took: 38.3039s
Batch 1500/3907, reward: 3.828, loss: 0.0032, took: 38.4193s
Batch 1600/3907, reward: 3.825, loss: -0.0655, took: 38.0602s
Batch 1700/3907, reward: 3.826, loss: -0.0363, took: 38.2795s
Batch 1800/3907, reward: 3.827, loss: 0.0108, took: 38.0427s
Batch 1900/3907, reward: 3.828, loss: -0.0038, took: 38.1423s
Batch 2000/3907, reward: 3.827, loss: -0.0134, took: 38.0719s
Batch 2100/3907, reward: 3.824, loss: -0.0128, took: 38.0512s
Batch 2200/3907, reward: 3.826, loss: -0.0184, took: 37.9606s
Batch 2300/3907, reward: 3.830, loss: 0.0111, took: 38.2109s
Batch 2400/3907, reward: 3.822, loss: -0.0708, took: 35.4012s
Batch 2500/3907, reward: 3.829, loss: 0.0043, took: 37.9363s
Batch 2600/3907, reward: 3.824, loss: -0.0141, took: 38.0410s
Batch 2700/3907, reward: 3.824, loss: -0.0542, took: 37.9008s
Batch 2800/3907, reward: 3.822, loss: 0.0107, took: 38.1198s
Batch 2900/3907, reward: 3.822, loss: -0.0538, took: 38.2304s
Batch 3000/3907, reward: 3.827, loss: 0.0391, took: 38.0951s
Batch 3100/3907, reward: 3.827, loss: -0.0213, took: 35.3848s
Batch 3200/3907, reward: 3.825, loss: -0.0198, took: 38.4322s
Batch 3300/3907, reward: 3.822, loss: -0.0528, took: 38.0924s
Batch 3400/3907, reward: 3.821, loss: -0.0281, took: 35.8051s
Batch 3500/3907, reward: 3.826, loss: -0.0349, took: 37.6417s
Batch 3600/3907, reward: 3.826, loss: -0.0673, took: 38.0205s
Batch 3700/3907, reward: 3.824, loss: 0.0346, took: 38.2433s
Batch 3800/3907, reward: 3.828, loss: 0.0019, took: 38.1281s
Batch 3900/3907, reward: 3.839, loss: -0.0231, took: 38.1439s
Mean epoch loss/reward: -0.0193, 3.8267, 3.8252, took: 1490.3495s (36.9601s / 100 batches)
Batch 0/3907, reward: 3.823, loss: 0.3811, took: 0.5939s
Batch 100/3907, reward: 3.829, loss: -0.0286, took: 38.4399s
Batch 200/3907, reward: 3.833, loss: -0.0346, took: 38.0294s
Batch 300/3907, reward: 3.835, loss: 0.0182, took: 38.2321s
Batch 400/3907, reward: 3.848, loss: -0.0621, took: 37.9708s
Batch 500/3907, reward: 3.843, loss: 0.0386, took: 37.9992s
Batch 600/3907, reward: 3.843, loss: -0.0335, took: 38.0943s
Batch 700/3907, reward: 3.835, loss: -0.0726, took: 38.1318s
Batch 800/3907, reward: 3.834, loss: 0.0041, took: 38.2909s
Batch 900/3907, reward: 3.831, loss: 0.0259, took: 38.2360s
Batch 1000/3907, reward: 3.840, loss: -0.0144, took: 38.1536s
Batch 1100/3907, reward: 3.832, loss: -0.0093, took: 38.1614s
Batch 1200/3907, reward: 3.840, loss: 0.0213, took: 38.2097s
Batch 1300/3907, reward: 3.829, loss: -0.0288, took: 37.9957s
Batch 1400/3907, reward: 3.820, loss: 0.0021, took: 38.1601s
Batch 1500/3907, reward: 3.823, loss: 0.0183, took: 38.2006s
Batch 1600/3907, reward: 3.828, loss: 0.0047, took: 37.8848s
Batch 1700/3907, reward: 3.824, loss: -0.0239, took: 38.1595s
Batch 1800/3907, reward: 3.828, loss: -0.0189, took: 37.8709s
Batch 1900/3907, reward: 3.819, loss: -0.0343, took: 38.1311s
Batch 2000/3907, reward: 3.823, loss: 0.0323, took: 38.1853s
Batch 2100/3907, reward: 3.824, loss: -0.0358, took: 37.9480s
Batch 2200/3907, reward: 3.823, loss: 0.0178, took: 38.1390s
Batch 2300/3907, reward: 3.826, loss: -0.0251, took: 38.0926s
Batch 2400/3907, reward: 3.822, loss: 0.0019, took: 34.9819s
Batch 2500/3907, reward: 3.825, loss: -0.0370, took: 38.0874s
Batch 2600/3907, reward: 3.823, loss: -0.0804, took: 37.8647s
Batch 2700/3907, reward: 3.824, loss: -0.0189, took: 38.0116s
Batch 2800/3907, reward: 3.824, loss: -0.0266, took: 38.1688s
Batch 2900/3907, reward: 3.831, loss: -0.0355, took: 38.2875s
Batch 3000/3907, reward: 3.833, loss: 0.0807, took: 38.1445s
Batch 3100/3907, reward: 3.827, loss: -0.0047, took: 35.5507s
Batch 3200/3907, reward: 3.821, loss: -0.0096, took: 38.2421s
Batch 3300/3907, reward: 3.829, loss: 0.0112, took: 38.3298s
Batch 3400/3907, reward: 3.821, loss: -0.0130, took: 35.6198s
Batch 3500/3907, reward: 3.822, loss: -0.0177, took: 37.9497s
Batch 3600/3907, reward: 3.827, loss: 0.0017, took: 38.2255s
Batch 3700/3907, reward: 3.821, loss: -0.0800, took: 38.2787s
Batch 3800/3907, reward: 3.825, loss: -0.0275, took: 37.9750s
Batch 3900/3907, reward: 3.818, loss: -0.0576, took: 38.1422s
Mean epoch loss/reward: -0.0141, 3.8283, 3.8222, took: 1490.7147s (36.9793s / 100 batches)
Batch 0/3907, reward: 3.812, loss: 0.2598, took: 0.5696s
Batch 100/3907, reward: 3.823, loss: -0.0068, took: 38.3042s
Batch 200/3907, reward: 3.820, loss: 0.0346, took: 38.0943s
Batch 300/3907, reward: 3.826, loss: -0.0123, took: 38.0609s
Batch 400/3907, reward: 3.822, loss: -0.0004, took: 38.1504s
Batch 500/3907, reward: 3.826, loss: 0.0002, took: 38.1851s
Batch 600/3907, reward: 3.823, loss: -0.0211, took: 37.9148s
Batch 700/3907, reward: 3.821, loss: 0.0242, took: 38.1683s
Batch 800/3907, reward: 3.826, loss: -0.0359, took: 38.0571s
Batch 900/3907, reward: 3.820, loss: 0.0136, took: 38.2764s
Batch 1000/3907, reward: 3.822, loss: -0.0371, took: 38.2514s
Batch 1100/3907, reward: 3.830, loss: -0.0215, took: 38.1497s
Batch 1200/3907, reward: 3.820, loss: -0.0332, took: 38.0473s
Batch 1300/3907, reward: 3.828, loss: -0.0270, took: 37.9605s
Batch 1400/3907, reward: 3.823, loss: -0.0235, took: 38.1532s
Batch 1500/3907, reward: 3.818, loss: 0.0116, took: 38.0307s
Batch 1600/3907, reward: 3.817, loss: -0.0025, took: 38.1212s
Batch 1700/3907, reward: 3.822, loss: -0.0146, took: 38.0231s
Batch 1800/3907, reward: 3.822, loss: 0.0194, took: 38.0346s
Batch 1900/3907, reward: 3.827, loss: -0.0028, took: 37.9139s
Batch 2000/3907, reward: 3.819, loss: -0.0123, took: 37.9343s
Batch 2100/3907, reward: 3.825, loss: -0.0127, took: 38.0690s
Batch 2200/3907, reward: 3.819, loss: 0.0027, took: 37.9603s
Batch 2300/3907, reward: 3.823, loss: 0.0005, took: 37.9854s
Batch 2400/3907, reward: 3.817, loss: -0.0031, took: 35.2288s
Batch 2500/3907, reward: 3.825, loss: -0.0191, took: 38.1686s
Batch 2600/3907, reward: 3.820, loss: -0.0032, took: 38.2338s
Batch 2700/3907, reward: 3.822, loss: -0.0419, took: 38.1968s
Batch 2800/3907, reward: 3.822, loss: -0.0268, took: 38.0748s
Batch 2900/3907, reward: 3.822, loss: 0.0087, took: 38.2555s
Batch 3000/3907, reward: 3.824, loss: -0.0271, took: 38.3083s
Batch 3100/3907, reward: 3.820, loss: 0.0176, took: 35.3030s
Batch 3200/3907, reward: 3.821, loss: 0.0001, took: 37.9468s
Batch 3300/3907, reward: 3.820, loss: -0.0390, took: 37.9907s
Batch 3400/3907, reward: 3.826, loss: -0.0389, took: 35.4113s
Batch 3500/3907, reward: 3.820, loss: -0.0251, took: 37.5021s
Batch 3600/3907, reward: 3.822, loss: -0.0265, took: 37.9854s
Batch 3700/3907, reward: 3.824, loss: 0.0059, took: 38.1638s
Batch 3800/3907, reward: 3.817, loss: 0.0131, took: 37.9843s
Batch 3900/3907, reward: 3.824, loss: -0.0174, took: 38.0899s
Mean epoch loss/reward: -0.0095, 3.8222, 3.8190, took: 1488.7864s (36.9315s / 100 batches)
Batch 0/3907, reward: 3.799, loss: 0.3055, took: 0.5214s
Batch 100/3907, reward: 3.819, loss: -0.0163, took: 38.0416s
Batch 200/3907, reward: 3.818, loss: 0.0057, took: 38.0248s
Batch 300/3907, reward: 3.821, loss: -0.0276, took: 38.1393s
Batch 400/3907, reward: 3.819, loss: -0.0085, took: 37.9944s
Batch 500/3907, reward: 3.819, loss: -0.0102, took: 38.3574s
Batch 600/3907, reward: 3.819, loss: -0.0676, took: 38.1223s
Batch 700/3907, reward: 3.822, loss: 0.0198, took: 38.3025s
Batch 800/3907, reward: 3.816, loss: -0.0228, took: 38.3170s
Batch 900/3907, reward: 3.822, loss: -0.0387, took: 38.2727s
Batch 1000/3907, reward: 3.822, loss: -0.0410, took: 38.1369s
Batch 1100/3907, reward: 3.818, loss: -0.0059, took: 38.1140s
Batch 1200/3907, reward: 3.825, loss: -0.0176, took: 38.2029s
Batch 1300/3907, reward: 3.823, loss: -0.0048, took: 38.1131s
Batch 1400/3907, reward: 3.823, loss: -0.0211, took: 38.0807s
Batch 1500/3907, reward: 3.820, loss: -0.0271, took: 38.0647s
Batch 1600/3907, reward: 3.818, loss: 0.0145, took: 38.1047s
Batch 1700/3907, reward: 3.819, loss: -0.0171, took: 37.9942s
Batch 1800/3907, reward: 3.819, loss: 0.0004, took: 37.9891s
Batch 1900/3907, reward: 3.822, loss: -0.0158, took: 37.8958s
Batch 2000/3907, reward: 3.827, loss: -0.0373, took: 37.9433s
Batch 2100/3907, reward: 3.824, loss: 0.0003, took: 37.9080s
Batch 2200/3907, reward: 3.816, loss: -0.0062, took: 37.9809s
Batch 2300/3907, reward: 3.821, loss: -0.0131, took: 38.0479s
Batch 2400/3907, reward: 3.821, loss: -0.0483, took: 35.0561s
Batch 2500/3907, reward: 3.821, loss: -0.0342, took: 38.0481s
Batch 2600/3907, reward: 3.817, loss: -0.0391, took: 38.0116s
Batch 2700/3907, reward: 3.819, loss: -0.0388, took: 38.2828s
Batch 2800/3907, reward: 3.817, loss: 0.0099, took: 38.3390s
Batch 2900/3907, reward: 3.822, loss: -0.0029, took: 38.3721s
Batch 3000/3907, reward: 3.817, loss: -0.0636, took: 38.1018s
Batch 3100/3907, reward: 3.821, loss: -0.0257, took: 35.5650s
Batch 3200/3907, reward: 3.819, loss: -0.0092, took: 38.0635s
Batch 3300/3907, reward: 3.818, loss: -0.0433, took: 38.0567s
Batch 3400/3907, reward: 3.818, loss: -0.0579, took: 35.6680s
Batch 3500/3907, reward: 3.819, loss: -0.0035, took: 37.2837s
Batch 3600/3907, reward: 3.819, loss: -0.0260, took: 38.0862s
Batch 3700/3907, reward: 3.820, loss: 0.0224, took: 37.9904s
Batch 3800/3907, reward: 3.823, loss: -0.0065, took: 38.0512s
Batch 3900/3907, reward: 3.818, loss: -0.0352, took: 38.0344s
Mean epoch loss/reward: -0.0191, 3.8200, 3.8201, took: 1489.0840s (36.9420s / 100 batches)
Batch 0/3907, reward: 3.769, loss: -0.2330, took: 0.5180s
Batch 100/3907, reward: 3.822, loss: -0.0115, took: 38.2626s
Batch 200/3907, reward: 3.824, loss: -0.0197, took: 38.0737s
Batch 300/3907, reward: 3.819, loss: -0.0111, took: 37.9498s
Batch 400/3907, reward: 3.815, loss: -0.0032, took: 37.9655s
Batch 500/3907, reward: 3.823, loss: -0.0146, took: 37.9808s
Batch 600/3907, reward: 3.816, loss: 0.0240, took: 37.9158s
Batch 700/3907, reward: 3.820, loss: -0.0047, took: 38.2301s
Batch 800/3907, reward: 3.818, loss: -0.0183, took: 37.9685s
Batch 900/3907, reward: 3.818, loss: -0.0240, took: 38.1378s
Batch 1000/3907, reward: 3.822, loss: -0.0786, took: 38.0310s
Batch 1100/3907, reward: 3.819, loss: -0.0226, took: 37.9151s
Batch 1200/3907, reward: 3.817, loss: -0.0250, took: 38.0776s
Batch 1300/3907, reward: 3.817, loss: 0.0096, took: 37.9936s
Batch 1400/3907, reward: 3.819, loss: 0.0281, took: 37.9825s
Batch 1500/3907, reward: 3.822, loss: -0.0153, took: 38.0558s
Batch 1600/3907, reward: 3.820, loss: -0.0066, took: 37.9802s
Batch 1700/3907, reward: 3.822, loss: -0.0033, took: 37.8867s
Batch 1800/3907, reward: 3.816, loss: 0.0186, took: 37.9887s
Batch 1900/3907, reward: 3.818, loss: -0.0185, took: 38.1039s
Batch 2000/3907, reward: 3.819, loss: 0.0353, took: 37.9865s
Batch 2100/3907, reward: 3.820, loss: -0.0035, took: 38.0436s
Batch 2200/3907, reward: 3.819, loss: -0.0125, took: 37.8775s
Batch 2300/3907, reward: 3.814, loss: -0.0221, took: 37.9336s
Batch 2400/3907, reward: 3.816, loss: 0.0051, took: 35.0440s
Batch 2500/3907, reward: 3.820, loss: 0.0252, took: 38.1668s
Batch 2600/3907, reward: 3.821, loss: -0.0455, took: 37.8616s
Batch 2700/3907, reward: 3.822, loss: -0.0193, took: 38.0948s
Batch 2800/3907, reward: 3.822, loss: -0.0429, took: 38.0550s
Batch 2900/3907, reward: 3.820, loss: 0.0394, took: 38.1776s
Batch 3000/3907, reward: 3.817, loss: -0.0340, took: 37.9543s
Batch 3100/3907, reward: 3.819, loss: -0.0080, took: 35.4556s
Batch 3200/3907, reward: 3.822, loss: -0.0463, took: 37.9678s
Batch 3300/3907, reward: 3.825, loss: -0.0270, took: 37.8810s
Batch 3400/3907, reward: 3.818, loss: -0.0001, took: 35.6739s
Batch 3500/3907, reward: 3.823, loss: -0.0187, took: 37.6845s
Batch 3600/3907, reward: 3.822, loss: -0.0347, took: 37.9550s
Batch 3700/3907, reward: 3.818, loss: -0.0123, took: 38.0131s
Batch 3800/3907, reward: 3.818, loss: -0.0088, took: 37.9323s
Batch 3900/3907, reward: 3.817, loss: -0.0208, took: 37.9292s
Mean epoch loss/reward: -0.0114, 3.8194, 3.8160, took: 1486.2495s (36.8676s / 100 batches)
Batch 0/3907, reward: 3.812, loss: -0.0049, took: 0.5151s
Batch 100/3907, reward: 3.820, loss: -0.0034, took: 38.0403s
Batch 200/3907, reward: 3.820, loss: 0.0373, took: 38.0229s
Batch 300/3907, reward: 3.817, loss: 0.0068, took: 38.1544s
Batch 400/3907, reward: 3.819, loss: 0.0274, took: 37.9664s
Batch 500/3907, reward: 3.821, loss: -0.0116, took: 38.1549s
Batch 600/3907, reward: 3.819, loss: 0.0477, took: 38.0855s
Batch 700/3907, reward: 3.818, loss: -0.0094, took: 38.0249s
Batch 800/3907, reward: 3.818, loss: 0.0080, took: 38.0245s
Batch 900/3907, reward: 3.821, loss: -0.0234, took: 38.0066s
Batch 1000/3907, reward: 3.814, loss: 0.0116, took: 37.9584s
Batch 1100/3907, reward: 3.819, loss: -0.0113, took: 37.9348s
Batch 1200/3907, reward: 3.820, loss: -0.0041, took: 37.9457s
Batch 1300/3907, reward: 3.819, loss: 0.0071, took: 37.9037s
Batch 1400/3907, reward: 3.818, loss: -0.0255, took: 38.0497s
Batch 1500/3907, reward: 3.818, loss: -0.0354, took: 37.8658s
Batch 1600/3907, reward: 3.821, loss: -0.0231, took: 37.9571s
Batch 1700/3907, reward: 3.819, loss: -0.0188, took: 37.9537s
Batch 1800/3907, reward: 3.818, loss: 0.0220, took: 37.9735s
Batch 1900/3907, reward: 3.818, loss: -0.0061, took: 37.9495s
Batch 2000/3907, reward: 3.819, loss: -0.0109, took: 38.0072s
Batch 2100/3907, reward: 3.818, loss: -0.0568, took: 38.0157s
Batch 2200/3907, reward: 3.820, loss: -0.0207, took: 37.9458s
Batch 2300/3907, reward: 3.818, loss: 0.0351, took: 38.0585s
Batch 2400/3907, reward: 3.817, loss: -0.0037, took: 34.8313s
Batch 2500/3907, reward: 3.814, loss: -0.0147, took: 37.9750s
Batch 2600/3907, reward: 3.821, loss: -0.0165, took: 37.9514s
Batch 2700/3907, reward: 3.819, loss: -0.0036, took: 37.9903s
Batch 2800/3907, reward: 3.819, loss: 0.0078, took: 37.9555s
Batch 2900/3907, reward: 3.817, loss: 0.0269, took: 38.0254s
Batch 3000/3907, reward: 3.817, loss: -0.0172, took: 37.9451s
Batch 3100/3907, reward: 3.823, loss: -0.0341, took: 35.3613s
Batch 3200/3907, reward: 3.818, loss: -0.0346, took: 37.8112s
Batch 3300/3907, reward: 3.818, loss: 0.0092, took: 37.9728s
Batch 3400/3907, reward: 3.817, loss: -0.0070, took: 35.6568s
Batch 3500/3907, reward: 3.820, loss: -0.0274, took: 37.3052s
Batch 3600/3907, reward: 3.815, loss: 0.0248, took: 37.9397s
Batch 3700/3907, reward: 3.817, loss: 0.0085, took: 37.9767s
Batch 3800/3907, reward: 3.818, loss: -0.0005, took: 37.9534s
Batch 3900/3907, reward: 3.816, loss: -0.0336, took: 37.9657s
Mean epoch loss/reward: -0.0046, 3.8184, 3.8160, took: 1484.7857s (36.8283s / 100 batches)
Batch 0/3907, reward: 3.841, loss: 0.4353, took: 0.5170s
Batch 100/3907, reward: 3.819, loss: -0.0298, took: 38.0655s
Batch 200/3907, reward: 3.813, loss: 0.0142, took: 38.0240s
Batch 300/3907, reward: 3.822, loss: 0.0213, took: 37.9951s
Batch 400/3907, reward: 3.819, loss: 0.0213, took: 38.1110s
Batch 500/3907, reward: 3.820, loss: -0.0133, took: 37.9474s
Batch 600/3907, reward: 3.817, loss: -0.0326, took: 38.0644s
Batch 700/3907, reward: 3.819, loss: -0.0036, took: 38.1223s
Batch 800/3907, reward: 3.819, loss: -0.0173, took: 38.0816s
Batch 900/3907, reward: 3.818, loss: -0.0077, took: 37.9095s
Batch 1000/3907, reward: 3.812, loss: -0.0107, took: 38.1221s
Batch 1100/3907, reward: 3.818, loss: -0.0262, took: 37.9479s
Batch 1200/3907, reward: 3.816, loss: -0.0230, took: 37.9849s
Batch 1300/3907, reward: 3.822, loss: -0.0151, took: 37.8857s
Batch 1400/3907, reward: 3.824, loss: -0.0575, took: 37.8890s
Batch 1500/3907, reward: 3.818, loss: -0.0057, took: 37.9410s
Batch 1600/3907, reward: 3.821, loss: -0.0117, took: 37.8900s
Batch 1700/3907, reward: 3.814, loss: -0.0067, took: 37.8180s
Batch 1800/3907, reward: 3.818, loss: -0.0322, took: 38.0388s
Batch 1900/3907, reward: 3.818, loss: -0.0240, took: 37.8708s
Batch 2000/3907, reward: 3.816, loss: -0.0253, took: 37.8700s
Batch 2100/3907, reward: 3.819, loss: -0.0273, took: 37.9681s
Batch 2200/3907, reward: 3.818, loss: -0.0361, took: 37.9737s
Batch 2300/3907, reward: 3.821, loss: -0.0046, took: 38.0101s
Batch 2400/3907, reward: 3.818, loss: -0.0236, took: 35.0367s
Batch 2500/3907, reward: 3.815, loss: -0.0194, took: 37.8531s
Batch 2600/3907, reward: 3.815, loss: -0.0091, took: 37.9247s
Batch 2700/3907, reward: 3.818, loss: -0.0185, took: 37.9778s
Batch 2800/3907, reward: 3.816, loss: -0.0328, took: 37.8935s
Batch 2900/3907, reward: 3.821, loss: -0.0091, took: 38.0719s
Batch 3000/3907, reward: 3.816, loss: -0.0265, took: 37.9462s
Batch 3100/3907, reward: 3.818, loss: -0.0003, took: 35.4833s
Batch 3200/3907, reward: 3.815, loss: 0.0039, took: 37.9628s
Batch 3300/3907, reward: 3.816, loss: -0.0179, took: 38.1289s
Batch 3400/3907, reward: 3.816, loss: -0.0111, took: 35.5509s
Batch 3500/3907, reward: 3.811, loss: 0.0022, took: 37.7175s
Batch 3600/3907, reward: 3.818, loss: 0.0146, took: 38.1232s
Batch 3700/3907, reward: 3.819, loss: -0.0107, took: 38.0450s
Batch 3800/3907, reward: 3.816, loss: 0.0042, took: 37.8692s
Batch 3900/3907, reward: 3.816, loss: -0.0165, took: 38.0849s
Mean epoch loss/reward: -0.0135, 3.8175, 3.8181, took: 1485.2470s (36.8429s / 100 batches)
Batch 0/3907, reward: 3.815, loss: 0.2632, took: 0.5306s
Batch 100/3907, reward: 3.816, loss: -0.0331, took: 38.0872s
Batch 200/3907, reward: 3.815, loss: -0.0027, took: 37.8679s
Batch 300/3907, reward: 3.821, loss: -0.0218, took: 37.9310s
Batch 400/3907, reward: 3.814, loss: -0.0344, took: 37.8141s
Batch 500/3907, reward: 3.815, loss: 0.0052, took: 38.1229s
Batch 600/3907, reward: 3.817, loss: 0.0214, took: 37.8608s
Batch 700/3907, reward: 3.816, loss: -0.0496, took: 38.2414s
Batch 800/3907, reward: 3.813, loss: 0.0153, took: 38.1396s
Batch 900/3907, reward: 3.822, loss: 0.0062, took: 38.1545s
Batch 1000/3907, reward: 3.815, loss: 0.0020, took: 38.0623s
Batch 1100/3907, reward: 3.819, loss: 0.0026, took: 38.1250s
Batch 1200/3907, reward: 3.813, loss: 0.0034, took: 38.1367s
Batch 1300/3907, reward: 3.819, loss: -0.0082, took: 38.0207s
Batch 1400/3907, reward: 3.814, loss: -0.0050, took: 38.0408s
Batch 1500/3907, reward: 3.812, loss: -0.0153, took: 37.8738s
Batch 1600/3907, reward: 3.814, loss: -0.0117, took: 37.8984s
Batch 1700/3907, reward: 3.813, loss: -0.0112, took: 38.0031s
Batch 1800/3907, reward: 3.816, loss: -0.0024, took: 37.8326s
Batch 1900/3907, reward: 3.818, loss: -0.0047, took: 37.9679s
Batch 2000/3907, reward: 3.817, loss: 0.0239, took: 37.9585s
Batch 2100/3907, reward: 3.819, loss: -0.0189, took: 37.9772s
Batch 2200/3907, reward: 3.819, loss: -0.0016, took: 37.9251s
Batch 2300/3907, reward: 3.820, loss: 0.0223, took: 38.1505s
Batch 2400/3907, reward: 3.817, loss: 0.0051, took: 35.0270s
Batch 2500/3907, reward: 3.814, loss: -0.0166, took: 38.1083s
Batch 2600/3907, reward: 3.816, loss: -0.0321, took: 38.0228s
Batch 2700/3907, reward: 3.815, loss: 0.0122, took: 38.0741s
Batch 2800/3907, reward: 3.811, loss: -0.0037, took: 38.1812s
Batch 2900/3907, reward: 3.809, loss: -0.0184, took: 38.2066s
Batch 3000/3907, reward: 3.822, loss: -0.0404, took: 38.0513s
Batch 3100/3907, reward: 3.816, loss: -0.0464, took: 35.2517s
Batch 3200/3907, reward: 3.817, loss: -0.0198, took: 37.9589s
Batch 3300/3907, reward: 3.818, loss: -0.0340, took: 37.9769s
Batch 3400/3907, reward: 3.813, loss: -0.0064, took: 35.5945s
Batch 3500/3907, reward: 3.817, loss: -0.0121, took: 37.1987s
Batch 3600/3907, reward: 3.816, loss: 0.0025, took: 37.9672s
Batch 3700/3907, reward: 3.814, loss: 0.0125, took: 37.9416s
Batch 3800/3907, reward: 3.821, loss: 0.0067, took: 37.8846s
Batch 3900/3907, reward: 3.812, loss: -0.0055, took: 37.9283s
Mean epoch loss/reward: -0.0080, 3.8160, 3.8149, took: 1485.4343s (36.8524s / 100 batches)
Batch 0/3907, reward: 3.867, loss: -0.1187, took: 0.5612s
Batch 100/3907, reward: 3.819, loss: 0.0116, took: 38.0938s
Batch 200/3907, reward: 3.816, loss: -0.0117, took: 37.9098s
Batch 300/3907, reward: 3.816, loss: -0.0150, took: 37.9222s
Batch 400/3907, reward: 3.818, loss: -0.0077, took: 38.1499s
Batch 500/3907, reward: 3.820, loss: 0.0011, took: 38.1076s
Batch 600/3907, reward: 3.816, loss: -0.0055, took: 38.1532s
Batch 700/3907, reward: 3.814, loss: -0.0173, took: 38.1290s
Batch 800/3907, reward: 3.814, loss: -0.0067, took: 37.9780s
Batch 900/3907, reward: 3.817, loss: 0.0184, took: 38.0453s
Batch 1000/3907, reward: 3.819, loss: 0.0012, took: 38.1958s
Batch 1100/3907, reward: 3.817, loss: -0.0067, took: 37.8640s
Batch 1200/3907, reward: 3.814, loss: -0.0023, took: 38.0286s
Batch 1300/3907, reward: 3.816, loss: -0.0159, took: 37.9952s
Batch 1400/3907, reward: 3.827, loss: -0.0011, took: 37.8557s
Batch 1500/3907, reward: 3.817, loss: 0.0025, took: 38.0364s
Batch 1600/3907, reward: 3.813, loss: 0.0065, took: 38.0914s
Batch 1700/3907, reward: 3.816, loss: -0.0151, took: 37.9224s
Batch 1800/3907, reward: 3.816, loss: -0.0199, took: 38.2454s
Batch 1900/3907, reward: 3.816, loss: -0.0161, took: 38.1606s
Batch 2000/3907, reward: 3.818, loss: -0.0100, took: 38.0940s
Batch 2100/3907, reward: 3.816, loss: -0.0044, took: 38.3054s
Batch 2200/3907, reward: 3.815, loss: 0.0060, took: 38.0164s
Batch 2300/3907, reward: 3.810, loss: -0.0086, took: 38.0020s
Batch 2400/3907, reward: 3.812, loss: -0.0130, took: 35.2273s
Batch 2500/3907, reward: 3.818, loss: 0.0071, took: 38.0054s
Batch 2600/3907, reward: 3.821, loss: -0.0008, took: 38.2860s
Batch 2700/3907, reward: 3.815, loss: -0.0259, took: 38.0147s
Batch 2800/3907, reward: 3.815, loss: 0.0082, took: 37.9618s
Batch 2900/3907, reward: 3.818, loss: -0.0225, took: 37.9171s
Batch 3000/3907, reward: 3.821, loss: 0.0066, took: 37.8773s
Batch 3100/3907, reward: 3.822, loss: -0.0058, took: 35.3047s
Batch 3200/3907, reward: 3.818, loss: -0.0094, took: 37.9683s
Batch 3300/3907, reward: 3.820, loss: -0.0202, took: 37.9010s
Batch 3400/3907, reward: 3.817, loss: -0.0130, took: 35.6970s
Batch 3500/3907, reward: 3.817, loss: -0.0209, took: 37.4197s
Batch 3600/3907, reward: 3.816, loss: -0.0083, took: 37.9919s
Batch 3700/3907, reward: 3.814, loss: 0.0292, took: 38.0503s
Batch 3800/3907, reward: 3.815, loss: -0.0147, took: 37.8748s
Batch 3900/3907, reward: 3.813, loss: 0.0048, took: 37.7417s
Mean epoch loss/reward: -0.0055, 3.8167, 3.8132, took: 1486.6139s (36.8776s / 100 batches)
Batch 0/3907, reward: 3.809, loss: -0.5813, took: 0.5261s
Batch 100/3907, reward: 3.818, loss: -0.0101, took: 38.1085s
Batch 200/3907, reward: 3.814, loss: -0.0119, took: 37.9117s
Batch 300/3907, reward: 3.812, loss: 0.0361, took: 37.9737s
Batch 400/3907, reward: 3.813, loss: 0.0102, took: 37.9285s
Batch 500/3907, reward: 3.819, loss: -0.0014, took: 38.0665s
Batch 600/3907, reward: 3.813, loss: 0.0082, took: 38.1730s
Batch 700/3907, reward: 3.812, loss: -0.0002, took: 38.1560s
Batch 800/3907, reward: 3.816, loss: 0.0028, took: 37.9390s
Batch 900/3907, reward: 3.814, loss: -0.0122, took: 38.0850s
Batch 1000/3907, reward: 3.814, loss: 0.0009, took: 38.0241s
Batch 1100/3907, reward: 3.812, loss: 0.0147, took: 37.9693s
Batch 1200/3907, reward: 3.821, loss: -0.0007, took: 38.0304s
Batch 1300/3907, reward: 3.816, loss: 0.0113, took: 37.9859s
Batch 1400/3907, reward: 3.816, loss: -0.0217, took: 38.1637s
Batch 1500/3907, reward: 3.813, loss: 0.0090, took: 38.0366s
Batch 1600/3907, reward: 3.818, loss: 0.0053, took: 38.0233s
Batch 1700/3907, reward: 3.811, loss: -0.0306, took: 38.0553s
Batch 1800/3907, reward: 3.817, loss: -0.0014, took: 37.9166s
Batch 1900/3907, reward: 3.817, loss: 0.0120, took: 38.1487s
Batch 2000/3907, reward: 3.815, loss: -0.0011, took: 38.2506s
Batch 2100/3907, reward: 3.818, loss: 0.0124, took: 38.0782s
Batch 2200/3907, reward: 3.815, loss: -0.0396, took: 37.9949s
Batch 2300/3907, reward: 3.812, loss: -0.0051, took: 38.0653s
Batch 2400/3907, reward: 3.818, loss: -0.0148, took: 35.2112s
Batch 2500/3907, reward: 3.819, loss: -0.0005, took: 38.0744s
Batch 2600/3907, reward: 3.821, loss: -0.0335, took: 37.9499s
Batch 2700/3907, reward: 3.825, loss: -0.0176, took: 37.9445s
Batch 2800/3907, reward: 3.818, loss: -0.0184, took: 38.0562s
Batch 2900/3907, reward: 3.814, loss: -0.0408, took: 37.9572s
Batch 3000/3907, reward: 3.815, loss: -0.0080, took: 37.9981s
Batch 3100/3907, reward: 3.816, loss: -0.0160, took: 35.2052s
Batch 3200/3907, reward: 3.818, loss: -0.0220, took: 38.0374s
Batch 3300/3907, reward: 3.812, loss: -0.0226, took: 38.0883s
Batch 3400/3907, reward: 3.812, loss: 0.0152, took: 35.9048s
Batch 3500/3907, reward: 3.811, loss: 0.0013, took: 36.9715s
Batch 3600/3907, reward: 3.816, loss: -0.0001, took: 37.8594s
Batch 3700/3907, reward: 3.814, loss: -0.0052, took: 37.7992s
Batch 3800/3907, reward: 3.811, loss: -0.0081, took: 37.8213s
Batch 3900/3907, reward: 3.812, loss: -0.0112, took: 38.0393s
Mean epoch loss/reward: -0.0056, 3.8153, 3.8119, took: 1486.0473s (36.8632s / 100 batches)
Batch 0/3907, reward: 3.825, loss: -0.1757, took: 0.5634s
Batch 100/3907, reward: 3.809, loss: -0.0155, took: 38.0705s
Batch 200/3907, reward: 3.818, loss: -0.0079, took: 37.9765s
Batch 300/3907, reward: 3.813, loss: -0.0004, took: 37.9047s
Batch 400/3907, reward: 3.814, loss: -0.0340, took: 38.1502s
Batch 500/3907, reward: 3.810, loss: -0.0011, took: 38.1496s
Batch 600/3907, reward: 3.812, loss: 0.0133, took: 37.7992s
Batch 700/3907, reward: 3.813, loss: -0.0072, took: 37.8463s
Batch 800/3907, reward: 3.812, loss: -0.0222, took: 37.9013s
Batch 900/3907, reward: 3.816, loss: -0.0135, took: 37.9255s
Batch 1000/3907, reward: 3.815, loss: -0.0200, took: 38.1000s
Batch 1100/3907, reward: 3.813, loss: 0.0113, took: 38.0345s
Batch 1200/3907, reward: 3.814, loss: -0.0102, took: 38.0412s
Batch 1300/3907, reward: 3.814, loss: -0.0113, took: 37.9665s
Batch 1400/3907, reward: 3.814, loss: -0.0112, took: 38.0585s
Batch 1500/3907, reward: 3.818, loss: -0.0207, took: 38.0582s
Batch 1600/3907, reward: 3.819, loss: -0.0254, took: 38.0626s
Batch 1700/3907, reward: 3.813, loss: -0.0334, took: 37.8170s
Batch 1800/3907, reward: 3.820, loss: 0.0015, took: 38.0924s
Batch 1900/3907, reward: 3.814, loss: -0.0107, took: 37.9257s
Batch 2000/3907, reward: 3.815, loss: 0.0127, took: 38.0275s
Batch 2100/3907, reward: 3.813, loss: -0.0088, took: 37.7756s
Batch 2200/3907, reward: 3.819, loss: 0.0021, took: 37.8690s
Batch 2300/3907, reward: 3.817, loss: -0.0254, took: 38.0186s
Batch 2400/3907, reward: 3.816, loss: 0.0014, took: 34.8082s
Batch 2500/3907, reward: 3.811, loss: 0.0024, took: 37.9709s
Batch 2600/3907, reward: 3.814, loss: -0.0015, took: 38.0496s
Batch 2700/3907, reward: 3.817, loss: -0.0086, took: 38.0090s
Batch 2800/3907, reward: 3.817, loss: -0.0043, took: 37.8771s
Batch 2900/3907, reward: 3.820, loss: -0.0371, took: 37.9599s
Batch 3000/3907, reward: 3.815, loss: -0.0250, took: 37.9726s
Batch 3100/3907, reward: 3.815, loss: -0.0176, took: 35.4824s
Batch 3200/3907, reward: 3.813, loss: -0.0194, took: 37.9366s
Batch 3300/3907, reward: 3.813, loss: 0.0097, took: 38.1007s
Batch 3400/3907, reward: 3.816, loss: -0.0038, took: 36.1063s
Batch 3500/3907, reward: 3.818, loss: -0.0210, took: 36.8422s
Batch 3600/3907, reward: 3.814, loss: 0.0059, took: 37.9333s
Batch 3700/3907, reward: 3.814, loss: 0.0024, took: 37.9764s
Batch 3800/3907, reward: 3.814, loss: -0.0159, took: 37.8642s
Batch 3900/3907, reward: 3.819, loss: -0.0112, took: 38.0446s
Mean epoch loss/reward: -0.0099, 3.8149, 3.8149, took: 1484.6650s (36.8267s / 100 batches)
Batch 0/3907, reward: 3.807, loss: -0.0508, took: 0.5436s
Batch 100/3907, reward: 3.812, loss: -0.0119, took: 37.9569s
Batch 200/3907, reward: 3.815, loss: -0.0259, took: 37.7975s
Batch 300/3907, reward: 3.815, loss: 0.0103, took: 38.2669s
Batch 400/3907, reward: 3.816, loss: -0.0100, took: 37.9661s
Batch 500/3907, reward: 3.816, loss: -0.0057, took: 37.8426s
Batch 600/3907, reward: 3.815, loss: -0.0009, took: 37.9414s
Batch 700/3907, reward: 3.815, loss: 0.0253, took: 38.0219s
Batch 800/3907, reward: 3.816, loss: -0.0082, took: 37.8641s
Batch 900/3907, reward: 3.814, loss: -0.0398, took: 37.9856s
Batch 1000/3907, reward: 3.819, loss: -0.0083, took: 38.0720s
Batch 1100/3907, reward: 3.813, loss: -0.0019, took: 37.9509s
Batch 1200/3907, reward: 3.817, loss: 0.0035, took: 38.1416s
Batch 1300/3907, reward: 3.816, loss: -0.0099, took: 37.9771s
Batch 1400/3907, reward: 3.817, loss: 0.0045, took: 38.0926s
Batch 1500/3907, reward: 3.811, loss: -0.0170, took: 37.9536s
Batch 1600/3907, reward: 3.824, loss: -0.0158, took: 38.0503s
Batch 1700/3907, reward: 3.816, loss: -0.0179, took: 37.9611s
Batch 1800/3907, reward: 3.815, loss: -0.0246, took: 38.0129s
Batch 1900/3907, reward: 3.817, loss: -0.0115, took: 38.0213s
Batch 2000/3907, reward: 3.819, loss: -0.0216, took: 37.8317s
Batch 2100/3907, reward: 3.812, loss: 0.0056, took: 37.8950s
Batch 2200/3907, reward: 3.815, loss: -0.0270, took: 37.8549s
Batch 2300/3907, reward: 3.810, loss: 0.0200, took: 37.9059s
Batch 2400/3907, reward: 3.810, loss: -0.0235, took: 35.1202s
Batch 2500/3907, reward: 3.815, loss: 0.0094, took: 37.8088s
Batch 2600/3907, reward: 3.808, loss: -0.0082, took: 37.7606s
Batch 2700/3907, reward: 3.815, loss: -0.0035, took: 37.8992s
Batch 2800/3907, reward: 3.811, loss: -0.0046, took: 37.9104s
Batch 2900/3907, reward: 3.810, loss: -0.0141, took: 37.8875s
Batch 3000/3907, reward: 3.814, loss: 0.0069, took: 37.9290s
Batch 3100/3907, reward: 3.817, loss: 0.0101, took: 35.2447s
Batch 3200/3907, reward: 3.817, loss: -0.0257, took: 37.8054s
Batch 3300/3907, reward: 3.811, loss: -0.0202, took: 37.8848s
Batch 3400/3907, reward: 3.813, loss: 0.0101, took: 36.3139s
Batch 3500/3907, reward: 3.816, loss: -0.0284, took: 28.8003s
Batch 3600/3907, reward: 3.809, loss: -0.0142, took: 29.0023s
Batch 3700/3907, reward: 3.815, loss: -0.0163, took: 29.1027s
Batch 3800/3907, reward: 3.814, loss: -0.0316, took: 29.1928s
Batch 3900/3907, reward: 3.814, loss: 0.0178, took: 37.6238s
Mean epoch loss/reward: -0.0081, 3.8144, 3.8164, took: 1448.4700s (35.9298s / 100 batches)
Batch 0/3907, reward: 3.811, loss: -0.3313, took: 0.5091s
Batch 100/3907, reward: 3.809, loss: -0.0104, took: 37.8935s
Batch 200/3907, reward: 3.815, loss: -0.0027, took: 37.8291s
Batch 300/3907, reward: 3.814, loss: -0.0341, took: 37.9295s
Batch 400/3907, reward: 3.812, loss: 0.0120, took: 37.9681s
Batch 500/3907, reward: 3.811, loss: -0.0192, took: 37.9530s
Batch 600/3907, reward: 3.812, loss: 0.0022, took: 37.9010s
Batch 700/3907, reward: 3.813, loss: -0.0227, took: 37.8692s
Batch 800/3907, reward: 3.809, loss: -0.0105, took: 37.7436s
Batch 900/3907, reward: 3.816, loss: -0.0078, took: 37.7866s
Batch 1000/3907, reward: 3.814, loss: -0.0033, took: 37.7648s
Batch 1100/3907, reward: 3.812, loss: -0.0264, took: 37.4961s
Batch 1200/3907, reward: 3.811, loss: -0.0013, took: 29.3330s
Batch 1300/3907, reward: 3.812, loss: -0.0008, took: 29.0376s
Batch 1400/3907, reward: 3.813, loss: -0.0282, took: 28.6783s
Batch 1500/3907, reward: 3.812, loss: -0.0121, took: 28.8166s
Batch 1600/3907, reward: 3.816, loss: -0.0018, took: 28.8492s
Batch 1700/3907, reward: 3.815, loss: -0.0287, took: 29.1578s
Batch 1800/3907, reward: 3.813, loss: -0.0176, took: 29.0575s
Batch 1900/3907, reward: 3.809, loss: -0.0026, took: 28.9297s
Batch 2000/3907, reward: 3.812, loss: 0.0047, took: 28.4794s
Batch 2100/3907, reward: 3.811, loss: -0.0083, took: 28.9937s
Batch 2200/3907, reward: 3.812, loss: -0.0361, took: 28.7189s
Batch 2300/3907, reward: 3.810, loss: -0.0106, took: 28.5035s
Batch 2400/3907, reward: 3.814, loss: 0.0010, took: 25.3975s
Batch 2500/3907, reward: 3.813, loss: -0.0243, took: 28.6313s
Batch 2600/3907, reward: 3.808, loss: -0.0015, took: 28.6550s
Batch 2700/3907, reward: 3.823, loss: 0.0123, took: 28.5336s
Batch 2800/3907, reward: 3.818, loss: -0.0029, took: 29.0540s
Batch 2900/3907, reward: 3.817, loss: -0.0251, took: 29.1384s
Batch 3000/3907, reward: 3.814, loss: -0.0177, took: 29.3013s
Batch 3100/3907, reward: 3.809, loss: -0.0087, took: 25.3049s
Batch 3200/3907, reward: 3.818, loss: -0.0198, took: 29.2476s
Batch 3300/3907, reward: 3.814, loss: -0.0009, took: 29.1519s
Batch 3400/3907, reward: 3.812, loss: -0.0035, took: 29.0093s
Batch 3500/3907, reward: 3.812, loss: -0.0264, took: 28.8651s
Batch 3600/3907, reward: 3.810, loss: -0.0202, took: 28.7584s
Batch 3700/3907, reward: 3.812, loss: -0.0167, took: 28.9045s
Batch 3800/3907, reward: 3.817, loss: -0.0384, took: 29.1762s
Batch 3900/3907, reward: 3.815, loss: -0.0215, took: 28.7776s
Mean epoch loss/reward: -0.0123, 3.8131, 3.8161, took: 1229.7216s (30.4776s / 100 batches)
Average tour length for uniform: 4.1999063711505675
Average tour length for shifted: 3.537933942024928
Average tour length for adversary: 2.857391744554577
