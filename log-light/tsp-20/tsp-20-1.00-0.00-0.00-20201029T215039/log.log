Current device: cuda
Batch 0/3907, reward: 10.453, loss: -631.1381, took: 0.2533s
Batch 100/3907, reward: 9.569, loss: -9.0607, took: 10.1596s
Batch 200/3907, reward: 7.773, loss: -0.1676, took: 10.2244s
Batch 300/3907, reward: 7.607, loss: -0.1952, took: 10.2057s
Batch 400/3907, reward: 7.562, loss: 0.4925, took: 10.3591s
Batch 500/3907, reward: 7.549, loss: -0.3926, took: 12.5382s
Batch 600/3907, reward: 7.496, loss: -0.1342, took: 18.6130s
Batch 700/3907, reward: 7.482, loss: -0.2490, took: 21.6830s
Batch 800/3907, reward: 7.457, loss: 0.1963, took: 19.4125s
Batch 900/3907, reward: 7.449, loss: 0.2542, took: 20.0844s
Batch 1000/3907, reward: 6.946, loss: -0.2296, took: 19.6050s
Batch 1100/3907, reward: 5.756, loss: -0.6462, took: 20.3734s
Batch 1200/3907, reward: 5.336, loss: -0.2042, took: 20.8234s
Batch 1300/3907, reward: 5.168, loss: -0.3604, took: 21.0941s
Batch 1400/3907, reward: 5.105, loss: -0.0295, took: 18.7992s
Batch 1500/3907, reward: 4.992, loss: 0.2492, took: 20.2272s
Batch 1600/3907, reward: 4.908, loss: -0.1897, took: 21.1542s
Batch 1700/3907, reward: 4.838, loss: 0.0753, took: 17.8647s
Batch 1800/3907, reward: 4.775, loss: -0.6919, took: 17.0065s
Batch 1900/3907, reward: 4.711, loss: -0.0653, took: 17.4406s
Batch 2000/3907, reward: 4.615, loss: -0.3458, took: 17.4239s
Batch 2100/3907, reward: 4.555, loss: -0.3635, took: 18.5492s
Batch 2200/3907, reward: 4.495, loss: -0.5075, took: 20.9832s
Batch 2300/3907, reward: 4.462, loss: -0.3973, took: 18.8495s
Batch 2400/3907, reward: 4.425, loss: -0.6203, took: 19.5810s
Batch 2500/3907, reward: 4.398, loss: -0.0172, took: 17.9854s
Batch 2600/3907, reward: 4.374, loss: -0.5014, took: 19.7234s
Batch 2700/3907, reward: 4.356, loss: -0.4023, took: 19.8413s
Batch 2800/3907, reward: 4.345, loss: -0.3449, took: 21.4238s
Batch 2900/3907, reward: 4.331, loss: -0.1727, took: 21.1323s
Batch 3000/3907, reward: 4.315, loss: 0.1678, took: 21.5697s
Batch 3100/3907, reward: 4.303, loss: -0.0932, took: 19.4764s
Batch 3200/3907, reward: 4.286, loss: -0.2138, took: 21.6778s
Batch 3300/3907, reward: 4.289, loss: -0.0918, took: 22.9231s
Batch 3400/3907, reward: 4.282, loss: -0.1181, took: 22.9937s
Batch 3500/3907, reward: 4.275, loss: 0.1080, took: 23.6521s
Batch 3600/3907, reward: 4.268, loss: -0.0392, took: 23.6939s
Batch 3700/3907, reward: 4.263, loss: -0.1072, took: 22.1968s
Batch 3800/3907, reward: 4.262, loss: -0.1702, took: 20.1141s
Batch 3900/3907, reward: 4.252, loss: -0.1111, took: 21.5628s
Mean epoch loss/reward: -0.5633, 5.3747, 4.1469, took: 755.2265s (18.5819s / 100 batches)
Batch 0/3907, reward: 4.216, loss: -0.9031, took: 0.4544s
Batch 100/3907, reward: 4.253, loss: 0.0258, took: 22.6059s
Batch 200/3907, reward: 4.240, loss: -0.0365, took: 21.5554s
Batch 300/3907, reward: 4.242, loss: -0.1809, took: 21.7119s
Batch 400/3907, reward: 4.229, loss: -0.2074, took: 24.4149s
Batch 500/3907, reward: 4.225, loss: -0.0842, took: 21.1219s
Batch 600/3907, reward: 4.219, loss: -0.1710, took: 21.4849s
Batch 700/3907, reward: 4.224, loss: -0.2257, took: 23.0935s
Batch 800/3907, reward: 4.229, loss: -0.0388, took: 21.8655s
Batch 900/3907, reward: 4.223, loss: -0.0847, took: 24.1962s
Batch 1000/3907, reward: 4.212, loss: -0.2091, took: 20.9472s
Batch 1100/3907, reward: 4.208, loss: -0.0808, took: 23.9210s
Batch 1200/3907, reward: 4.213, loss: -0.1562, took: 21.4346s
Batch 1300/3907, reward: 4.210, loss: -0.1802, took: 19.6713s
Batch 1400/3907, reward: 4.198, loss: -0.1435, took: 22.9448s
Batch 1500/3907, reward: 4.210, loss: -0.1184, took: 26.7609s
Batch 1600/3907, reward: 4.200, loss: -0.1726, took: 29.4092s
Batch 1700/3907, reward: 4.196, loss: -0.0850, took: 29.8379s
Batch 1800/3907, reward: 4.188, loss: -0.1333, took: 30.2491s
Batch 1900/3907, reward: 4.193, loss: -0.1404, took: 29.1468s
Batch 2000/3907, reward: 4.190, loss: -0.1924, took: 29.2948s
Batch 2100/3907, reward: 4.187, loss: -0.1745, took: 30.7206s
Batch 2200/3907, reward: 4.186, loss: -0.1901, took: 30.1072s
Batch 2300/3907, reward: 4.181, loss: -0.0972, took: 31.0995s
Batch 2400/3907, reward: 4.192, loss: -0.1543, took: 30.2089s
Batch 2500/3907, reward: 4.190, loss: -0.2039, took: 26.8873s
Batch 2600/3907, reward: 4.174, loss: -0.1010, took: 29.5387s
Batch 2700/3907, reward: 4.174, loss: -0.0794, took: 30.2311s
Batch 2800/3907, reward: 4.178, loss: -0.0671, took: 30.2257s
Batch 2900/3907, reward: 4.175, loss: -0.1010, took: 28.7479s
Batch 3000/3907, reward: 4.168, loss: -0.0606, took: 29.6138s
Batch 3100/3907, reward: 4.174, loss: -0.1103, took: 30.2701s
Batch 3200/3907, reward: 4.168, loss: -0.1327, took: 29.5607s
Batch 3300/3907, reward: 4.163, loss: -0.1464, took: 28.4574s
Batch 3400/3907, reward: 4.168, loss: -0.0773, took: 29.3502s
Batch 3500/3907, reward: 4.166, loss: -0.1215, took: 29.8095s
Batch 3600/3907, reward: 4.163, loss: -0.2009, took: 30.1898s
Batch 3700/3907, reward: 4.167, loss: -0.0895, took: 39.2790s
Batch 3800/3907, reward: 4.159, loss: -0.1451, took: 39.7016s
Batch 3900/3907, reward: 4.158, loss: -0.1263, took: 38.4834s
Mean epoch loss/reward: -0.1284, 4.1947, 4.1064, took: 1093.0141s (26.9651s / 100 batches)
Batch 0/3907, reward: 4.142, loss: -0.5433, took: 0.4829s
Batch 100/3907, reward: 4.151, loss: -0.1721, took: 39.9894s
Batch 200/3907, reward: 4.155, loss: -0.0860, took: 38.7591s
Batch 300/3907, reward: 4.157, loss: -0.1259, took: 39.4468s
Batch 400/3907, reward: 4.153, loss: -0.1912, took: 48.2660s
Batch 500/3907, reward: 4.154, loss: -0.0992, took: 47.8711s
Batch 600/3907, reward: 4.150, loss: -0.0841, took: 48.3621s
Batch 700/3907, reward: 4.153, loss: -0.1031, took: 48.4679s
Batch 800/3907, reward: 4.150, loss: -0.1248, took: 46.7557s
Batch 900/3907, reward: 4.149, loss: -0.0848, took: 48.8339s
Batch 1000/3907, reward: 4.141, loss: -0.1098, took: 48.5962s
Batch 1100/3907, reward: 4.143, loss: -0.0990, took: 48.5220s
Batch 1200/3907, reward: 4.144, loss: -0.0653, took: 48.6518s
Batch 1300/3907, reward: 4.146, loss: -0.0923, took: 49.3264s
Batch 1400/3907, reward: 4.140, loss: -0.0771, took: 47.7772s
Batch 1500/3907, reward: 4.141, loss: -0.1490, took: 46.3721s
Batch 1600/3907, reward: 4.136, loss: -0.0123, took: 47.9127s
Batch 1700/3907, reward: 4.137, loss: -0.0724, took: 47.6648s
Batch 1800/3907, reward: 4.128, loss: -0.1340, took: 48.6854s
Batch 1900/3907, reward: 4.131, loss: -0.1586, took: 48.4523s
Batch 2000/3907, reward: 4.128, loss: -0.0740, took: 48.4616s
Batch 2100/3907, reward: 4.131, loss: -0.0843, took: 48.2402s
Batch 2200/3907, reward: 4.130, loss: -0.1234, took: 48.9760s
Batch 2300/3907, reward: 4.133, loss: -0.0477, took: 49.4574s
Batch 2400/3907, reward: 4.134, loss: -0.1376, took: 48.4431s
Batch 2500/3907, reward: 4.129, loss: -0.1176, took: 45.8101s
Batch 2600/3907, reward: 4.126, loss: -0.0777, took: 48.1097s
Batch 2700/3907, reward: 4.124, loss: -0.1152, took: 48.8928s
Batch 2800/3907, reward: 4.120, loss: -0.0804, took: 48.0368s
Batch 2900/3907, reward: 4.127, loss: -0.1192, took: 47.6822s
Batch 3000/3907, reward: 4.123, loss: -0.0970, took: 49.0563s
Batch 3100/3907, reward: 4.117, loss: -0.1080, took: 49.3299s
Batch 3200/3907, reward: 4.119, loss: -0.0372, took: 48.0192s
Batch 3300/3907, reward: 4.120, loss: -0.0860, took: 47.8430s
Batch 3400/3907, reward: 4.126, loss: -0.0634, took: 48.3477s
Batch 3500/3907, reward: 4.127, loss: -0.0806, took: 48.4037s
Batch 3600/3907, reward: 4.113, loss: -0.0684, took: 44.8336s
Batch 3700/3907, reward: 4.126, loss: -0.0776, took: 48.8961s
Batch 3800/3907, reward: 4.119, loss: -0.0832, took: 49.7119s
Batch 3900/3907, reward: 4.116, loss: -0.0754, took: 48.4153s
Mean epoch loss/reward: -0.0971, 4.1346, 4.0919, took: 1868.9385s (46.3541s / 100 batches)
Batch 0/3907, reward: 4.118, loss: -0.3634, took: 0.6845s
Batch 100/3907, reward: 4.119, loss: -0.0925, took: 48.8042s
Batch 200/3907, reward: 4.122, loss: -0.0564, took: 48.2410s
Batch 300/3907, reward: 4.116, loss: -0.0998, took: 44.8329s
Batch 400/3907, reward: 4.108, loss: -0.0706, took: 47.9117s
Batch 500/3907, reward: 4.115, loss: -0.0939, took: 47.4920s
Batch 600/3907, reward: 4.113, loss: -0.0807, took: 49.7637s
Batch 700/3907, reward: 4.122, loss: -0.0411, took: 48.1992s
Batch 800/3907, reward: 4.112, loss: -0.0674, took: 47.4443s
Batch 900/3907, reward: 4.118, loss: -0.0763, took: 48.7270s
Batch 1000/3907, reward: 4.111, loss: -0.0669, took: 50.3733s
Batch 1100/3907, reward: 4.106, loss: -0.0737, took: 47.6311s
Batch 1200/3907, reward: 4.107, loss: -0.0819, took: 48.3041s
Batch 1300/3907, reward: 4.109, loss: -0.0667, took: 48.4538s
Batch 1400/3907, reward: 4.104, loss: -0.0993, took: 47.9686s
Batch 1500/3907, reward: 4.110, loss: -0.0901, took: 45.3190s
Batch 1600/3907, reward: 4.110, loss: -0.0557, took: 50.5800s
Batch 1700/3907, reward: 4.107, loss: -0.0707, took: 48.3192s
Batch 1800/3907, reward: 4.105, loss: -0.0820, took: 48.9767s
Batch 1900/3907, reward: 4.113, loss: -0.0675, took: 48.7033s
Batch 2000/3907, reward: 4.110, loss: -0.0606, took: 49.4229s
Batch 2100/3907, reward: 4.107, loss: -0.0390, took: 49.4522s
Batch 2200/3907, reward: 4.109, loss: -0.0756, took: 47.8124s
Batch 2300/3907, reward: 4.104, loss: -0.0948, took: 47.8641s
Batch 2400/3907, reward: 4.106, loss: -0.0760, took: 48.8936s
Batch 2500/3907, reward: 4.107, loss: -0.0599, took: 45.8819s
Batch 2600/3907, reward: 4.106, loss: -0.0656, took: 48.2889s
Batch 2700/3907, reward: 4.107, loss: -0.0745, took: 48.6935s
Batch 2800/3907, reward: 4.101, loss: -0.0953, took: 49.1394s
Batch 2900/3907, reward: 4.100, loss: -0.0632, took: 49.3911s
Batch 3000/3907, reward: 4.102, loss: -0.0466, took: 49.3694s
Batch 3100/3907, reward: 4.104, loss: -0.0673, took: 48.1029s
Batch 3200/3907, reward: 4.098, loss: -0.0735, took: 48.7450s
Batch 3300/3907, reward: 4.104, loss: -0.0656, took: 48.7362s
Batch 3400/3907, reward: 4.106, loss: -0.0626, took: 48.0766s
Batch 3500/3907, reward: 4.102, loss: -0.0818, took: 48.4972s
Batch 3600/3907, reward: 4.100, loss: -0.0635, took: 45.2957s
Batch 3700/3907, reward: 4.104, loss: -0.0673, took: 48.4874s
Batch 3800/3907, reward: 4.101, loss: -0.0705, took: 48.8545s
Batch 3900/3907, reward: 4.099, loss: -0.0556, took: 47.6679s
Mean epoch loss/reward: -0.0718, 4.1078, 4.0939, took: 1898.0410s (47.0851s / 100 batches)
Batch 0/3907, reward: 4.128, loss: 0.7228, took: 0.5989s
Batch 100/3907, reward: 4.106, loss: -0.0730, took: 48.8873s
Batch 200/3907, reward: 4.096, loss: -0.0533, took: 48.4942s
Batch 300/3907, reward: 4.098, loss: -0.0826, took: 45.2314s
Batch 400/3907, reward: 4.097, loss: -0.0800, took: 47.4090s
Batch 500/3907, reward: 4.096, loss: -0.0611, took: 47.9174s
Batch 600/3907, reward: 4.097, loss: -0.0512, took: 48.4189s
Batch 700/3907, reward: 4.099, loss: -0.0824, took: 49.0016s
Batch 800/3907, reward: 4.098, loss: -0.0529, took: 49.1668s
Batch 900/3907, reward: 4.098, loss: -0.0910, took: 49.1753s
Batch 1000/3907, reward: 4.099, loss: -0.1004, took: 47.2028s
Batch 1100/3907, reward: 4.096, loss: -0.0692, took: 47.3549s
Batch 1200/3907, reward: 4.088, loss: -0.0618, took: 47.2655s
Batch 1300/3907, reward: 4.093, loss: -0.0760, took: 47.5073s
Batch 1400/3907, reward: 4.100, loss: -0.0741, took: 47.0134s
Batch 1500/3907, reward: 4.101, loss: -0.0399, took: 45.6511s
Batch 1600/3907, reward: 4.103, loss: -0.0619, took: 49.0020s
Batch 1700/3907, reward: 4.094, loss: -0.0625, took: 47.9354s
Batch 1800/3907, reward: 4.099, loss: -0.0673, took: 49.4400s
Batch 1900/3907, reward: 4.095, loss: -0.0824, took: 47.3841s
Batch 2000/3907, reward: 4.096, loss: -0.0801, took: 47.7470s
Batch 2100/3907, reward: 4.098, loss: -0.0556, took: 48.3768s
Batch 2200/3907, reward: 4.093, loss: -0.0587, took: 48.4992s
Batch 2300/3907, reward: 4.104, loss: -0.0437, took: 48.4342s
Batch 2400/3907, reward: 4.091, loss: -0.0646, took: 47.4177s
Batch 2500/3907, reward: 4.098, loss: -0.0915, took: 45.6739s
Batch 2600/3907, reward: 4.088, loss: -0.0645, took: 49.1116s
Batch 2700/3907, reward: 4.092, loss: -0.0789, took: 47.4984s
Batch 2800/3907, reward: 4.095, loss: -0.0681, took: 48.5138s
Batch 2900/3907, reward: 4.090, loss: -0.0838, took: 50.1725s
Batch 3000/3907, reward: 4.097, loss: -0.0775, took: 48.2530s
Batch 3100/3907, reward: 4.091, loss: -0.0553, took: 48.1411s
Batch 3200/3907, reward: 4.094, loss: -0.0445, took: 48.7501s
Batch 3300/3907, reward: 4.093, loss: -0.0702, took: 47.7439s
Batch 3400/3907, reward: 4.091, loss: -0.0479, took: 49.4528s
Batch 3500/3907, reward: 4.090, loss: -0.0757, took: 49.0154s
Batch 3600/3907, reward: 4.087, loss: -0.0485, took: 44.6469s
Batch 3700/3907, reward: 4.090, loss: -0.0568, took: 48.2546s
Batch 3800/3907, reward: 4.088, loss: -0.0410, took: 46.9132s
Batch 3900/3907, reward: 4.093, loss: -0.0620, took: 48.7416s
Mean epoch loss/reward: -0.0661, 4.0951, 4.0768, took: 1885.4570s (46.7854s / 100 batches)
Batch 0/3907, reward: 4.129, loss: 0.0831, took: 0.5642s
Batch 100/3907, reward: 4.102, loss: -0.0612, took: 49.3298s
Batch 200/3907, reward: 4.092, loss: -0.0664, took: 48.2310s
Batch 300/3907, reward: 4.086, loss: -0.0811, took: 45.1885s
Batch 400/3907, reward: 4.093, loss: -0.0672, took: 48.2293s
Batch 500/3907, reward: 4.097, loss: -0.0521, took: 47.6786s
Batch 600/3907, reward: 4.086, loss: -0.0599, took: 48.6709s
Batch 700/3907, reward: 4.089, loss: -0.0615, took: 47.3919s
Batch 800/3907, reward: 4.088, loss: -0.0799, took: 48.6928s
Batch 900/3907, reward: 4.089, loss: -0.0739, took: 49.0618s
Batch 1000/3907, reward: 4.088, loss: -0.0546, took: 47.6496s
Batch 1100/3907, reward: 4.083, loss: -0.0716, took: 48.3586s
Batch 1200/3907, reward: 4.085, loss: -0.0623, took: 47.8129s
Batch 1300/3907, reward: 4.092, loss: -0.0590, took: 48.9456s
Batch 1400/3907, reward: 4.091, loss: -0.0564, took: 48.1169s
Batch 1500/3907, reward: 4.085, loss: -0.0532, took: 45.8989s
Batch 1600/3907, reward: 4.085, loss: -0.0860, took: 48.2478s
Batch 1700/3907, reward: 4.096, loss: -0.0517, took: 49.0319s
Batch 1800/3907, reward: 4.095, loss: -0.0706, took: 48.9218s
Batch 1900/3907, reward: 4.089, loss: -0.0472, took: 48.8489s
Batch 2000/3907, reward: 4.091, loss: -0.0691, took: 48.1352s
Batch 2100/3907, reward: 4.089, loss: -0.0716, took: 48.6779s
Batch 2200/3907, reward: 4.085, loss: -0.0669, took: 47.2938s
Batch 2300/3907, reward: 4.090, loss: -0.0596, took: 47.5978s
Batch 2400/3907, reward: 4.087, loss: -0.0538, took: 48.6588s
Batch 2500/3907, reward: 4.088, loss: -0.0787, took: 46.5075s
Batch 2600/3907, reward: 4.082, loss: -0.0778, took: 48.4425s
Batch 2700/3907, reward: 4.089, loss: -0.0627, took: 48.5910s
Batch 2800/3907, reward: 4.086, loss: -0.0831, took: 48.1228s
Batch 2900/3907, reward: 4.085, loss: -0.0486, took: 49.0616s
Batch 3000/3907, reward: 4.090, loss: -0.0458, took: 48.0305s
Batch 3100/3907, reward: 4.082, loss: -0.0594, took: 47.8180s
Batch 3200/3907, reward: 4.091, loss: -0.0590, took: 47.9552s
Batch 3300/3907, reward: 4.086, loss: -0.0602, took: 48.3337s
Batch 3400/3907, reward: 4.085, loss: -0.0661, took: 48.0904s
Batch 3500/3907, reward: 4.088, loss: -0.0602, took: 47.4089s
Batch 3600/3907, reward: 4.083, loss: -0.0844, took: 46.0531s
Batch 3700/3907, reward: 4.083, loss: -0.0519, took: 47.8570s
Batch 3800/3907, reward: 4.080, loss: -0.0605, took: 48.8142s
Batch 3900/3907, reward: 4.087, loss: -0.0674, took: 47.7310s
Mean epoch loss/reward: -0.0641, 4.0881, 4.0646, took: 1888.1704s (46.8513s / 100 batches)
Batch 0/3907, reward: 4.093, loss: -0.0487, took: 0.6268s
Batch 100/3907, reward: 4.085, loss: -0.0809, took: 47.5497s
Batch 200/3907, reward: 4.089, loss: -0.0465, took: 48.1481s
Batch 300/3907, reward: 4.082, loss: -0.0682, took: 46.7338s
Batch 400/3907, reward: 4.082, loss: -0.0543, took: 48.7655s
Batch 500/3907, reward: 4.077, loss: -0.0657, took: 48.1833s
Batch 600/3907, reward: 4.085, loss: -0.0645, took: 49.0854s
Batch 700/3907, reward: 4.085, loss: -0.0473, took: 47.4873s
Batch 800/3907, reward: 4.080, loss: -0.0447, took: 47.5155s
Batch 900/3907, reward: 4.076, loss: -0.0499, took: 48.4315s
Batch 1000/3907, reward: 4.085, loss: -0.0590, took: 47.6505s
Batch 1100/3907, reward: 4.081, loss: -0.0444, took: 47.3070s
Batch 1200/3907, reward: 4.084, loss: -0.0753, took: 47.2560s
Batch 1300/3907, reward: 4.076, loss: -0.0715, took: 47.9322s
Batch 1400/3907, reward: 4.085, loss: -0.0474, took: 47.4873s
Batch 1500/3907, reward: 4.084, loss: -0.0562, took: 46.5988s
Batch 1600/3907, reward: 4.080, loss: -0.0797, took: 49.0732s
Batch 1700/3907, reward: 4.089, loss: -0.0691, took: 47.5425s
Batch 1800/3907, reward: 4.082, loss: -0.0584, took: 49.0566s
Batch 1900/3907, reward: 4.078, loss: -0.0552, took: 46.9019s
Batch 2000/3907, reward: 4.077, loss: -0.0351, took: 47.2955s
Batch 2100/3907, reward: 4.081, loss: -0.0502, took: 48.0791s
Batch 2200/3907, reward: 4.083, loss: -0.0490, took: 48.1394s
Batch 2300/3907, reward: 4.087, loss: -0.0533, took: 47.7070s
Batch 2400/3907, reward: 4.084, loss: -0.0530, took: 48.7934s
Batch 2500/3907, reward: 4.083, loss: -0.0634, took: 46.3050s
Batch 2600/3907, reward: 4.090, loss: -0.0502, took: 46.9795s
Batch 2700/3907, reward: 4.091, loss: -0.0501, took: 47.8100s
Batch 2800/3907, reward: 4.075, loss: -0.0739, took: 49.3101s
Batch 2900/3907, reward: 4.081, loss: -0.0327, took: 47.6020s
Batch 3000/3907, reward: 4.082, loss: -0.0802, took: 46.7969s
Batch 3100/3907, reward: 4.080, loss: -0.0542, took: 49.3058s
Batch 3200/3907, reward: 4.086, loss: -0.0538, took: 47.8037s
Batch 3300/3907, reward: 4.077, loss: -0.0745, took: 49.0157s
Batch 3400/3907, reward: 4.080, loss: -0.0568, took: 47.3712s
Batch 3500/3907, reward: 4.073, loss: -0.0423, took: 48.1076s
Batch 3600/3907, reward: 4.080, loss: -0.0498, took: 44.1127s
Batch 3700/3907, reward: 4.071, loss: -0.0472, took: 49.0563s
Batch 3800/3907, reward: 4.083, loss: -0.0528, took: 48.7436s
Batch 3900/3907, reward: 4.079, loss: -0.0502, took: 48.2209s
Mean epoch loss/reward: -0.0566, 4.0818, 4.0544, took: 1879.5392s (46.6472s / 100 batches)
Batch 0/3907, reward: 4.050, loss: -0.1811, took: 0.8735s
Batch 100/3907, reward: 4.080, loss: -0.0613, took: 47.1599s
Batch 200/3907, reward: 4.079, loss: -0.0589, took: 46.7615s
Batch 300/3907, reward: 4.083, loss: -0.0466, took: 43.1575s
Batch 400/3907, reward: 4.078, loss: -0.0557, took: 48.9994s
Batch 500/3907, reward: 4.080, loss: -0.0493, took: 48.5930s
Batch 600/3907, reward: 4.080, loss: -0.0357, took: 48.8430s
Batch 700/3907, reward: 4.077, loss: -0.0454, took: 47.8662s
Batch 800/3907, reward: 4.076, loss: -0.0628, took: 48.9781s
Batch 900/3907, reward: 4.079, loss: -0.0602, took: 47.1618s
Batch 1000/3907, reward: 4.074, loss: -0.0603, took: 47.6841s
Batch 1100/3907, reward: 4.077, loss: -0.0388, took: 48.8598s
Batch 1200/3907, reward: 4.072, loss: -0.0417, took: 47.9367s
Batch 1300/3907, reward: 4.079, loss: -0.0586, took: 47.9197s
Batch 1400/3907, reward: 4.086, loss: -0.0486, took: 46.9715s
Batch 1500/3907, reward: 4.072, loss: -0.0684, took: 47.4987s
Batch 1600/3907, reward: 4.074, loss: -0.0514, took: 48.6044s
Batch 1700/3907, reward: 4.073, loss: -0.0620, took: 49.7680s
Batch 1800/3907, reward: 4.077, loss: -0.0708, took: 49.5015s
Batch 1900/3907, reward: 4.081, loss: -0.0656, took: 48.1441s
Batch 2000/3907, reward: 4.079, loss: -0.0513, took: 47.4912s
Batch 2100/3907, reward: 4.084, loss: -0.0409, took: 48.8923s
Batch 2200/3907, reward: 4.069, loss: -0.0386, took: 48.2296s
Batch 2300/3907, reward: 4.075, loss: -0.0584, took: 48.3443s
Batch 2400/3907, reward: 4.082, loss: -0.0453, took: 48.1966s
Batch 2500/3907, reward: 4.075, loss: -0.0462, took: 46.7624s
Batch 2600/3907, reward: 4.074, loss: -0.0731, took: 47.8303s
Batch 2700/3907, reward: 4.074, loss: -0.0487, took: 47.7095s
Batch 2800/3907, reward: 4.073, loss: -0.0419, took: 48.9476s
Batch 2900/3907, reward: 4.076, loss: -0.0347, took: 48.3503s
Batch 3000/3907, reward: 4.076, loss: -0.0491, took: 48.3938s
Batch 3100/3907, reward: 4.071, loss: -0.0481, took: 48.2815s
Batch 3200/3907, reward: 4.074, loss: -0.0502, took: 47.8000s
Batch 3300/3907, reward: 4.077, loss: -0.0545, took: 47.2464s
Batch 3400/3907, reward: 4.072, loss: -0.0308, took: 48.9111s
Batch 3500/3907, reward: 4.079, loss: -0.0570, took: 48.7623s
Batch 3600/3907, reward: 4.078, loss: -0.0538, took: 47.1068s
Batch 3700/3907, reward: 4.072, loss: -0.0505, took: 48.6851s
Batch 3800/3907, reward: 4.077, loss: -0.0545, took: 48.3484s
Batch 3900/3907, reward: 4.075, loss: -0.0563, took: 47.4949s
Mean epoch loss/reward: -0.0519, 4.0766, 4.0608, took: 1887.1715s (46.8267s / 100 batches)
Batch 0/3907, reward: 4.090, loss: -0.0351, took: 0.5597s
Batch 100/3907, reward: 4.070, loss: -0.0558, took: 48.7061s
Batch 200/3907, reward: 4.077, loss: -0.0455, took: 47.7554s
Batch 300/3907, reward: 4.074, loss: -0.0538, took: 45.3782s
Batch 400/3907, reward: 4.076, loss: -0.0496, took: 48.4869s
Batch 500/3907, reward: 4.074, loss: -0.0547, took: 47.4257s
Batch 600/3907, reward: 4.073, loss: -0.0685, took: 49.1382s
Batch 700/3907, reward: 4.081, loss: -0.0559, took: 48.2706s
Batch 800/3907, reward: 4.073, loss: -0.0521, took: 47.6391s
Batch 900/3907, reward: 4.076, loss: -0.0350, took: 47.6403s
Batch 1000/3907, reward: 4.075, loss: -0.0493, took: 48.6515s
Batch 1100/3907, reward: 4.067, loss: -0.0456, took: 48.7149s
Batch 1200/3907, reward: 4.079, loss: -0.0661, took: 48.4361s
Batch 1300/3907, reward: 4.079, loss: -0.0475, took: 49.6736s
Batch 1400/3907, reward: 4.074, loss: -0.0608, took: 46.7286s
Batch 1500/3907, reward: 4.074, loss: -0.0426, took: 47.7295s
Batch 1600/3907, reward: 4.073, loss: -0.0594, took: 47.3820s
Batch 1700/3907, reward: 4.075, loss: -0.0533, took: 47.4707s
Batch 1800/3907, reward: 4.073, loss: -0.0574, took: 48.2646s
Batch 1900/3907, reward: 4.079, loss: -0.0636, took: 47.4694s
Batch 2000/3907, reward: 4.072, loss: -0.0267, took: 48.6293s
Batch 2100/3907, reward: 4.074, loss: -0.0565, took: 47.5408s
Batch 2200/3907, reward: 4.069, loss: -0.0517, took: 47.9070s
Batch 2300/3907, reward: 4.073, loss: -0.0414, took: 48.1049s
Batch 2400/3907, reward: 4.072, loss: -0.0476, took: 48.2305s
Batch 2500/3907, reward: 4.074, loss: -0.0483, took: 46.1913s
Batch 2600/3907, reward: 4.068, loss: -0.0357, took: 47.6269s
Batch 2700/3907, reward: 4.070, loss: -0.0384, took: 48.7032s
Batch 2800/3907, reward: 4.084, loss: -0.0594, took: 47.8324s
Batch 2900/3907, reward: 4.073, loss: -0.0648, took: 47.5595s
Batch 3000/3907, reward: 4.067, loss: -0.0314, took: 49.0450s
Batch 3100/3907, reward: 4.075, loss: -0.0613, took: 48.9013s
Batch 3200/3907, reward: 4.072, loss: -0.0459, took: 49.4255s
Batch 3300/3907, reward: 4.075, loss: -0.0364, took: 47.4072s
Batch 3400/3907, reward: 4.071, loss: -0.0506, took: 48.1629s
Batch 3500/3907, reward: 4.073, loss: -0.0445, took: 46.7547s
Batch 3600/3907, reward: 4.065, loss: -0.0424, took: 46.2415s
Batch 3700/3907, reward: 4.072, loss: -0.0485, took: 47.6887s
Batch 3800/3907, reward: 4.074, loss: -0.0504, took: 47.5878s
Batch 3900/3907, reward: 4.069, loss: -0.0327, took: 49.8224s
Mean epoch loss/reward: -0.0495, 4.0734, 4.0703, took: 1884.1951s (46.7721s / 100 batches)
Batch 0/3907, reward: 4.076, loss: -0.3611, took: 0.6303s
Batch 100/3907, reward: 4.066, loss: -0.0564, took: 47.3798s
Batch 200/3907, reward: 4.074, loss: -0.0519, took: 49.0537s
Batch 300/3907, reward: 4.074, loss: -0.0597, took: 46.2911s
Batch 400/3907, reward: 4.068, loss: -0.0506, took: 47.5210s
Batch 500/3907, reward: 4.070, loss: -0.0638, took: 48.0816s
Batch 600/3907, reward: 4.077, loss: -0.0438, took: 47.4709s
Batch 700/3907, reward: 4.075, loss: -0.0594, took: 47.7143s
Batch 800/3907, reward: 4.065, loss: -0.0411, took: 47.3324s
Batch 900/3907, reward: 4.070, loss: -0.0434, took: 47.6766s
Batch 1000/3907, reward: 4.081, loss: -0.0279, took: 47.7238s
Batch 1100/3907, reward: 4.074, loss: -0.0548, took: 48.6647s
Batch 1200/3907, reward: 4.077, loss: -0.0768, took: 48.6601s
Batch 1300/3907, reward: 4.078, loss: -0.0547, took: 47.5237s
Batch 1400/3907, reward: 4.077, loss: -0.0450, took: 45.5856s
Batch 1500/3907, reward: 4.070, loss: -0.0608, took: 49.1271s
Batch 1600/3907, reward: 4.073, loss: -0.0411, took: 47.3404s
Batch 1700/3907, reward: 4.075, loss: -0.0399, took: 47.4665s
Batch 1800/3907, reward: 4.073, loss: -0.0363, took: 47.0071s
Batch 1900/3907, reward: 4.070, loss: -0.0523, took: 48.1542s
Batch 2000/3907, reward: 4.070, loss: -0.0531, took: 49.0646s
Batch 2100/3907, reward: 4.070, loss: -0.0438, took: 47.9272s
Batch 2200/3907, reward: 4.073, loss: -0.0663, took: 47.7414s
Batch 2300/3907, reward: 4.073, loss: -0.0623, took: 47.6612s
Batch 2400/3907, reward: 4.075, loss: -0.0421, took: 49.1113s
Batch 2500/3907, reward: 4.073, loss: -0.0374, took: 45.3048s
Batch 2600/3907, reward: 4.069, loss: -0.0492, took: 49.5469s
Batch 2700/3907, reward: 4.062, loss: -0.0412, took: 49.1738s
Batch 2800/3907, reward: 4.073, loss: -0.0510, took: 47.8571s
Batch 2900/3907, reward: 4.073, loss: -0.0592, took: 48.1363s
Batch 3000/3907, reward: 4.074, loss: -0.0384, took: 47.3978s
Batch 3100/3907, reward: 4.075, loss: -0.0395, took: 49.0683s
Batch 3200/3907, reward: 4.066, loss: -0.0456, took: 48.0162s
Batch 3300/3907, reward: 4.074, loss: -0.0400, took: 47.3022s
Batch 3400/3907, reward: 4.074, loss: -0.0694, took: 47.9793s
Batch 3500/3907, reward: 4.071, loss: -0.0504, took: 46.1492s
Batch 3600/3907, reward: 4.069, loss: -0.0356, took: 46.0010s
Batch 3700/3907, reward: 4.066, loss: -0.0464, took: 48.0400s
Batch 3800/3907, reward: 4.070, loss: -0.0520, took: 46.4521s
Batch 3900/3907, reward: 4.064, loss: -0.0313, took: 48.1644s
Mean epoch loss/reward: -0.0491, 4.0718, 4.0511, took: 1877.7200s (46.5625s / 100 batches)
Batch 0/3907, reward: 4.064, loss: -0.0879, took: 0.6910s
Batch 100/3907, reward: 4.068, loss: -0.0502, took: 49.2147s
Batch 200/3907, reward: 4.064, loss: -0.0517, took: 47.7405s
Batch 300/3907, reward: 4.068, loss: -0.0342, took: 43.9382s
Batch 400/3907, reward: 4.064, loss: -0.0400, took: 47.3245s
Batch 500/3907, reward: 4.070, loss: -0.0463, took: 48.1914s
Batch 600/3907, reward: 4.069, loss: -0.0552, took: 48.6635s
Batch 700/3907, reward: 4.064, loss: -0.0564, took: 48.3442s
Batch 800/3907, reward: 4.066, loss: -0.0254, took: 47.9954s
Batch 900/3907, reward: 4.073, loss: -0.0515, took: 47.8543s
Batch 1000/3907, reward: 4.068, loss: -0.0500, took: 47.5409s
Batch 1100/3907, reward: 4.063, loss: -0.0367, took: 47.4212s
Batch 1200/3907, reward: 4.065, loss: -0.0419, took: 46.8640s
Batch 1300/3907, reward: 4.061, loss: -0.0318, took: 48.0671s
Batch 1400/3907, reward: 4.068, loss: -0.0346, took: 46.0373s
Batch 1500/3907, reward: 4.067, loss: -0.0499, took: 49.5105s
Batch 1600/3907, reward: 4.059, loss: -0.0487, took: 48.3178s
Batch 1700/3907, reward: 4.066, loss: -0.0429, took: 48.1503s
Batch 1800/3907, reward: 4.078, loss: -0.0433, took: 47.4918s
Batch 1900/3907, reward: 4.070, loss: -0.0416, took: 47.5678s
Batch 2000/3907, reward: 4.069, loss: -0.0482, took: 46.9630s
Batch 2100/3907, reward: 4.071, loss: -0.0408, took: 47.9055s
Batch 2200/3907, reward: 4.062, loss: -0.0524, took: 48.0051s
Batch 2300/3907, reward: 4.067, loss: -0.0479, took: 48.1748s
Batch 2400/3907, reward: 4.064, loss: -0.0400, took: 47.9003s
Batch 2500/3907, reward: 4.063, loss: -0.0454, took: 46.2071s
Batch 2600/3907, reward: 4.068, loss: -0.0398, took: 47.6703s
Batch 2700/3907, reward: 4.063, loss: -0.0309, took: 47.4117s
Batch 2800/3907, reward: 4.069, loss: -0.0555, took: 48.7657s
Batch 2900/3907, reward: 4.063, loss: -0.0378, took: 47.2433s
Batch 3000/3907, reward: 4.070, loss: -0.0340, took: 47.7569s
Batch 3100/3907, reward: 4.064, loss: -0.0447, took: 47.5759s
Batch 3200/3907, reward: 4.066, loss: -0.0642, took: 48.0888s
Batch 3300/3907, reward: 4.066, loss: -0.0287, took: 47.1144s
Batch 3400/3907, reward: 4.062, loss: -0.0375, took: 46.9290s
Batch 3500/3907, reward: 4.067, loss: -0.0585, took: 48.4194s
Batch 3600/3907, reward: 4.069, loss: -0.0453, took: 44.3892s
Batch 3700/3907, reward: 4.060, loss: -0.0398, took: 47.6519s
Batch 3800/3907, reward: 4.062, loss: -0.0453, took: 47.1015s
Batch 3900/3907, reward: 4.067, loss: -0.0274, took: 48.1369s
Mean epoch loss/reward: -0.0436, 4.0662, 4.0568, took: 1870.2518s (46.4084s / 100 batches)
Batch 0/3907, reward: 4.038, loss: 0.1011, took: 0.5695s
Batch 100/3907, reward: 4.063, loss: -0.0514, took: 47.1908s
Batch 200/3907, reward: 4.067, loss: -0.0518, took: 47.4215s
Batch 300/3907, reward: 4.062, loss: -0.0337, took: 46.0506s
Batch 400/3907, reward: 4.063, loss: -0.0426, took: 48.9287s
Batch 500/3907, reward: 4.060, loss: -0.0283, took: 47.6534s
Batch 600/3907, reward: 4.069, loss: -0.0501, took: 46.7102s
Batch 700/3907, reward: 4.061, loss: -0.0505, took: 47.9600s
Batch 800/3907, reward: 4.062, loss: -0.0311, took: 46.8596s
Batch 900/3907, reward: 4.067, loss: -0.0542, took: 46.4107s
Batch 1000/3907, reward: 4.064, loss: -0.0287, took: 48.1549s
Batch 1100/3907, reward: 4.061, loss: -0.0492, took: 48.6457s
Batch 1200/3907, reward: 4.060, loss: -0.0416, took: 46.6540s
Batch 1300/3907, reward: 4.067, loss: -0.0275, took: 47.5652s
Batch 1400/3907, reward: 4.063, loss: -0.0410, took: 46.1015s
Batch 1500/3907, reward: 4.064, loss: -0.0475, took: 46.3752s
Batch 1600/3907, reward: 4.063, loss: -0.0454, took: 47.7800s
Batch 1700/3907, reward: 4.058, loss: -0.0444, took: 48.0189s
Batch 1800/3907, reward: 4.063, loss: -0.0320, took: 48.5720s
Batch 1900/3907, reward: 4.062, loss: -0.0445, took: 48.2455s
Batch 2000/3907, reward: 4.071, loss: -0.0307, took: 49.4409s
Batch 2100/3907, reward: 4.064, loss: -0.0436, took: 47.8080s
Batch 2200/3907, reward: 4.053, loss: -0.0236, took: 49.1078s
Batch 2300/3907, reward: 4.062, loss: -0.0522, took: 47.1394s
Batch 2400/3907, reward: 4.060, loss: -0.0401, took: 48.4935s
Batch 2500/3907, reward: 4.072, loss: -0.0355, took: 44.4449s
Batch 2600/3907, reward: 4.060, loss: -0.0336, took: 48.3466s
Batch 2700/3907, reward: 4.065, loss: -0.0546, took: 48.4542s
Batch 2800/3907, reward: 4.064, loss: -0.0377, took: 47.6232s
Batch 2900/3907, reward: 4.066, loss: -0.0433, took: 48.1804s
Batch 3000/3907, reward: 4.060, loss: -0.0331, took: 47.6864s
Batch 3100/3907, reward: 4.062, loss: -0.0388, took: 47.2901s
Batch 3200/3907, reward: 4.062, loss: -0.0443, took: 48.6166s
Batch 3300/3907, reward: 4.062, loss: -0.0441, took: 47.8949s
Batch 3400/3907, reward: 4.061, loss: -0.0460, took: 48.7628s
Batch 3500/3907, reward: 4.064, loss: -0.0292, took: 47.8436s
Batch 3600/3907, reward: 4.062, loss: -0.0443, took: 44.8494s
Batch 3700/3907, reward: 4.065, loss: -0.0364, took: 48.0056s
Batch 3800/3907, reward: 4.061, loss: -0.0492, took: 48.0970s
Batch 3900/3907, reward: 4.060, loss: -0.0458, took: 48.0054s
Mean epoch loss/reward: -0.0410, 4.0630, 4.0498, took: 1872.3150s (46.4490s / 100 batches)
Batch 0/3907, reward: 4.072, loss: -0.0297, took: 0.7065s
Batch 100/3907, reward: 4.063, loss: -0.0552, took: 47.8208s
Batch 200/3907, reward: 4.067, loss: -0.0442, took: 47.2469s
Batch 300/3907, reward: 4.058, loss: -0.0374, took: 44.5735s
Batch 400/3907, reward: 4.069, loss: -0.0451, took: 48.5056s
Batch 500/3907, reward: 4.066, loss: -0.0431, took: 48.0578s
Batch 600/3907, reward: 4.067, loss: -0.0414, took: 47.7019s
Batch 700/3907, reward: 4.059, loss: -0.0321, took: 47.9268s
Batch 800/3907, reward: 4.057, loss: -0.0448, took: 47.3953s
Batch 900/3907, reward: 4.063, loss: -0.0381, took: 47.1434s
Batch 1000/3907, reward: 4.058, loss: -0.0374, took: 47.6345s
Batch 1100/3907, reward: 4.063, loss: -0.0442, took: 47.5969s
Batch 1200/3907, reward: 4.066, loss: -0.0421, took: 47.4740s
Batch 1300/3907, reward: 4.063, loss: -0.0415, took: 47.4939s
Batch 1400/3907, reward: 4.057, loss: -0.0368, took: 46.0214s
Batch 1500/3907, reward: 4.059, loss: -0.0413, took: 48.7388s
Batch 1600/3907, reward: 4.064, loss: -0.0430, took: 47.6835s
Batch 1700/3907, reward: 4.066, loss: -0.0460, took: 46.6824s
Batch 1800/3907, reward: 4.061, loss: -0.0500, took: 47.2554s
Batch 1900/3907, reward: 4.061, loss: -0.0664, took: 48.2105s
Batch 2000/3907, reward: 4.061, loss: -0.0376, took: 48.1902s
Batch 2100/3907, reward: 4.066, loss: -0.0454, took: 48.8336s
Batch 2200/3907, reward: 4.054, loss: -0.0279, took: 47.8120s
Batch 2300/3907, reward: 4.057, loss: -0.0380, took: 48.0654s
Batch 2400/3907, reward: 4.058, loss: -0.0470, took: 49.1224s
Batch 2500/3907, reward: 4.055, loss: -0.0225, took: 46.2844s
Batch 2600/3907, reward: 4.057, loss: -0.0374, took: 48.3566s
Batch 2700/3907, reward: 4.055, loss: -0.0346, took: 47.3747s
Batch 2800/3907, reward: 4.058, loss: -0.0396, took: 48.9601s
Batch 2900/3907, reward: 4.061, loss: -0.0429, took: 48.7004s
Batch 3000/3907, reward: 4.062, loss: -0.0339, took: 48.2369s
Batch 3100/3907, reward: 4.062, loss: -0.0382, took: 47.1368s
Batch 3200/3907, reward: 4.054, loss: -0.0522, took: 47.9265s
Batch 3300/3907, reward: 4.056, loss: -0.0371, took: 47.2926s
Batch 3400/3907, reward: 4.056, loss: -0.0275, took: 48.2282s
Batch 3500/3907, reward: 4.061, loss: -0.0138, took: 47.2763s
Batch 3600/3907, reward: 4.060, loss: -0.0448, took: 44.9814s
Batch 3700/3907, reward: 4.057, loss: -0.0352, took: 48.0070s
Batch 3800/3907, reward: 4.053, loss: -0.0408, took: 47.9711s
Batch 3900/3907, reward: 4.056, loss: -0.0318, took: 47.8747s
Mean epoch loss/reward: -0.0399, 4.0601, 4.0550, took: 1871.9564s (46.4625s / 100 batches)
Batch 0/3907, reward: 4.105, loss: -0.0544, took: 0.6669s
Batch 100/3907, reward: 4.062, loss: -0.0472, took: 47.9738s
Batch 200/3907, reward: 4.065, loss: -0.0423, took: 48.8757s
Batch 300/3907, reward: 4.062, loss: -0.0383, took: 45.4466s
Batch 400/3907, reward: 4.058, loss: -0.0272, took: 48.2838s
Batch 500/3907, reward: 4.056, loss: -0.0448, took: 47.7822s
Batch 600/3907, reward: 4.057, loss: -0.0519, took: 47.9750s
Batch 700/3907, reward: 4.063, loss: -0.0436, took: 47.6608s
Batch 800/3907, reward: 4.058, loss: -0.0344, took: 48.7031s
Batch 900/3907, reward: 4.061, loss: -0.0123, took: 48.9912s
Batch 1000/3907, reward: 4.057, loss: -0.0384, took: 48.2999s
Batch 1100/3907, reward: 4.059, loss: -0.0315, took: 47.5762s
Batch 1200/3907, reward: 4.051, loss: -0.0302, took: 48.3683s
Batch 1300/3907, reward: 4.057, loss: -0.0277, took: 46.9645s
Batch 1400/3907, reward: 4.060, loss: -0.0400, took: 45.5173s
Batch 1500/3907, reward: 4.062, loss: -0.0383, took: 47.3176s
Batch 1600/3907, reward: 4.057, loss: -0.0388, took: 47.6257s
Batch 1700/3907, reward: 4.055, loss: -0.0326, took: 47.7007s
Batch 1800/3907, reward: 4.055, loss: -0.0345, took: 48.8924s
Batch 1900/3907, reward: 4.059, loss: -0.0477, took: 48.2868s
Batch 2000/3907, reward: 4.056, loss: -0.0378, took: 48.1220s
Batch 2100/3907, reward: 4.057, loss: -0.0373, took: 48.2571s
Batch 2200/3907, reward: 4.060, loss: -0.0408, took: 48.5307s
Batch 2300/3907, reward: 4.063, loss: -0.0420, took: 48.0057s
Batch 2400/3907, reward: 4.053, loss: -0.0351, took: 47.9733s
Batch 2500/3907, reward: 4.064, loss: -0.0412, took: 45.3686s
Batch 2600/3907, reward: 4.049, loss: -0.0339, took: 47.6345s
Batch 2700/3907, reward: 4.059, loss: -0.0380, took: 48.2229s
Batch 2800/3907, reward: 4.055, loss: -0.0366, took: 49.8118s
Batch 2900/3907, reward: 4.054, loss: -0.0256, took: 48.7051s
Batch 3000/3907, reward: 4.059, loss: -0.0468, took: 47.3345s
Batch 3100/3907, reward: 4.055, loss: -0.0356, took: 48.2616s
Batch 3200/3907, reward: 4.055, loss: -0.0413, took: 46.5013s
Batch 3300/3907, reward: 4.059, loss: -0.0389, took: 48.3374s
Batch 3400/3907, reward: 4.054, loss: -0.0335, took: 47.9819s
Batch 3500/3907, reward: 4.052, loss: -0.0360, took: 47.8215s
Batch 3600/3907, reward: 4.054, loss: -0.0306, took: 47.1054s
Batch 3700/3907, reward: 4.053, loss: -0.0306, took: 47.6938s
Batch 3800/3907, reward: 4.063, loss: -0.0273, took: 49.0696s
Batch 3900/3907, reward: 4.055, loss: -0.0327, took: 48.0501s
Mean epoch loss/reward: -0.0364, 4.0574, 4.0409, took: 1882.4155s (46.6924s / 100 batches)
Batch 0/3907, reward: 4.017, loss: 0.0512, took: 0.5914s
Batch 100/3907, reward: 4.056, loss: -0.0318, took: 48.4669s
Batch 200/3907, reward: 4.060, loss: -0.0326, took: 48.2387s
Batch 300/3907, reward: 4.056, loss: -0.0357, took: 44.4315s
Batch 400/3907, reward: 4.057, loss: -0.0260, took: 47.5782s
Batch 500/3907, reward: 4.057, loss: -0.0246, took: 48.3074s
Batch 600/3907, reward: 4.060, loss: -0.0242, took: 49.7484s
Batch 700/3907, reward: 4.057, loss: -0.0285, took: 48.3487s
Batch 800/3907, reward: 4.062, loss: -0.0329, took: 46.8806s
Batch 900/3907, reward: 4.061, loss: -0.0337, took: 47.4961s
Batch 1000/3907, reward: 4.052, loss: -0.0339, took: 47.7061s
Batch 1100/3907, reward: 4.055, loss: -0.0429, took: 47.2008s
Batch 1200/3907, reward: 4.054, loss: -0.0415, took: 48.6080s
Batch 1300/3907, reward: 4.051, loss: -0.0265, took: 47.7344s
Batch 1400/3907, reward: 4.053, loss: -0.0234, took: 45.1424s
Batch 1500/3907, reward: 4.051, loss: -0.0408, took: 46.6241s
Batch 1600/3907, reward: 4.052, loss: -0.0502, took: 46.8366s
Batch 1700/3907, reward: 4.060, loss: -0.0343, took: 47.4530s
Batch 1800/3907, reward: 4.046, loss: -0.0341, took: 47.2882s
Batch 1900/3907, reward: 4.050, loss: -0.0431, took: 48.6451s
Batch 2000/3907, reward: 4.058, loss: -0.0294, took: 47.7637s
Batch 2100/3907, reward: 4.054, loss: -0.0276, took: 48.2121s
Batch 2200/3907, reward: 4.056, loss: -0.0225, took: 48.4950s
Batch 2300/3907, reward: 4.058, loss: -0.0352, took: 47.7815s
Batch 2400/3907, reward: 4.052, loss: -0.0328, took: 48.7411s
Batch 2500/3907, reward: 4.060, loss: -0.0355, took: 45.8284s
Batch 2600/3907, reward: 4.056, loss: -0.0347, took: 49.0815s
Batch 2700/3907, reward: 4.049, loss: -0.0360, took: 47.2367s
Batch 2800/3907, reward: 4.059, loss: -0.0485, took: 48.2614s
Batch 2900/3907, reward: 4.048, loss: -0.0323, took: 48.0341s
Batch 3000/3907, reward: 4.055, loss: -0.0408, took: 48.4259s
Batch 3100/3907, reward: 4.051, loss: -0.0394, took: 48.1644s
Batch 3200/3907, reward: 4.058, loss: -0.0435, took: 47.7697s
Batch 3300/3907, reward: 4.053, loss: -0.0412, took: 47.7119s
Batch 3400/3907, reward: 4.052, loss: -0.0220, took: 47.1333s
Batch 3500/3907, reward: 4.048, loss: -0.0403, took: 46.9855s
Batch 3600/3907, reward: 4.049, loss: -0.0343, took: 47.7068s
Batch 3700/3907, reward: 4.061, loss: -0.0340, took: 47.2243s
Batch 3800/3907, reward: 4.055, loss: -0.0396, took: 47.5140s
Batch 3900/3907, reward: 4.055, loss: -0.0359, took: 46.8884s
Mean epoch loss/reward: -0.0345, 4.0548, 4.0432, took: 1873.1456s (46.4572s / 100 batches)
Batch 0/3907, reward: 4.102, loss: -0.1314, took: 0.5571s
Batch 100/3907, reward: 4.052, loss: -0.0400, took: 47.8344s
Batch 200/3907, reward: 4.050, loss: -0.0365, took: 47.6295s
Batch 300/3907, reward: 4.056, loss: -0.0291, took: 44.1936s
Batch 400/3907, reward: 4.054, loss: -0.0439, took: 47.4313s
Batch 500/3907, reward: 4.056, loss: -0.0303, took: 47.9322s
Batch 600/3907, reward: 4.056, loss: -0.0355, took: 48.4849s
Batch 700/3907, reward: 4.052, loss: -0.0307, took: 48.0876s
Batch 800/3907, reward: 4.051, loss: -0.0365, took: 48.1005s
Batch 900/3907, reward: 4.050, loss: -0.0262, took: 47.0356s
Batch 1000/3907, reward: 4.056, loss: -0.0246, took: 49.0584s
Batch 1100/3907, reward: 4.045, loss: -0.0265, took: 48.8785s
Batch 1200/3907, reward: 4.053, loss: -0.0342, took: 47.6697s
Batch 1300/3907, reward: 4.051, loss: -0.0371, took: 48.3254s
Batch 1400/3907, reward: 4.051, loss: -0.0330, took: 45.5080s
Batch 1500/3907, reward: 4.050, loss: -0.0356, took: 47.3471s
Batch 1600/3907, reward: 4.051, loss: -0.0162, took: 47.3217s
Batch 1700/3907, reward: 4.047, loss: -0.0211, took: 47.5628s
Batch 1800/3907, reward: 4.052, loss: -0.0310, took: 48.7243s
Batch 1900/3907, reward: 4.055, loss: -0.0326, took: 47.1665s
Batch 2000/3907, reward: 4.058, loss: -0.0361, took: 47.8564s
Batch 2100/3907, reward: 4.054, loss: -0.0271, took: 48.4865s
Batch 2200/3907, reward: 4.047, loss: -0.0290, took: 47.8135s
Batch 2300/3907, reward: 4.056, loss: -0.0356, took: 47.7736s
Batch 2400/3907, reward: 4.055, loss: -0.0462, took: 47.5614s
Batch 2500/3907, reward: 4.055, loss: -0.0470, took: 45.2501s
Batch 2600/3907, reward: 4.052, loss: -0.0321, took: 47.1701s
Batch 2700/3907, reward: 4.051, loss: -0.0315, took: 46.8485s
Batch 2800/3907, reward: 4.050, loss: -0.0320, took: 47.6697s
Batch 2900/3907, reward: 4.050, loss: -0.0352, took: 47.0544s
Batch 3000/3907, reward: 4.046, loss: -0.0262, took: 47.6120s
Batch 3100/3907, reward: 4.049, loss: -0.0388, took: 47.6207s
Batch 3200/3907, reward: 4.052, loss: -0.0263, took: 48.3191s
Batch 3300/3907, reward: 4.049, loss: -0.0362, took: 48.6081s
Batch 3400/3907, reward: 4.053, loss: -0.0404, took: 47.8978s
Batch 3500/3907, reward: 4.051, loss: -0.0402, took: 45.9557s
Batch 3600/3907, reward: 4.053, loss: -0.0403, took: 45.4963s
Batch 3700/3907, reward: 4.048, loss: -0.0378, took: 48.5457s
Batch 3800/3907, reward: 4.050, loss: -0.0354, took: 48.9341s
Batch 3900/3907, reward: 4.051, loss: -0.0357, took: 47.3345s
Mean epoch loss/reward: -0.0336, 4.0518, 4.0442, took: 1868.6484s (46.3664s / 100 batches)
Batch 0/3907, reward: 4.046, loss: 0.0726, took: 0.6672s
Batch 100/3907, reward: 4.052, loss: -0.0403, took: 47.7662s
Batch 200/3907, reward: 4.048, loss: -0.0295, took: 47.8734s
Batch 300/3907, reward: 4.054, loss: -0.0349, took: 44.7898s
Batch 400/3907, reward: 4.055, loss: -0.0402, took: 46.9846s
Batch 500/3907, reward: 4.052, loss: -0.0321, took: 48.0074s
Batch 600/3907, reward: 4.058, loss: -0.0304, took: 47.4947s
Batch 700/3907, reward: 4.049, loss: -0.0258, took: 47.4611s
Batch 800/3907, reward: 4.053, loss: -0.0407, took: 47.5402s
Batch 900/3907, reward: 4.050, loss: -0.0382, took: 48.0359s
Batch 1000/3907, reward: 4.050, loss: -0.0320, took: 48.9329s
Batch 1100/3907, reward: 4.047, loss: -0.0394, took: 47.7866s
Batch 1200/3907, reward: 4.052, loss: -0.0381, took: 49.0532s
Batch 1300/3907, reward: 4.050, loss: -0.0210, took: 47.5032s
Batch 1400/3907, reward: 4.054, loss: -0.0463, took: 47.0111s
Batch 1500/3907, reward: 4.049, loss: -0.0358, took: 47.5129s
Batch 1600/3907, reward: 4.054, loss: -0.0405, took: 48.3281s
Batch 1700/3907, reward: 4.057, loss: -0.0313, took: 46.8629s
Batch 1800/3907, reward: 4.053, loss: -0.0242, took: 47.6611s
Batch 1900/3907, reward: 4.049, loss: -0.0443, took: 46.5311s
Batch 2000/3907, reward: 4.051, loss: -0.0378, took: 48.0907s
Batch 2100/3907, reward: 4.054, loss: -0.0365, took: 47.1624s
Batch 2200/3907, reward: 4.049, loss: -0.0267, took: 46.7786s
Batch 2300/3907, reward: 4.050, loss: -0.0239, took: 48.7499s
Batch 2400/3907, reward: 4.053, loss: -0.0278, took: 48.3712s
Batch 2500/3907, reward: 4.054, loss: -0.0416, took: 46.0719s
Batch 2600/3907, reward: 4.056, loss: -0.0309, took: 47.2354s
Batch 2700/3907, reward: 4.052, loss: -0.0348, took: 47.1596s
Batch 2800/3907, reward: 4.046, loss: -0.0293, took: 48.1912s
Batch 2900/3907, reward: 4.047, loss: -0.0315, took: 48.8564s
Batch 3000/3907, reward: 4.051, loss: -0.0345, took: 47.1003s
Batch 3100/3907, reward: 4.045, loss: -0.0292, took: 47.6322s
Batch 3200/3907, reward: 4.040, loss: -0.0320, took: 47.5057s
Batch 3300/3907, reward: 4.049, loss: -0.0210, took: 48.3575s
Batch 3400/3907, reward: 4.049, loss: -0.0312, took: 48.9153s
Batch 3500/3907, reward: 4.047, loss: -0.0357, took: 45.7133s
Batch 3600/3907, reward: 4.049, loss: -0.0430, took: 46.7063s
Batch 3700/3907, reward: 4.060, loss: -0.0383, took: 47.6796s
Batch 3800/3907, reward: 4.052, loss: -0.0268, took: 48.0164s
Batch 3900/3907, reward: 4.052, loss: -0.0341, took: 48.8716s
Mean epoch loss/reward: -0.0335, 4.0511, 4.0448, took: 1871.5150s (46.4242s / 100 batches)
Batch 0/3907, reward: 4.026, loss: 0.1734, took: 0.7091s
Batch 100/3907, reward: 4.044, loss: -0.0349, took: 48.2308s
Batch 200/3907, reward: 4.045, loss: -0.0439, took: 48.1218s
Batch 300/3907, reward: 4.051, loss: -0.0222, took: 46.5998s
Batch 400/3907, reward: 4.047, loss: -0.0292, took: 48.0483s
Batch 500/3907, reward: 4.049, loss: -0.0372, took: 48.0283s
Batch 600/3907, reward: 4.046, loss: -0.0248, took: 47.8831s
Batch 700/3907, reward: 4.047, loss: -0.0340, took: 47.2628s
Batch 800/3907, reward: 4.047, loss: -0.0321, took: 47.8694s
Batch 900/3907, reward: 4.044, loss: -0.0276, took: 48.3678s
Batch 1000/3907, reward: 4.053, loss: -0.0322, took: 49.8578s
Batch 1100/3907, reward: 4.054, loss: -0.0319, took: 47.2404s
Batch 1200/3907, reward: 4.056, loss: -0.0291, took: 48.4820s
Batch 1300/3907, reward: 4.047, loss: -0.0202, took: 46.5654s
Batch 1400/3907, reward: 4.052, loss: -0.0308, took: 45.0499s
Batch 1500/3907, reward: 4.052, loss: -0.0388, took: 48.0104s
Batch 1600/3907, reward: 4.052, loss: -0.0301, took: 47.9286s
Batch 1700/3907, reward: 4.049, loss: -0.0344, took: 47.7182s
Batch 1800/3907, reward: 4.053, loss: -0.0330, took: 48.2771s
Batch 1900/3907, reward: 4.055, loss: -0.0374, took: 46.7304s
Batch 2000/3907, reward: 4.049, loss: -0.0363, took: 46.9968s
Batch 2100/3907, reward: 4.048, loss: -0.0321, took: 48.3192s
Batch 2200/3907, reward: 4.048, loss: -0.0310, took: 46.2707s
Batch 2300/3907, reward: 4.050, loss: -0.0353, took: 47.7738s
Batch 2400/3907, reward: 4.048, loss: -0.0332, took: 47.5331s
Batch 2500/3907, reward: 4.049, loss: -0.0326, took: 45.1028s
Batch 2600/3907, reward: 4.047, loss: -0.0297, took: 47.1796s
Batch 2700/3907, reward: 4.050, loss: -0.0425, took: 47.0352s
Batch 2800/3907, reward: 4.050, loss: -0.0333, took: 47.3778s
Batch 2900/3907, reward: 4.049, loss: -0.0319, took: 47.5696s
Batch 3000/3907, reward: 4.047, loss: -0.0355, took: 47.0315s
Batch 3100/3907, reward: 4.052, loss: -0.0355, took: 47.4695s
Batch 3200/3907, reward: 4.047, loss: -0.0331, took: 48.6556s
Batch 3300/3907, reward: 4.051, loss: -0.0306, took: 47.1770s
Batch 3400/3907, reward: 4.048, loss: -0.0312, took: 47.2606s
Batch 3500/3907, reward: 4.047, loss: -0.0477, took: 47.2714s
Batch 3600/3907, reward: 4.045, loss: -0.0310, took: 45.7326s
Batch 3700/3907, reward: 4.047, loss: -0.0246, took: 48.5834s
Batch 3800/3907, reward: 4.046, loss: -0.0392, took: 47.0249s
Batch 3900/3907, reward: 4.046, loss: -0.0283, took: 47.8099s
Mean epoch loss/reward: -0.0327, 4.0488, 4.0392, took: 1866.6826s (46.3039s / 100 batches)
Batch 0/3907, reward: 4.050, loss: -0.0016, took: 0.6285s
Batch 100/3907, reward: 4.046, loss: -0.0287, took: 48.5560s
Batch 200/3907, reward: 4.060, loss: -0.0272, took: 47.8295s
Batch 300/3907, reward: 4.048, loss: -0.0321, took: 45.3222s
Batch 400/3907, reward: 4.051, loss: -0.0251, took: 47.7321s
Batch 500/3907, reward: 4.055, loss: -0.0209, took: 48.5394s
Batch 600/3907, reward: 4.053, loss: -0.0308, took: 47.8464s
Batch 700/3907, reward: 4.046, loss: -0.0324, took: 47.7509s
Batch 800/3907, reward: 4.047, loss: -0.0361, took: 48.1222s
Batch 900/3907, reward: 4.046, loss: -0.0350, took: 47.2067s
Batch 1000/3907, reward: 4.052, loss: -0.0312, took: 47.9104s
Batch 1100/3907, reward: 4.053, loss: -0.0354, took: 48.3246s
Batch 1200/3907, reward: 4.049, loss: -0.0233, took: 49.1477s
Batch 1300/3907, reward: 4.045, loss: -0.0308, took: 47.8561s
Batch 1400/3907, reward: 4.043, loss: -0.0311, took: 45.9876s
Batch 1500/3907, reward: 4.049, loss: -0.0343, took: 46.3887s
Batch 1600/3907, reward: 4.052, loss: -0.0210, took: 47.7116s
Batch 1700/3907, reward: 4.048, loss: -0.0259, took: 48.1606s
Batch 1800/3907, reward: 4.047, loss: -0.0246, took: 46.4024s
Batch 1900/3907, reward: 4.052, loss: -0.0397, took: 48.2037s
Batch 2000/3907, reward: 4.045, loss: -0.0321, took: 47.4819s
Batch 2100/3907, reward: 4.051, loss: -0.0329, took: 48.2793s
Batch 2200/3907, reward: 4.046, loss: -0.0249, took: 47.8536s
Batch 2300/3907, reward: 4.048, loss: -0.0315, took: 47.4052s
Batch 2400/3907, reward: 4.053, loss: -0.0293, took: 48.2204s
Batch 2500/3907, reward: 4.043, loss: -0.0245, took: 45.1244s
Batch 2600/3907, reward: 4.045, loss: -0.0326, took: 48.4519s
Batch 2700/3907, reward: 4.047, loss: -0.0215, took: 47.6310s
Batch 2800/3907, reward: 4.038, loss: -0.0428, took: 47.9555s
Batch 2900/3907, reward: 4.044, loss: -0.0276, took: 47.7932s
Batch 3000/3907, reward: 4.048, loss: -0.0281, took: 46.4446s
Batch 3100/3907, reward: 4.043, loss: -0.0363, took: 48.5236s
Batch 3200/3907, reward: 4.054, loss: -0.0244, took: 47.6778s
Batch 3300/3907, reward: 4.049, loss: -0.0235, took: 47.4244s
Batch 3400/3907, reward: 4.043, loss: -0.0421, took: 47.4671s
Batch 3500/3907, reward: 4.047, loss: -0.0355, took: 46.8000s
Batch 3600/3907, reward: 4.048, loss: -0.0234, took: 44.0690s
Batch 3700/3907, reward: 4.052, loss: -0.0277, took: 48.7199s
Batch 3800/3907, reward: 4.051, loss: -0.0324, took: 48.5299s
Batch 3900/3907, reward: 4.048, loss: -0.0341, took: 47.5413s
Mean epoch loss/reward: -0.0300, 4.0483, 4.0324, took: 1868.7892s (46.3755s / 100 batches)
Batch 0/3907, reward: 3.989, loss: -0.0715, took: 0.6635s
Batch 100/3907, reward: 4.048, loss: -0.0323, took: 48.3639s
Batch 200/3907, reward: 4.050, loss: -0.0242, took: 46.9860s
Batch 300/3907, reward: 4.045, loss: -0.0243, took: 44.8742s
Batch 400/3907, reward: 4.040, loss: -0.0255, took: 47.3697s
Batch 500/3907, reward: 4.043, loss: -0.0297, took: 47.8398s
Batch 600/3907, reward: 4.046, loss: -0.0258, took: 47.7819s
Batch 700/3907, reward: 4.049, loss: -0.0307, took: 46.8779s
Batch 800/3907, reward: 4.043, loss: -0.0268, took: 48.6435s
Batch 900/3907, reward: 4.047, loss: -0.0376, took: 47.5492s
Batch 1000/3907, reward: 4.055, loss: -0.0353, took: 48.9138s
Batch 1100/3907, reward: 4.043, loss: -0.0252, took: 47.7125s
Batch 1200/3907, reward: 4.051, loss: -0.0311, took: 47.8844s
Batch 1300/3907, reward: 4.047, loss: -0.0230, took: 48.3363s
Batch 1400/3907, reward: 4.043, loss: -0.0359, took: 45.3160s
Batch 1500/3907, reward: 4.046, loss: -0.0274, took: 47.9992s
Batch 1600/3907, reward: 4.046, loss: -0.0371, took: 47.2781s
Batch 1700/3907, reward: 4.049, loss: -0.0322, took: 47.7546s
Batch 1800/3907, reward: 4.046, loss: -0.0279, took: 48.3296s
Batch 1900/3907, reward: 4.047, loss: -0.0330, took: 47.3581s
Batch 2000/3907, reward: 4.042, loss: -0.0296, took: 48.1176s
Batch 2100/3907, reward: 4.045, loss: -0.0415, took: 47.9854s
Batch 2200/3907, reward: 4.047, loss: -0.0235, took: 49.2133s
Batch 2300/3907, reward: 4.046, loss: -0.0289, took: 48.3768s
Batch 2400/3907, reward: 4.048, loss: -0.0354, took: 47.7106s
Batch 2500/3907, reward: 4.047, loss: -0.0286, took: 46.0633s
Batch 2600/3907, reward: 4.043, loss: -0.0285, took: 48.2610s
Batch 2700/3907, reward: 4.049, loss: -0.0327, took: 46.9672s
Batch 2800/3907, reward: 4.043, loss: -0.0354, took: 47.5651s
Batch 2900/3907, reward: 4.045, loss: -0.0325, took: 46.9993s
Batch 3000/3907, reward: 4.042, loss: -0.0296, took: 46.9284s
Batch 3100/3907, reward: 4.047, loss: -0.0347, took: 47.3119s
Batch 3200/3907, reward: 4.049, loss: -0.0267, took: 48.1330s
Batch 3300/3907, reward: 4.044, loss: -0.0273, took: 47.8990s
Batch 3400/3907, reward: 4.040, loss: -0.0371, took: 47.5207s
Batch 3500/3907, reward: 4.044, loss: -0.0299, took: 47.4869s
Batch 3600/3907, reward: 4.041, loss: -0.0294, took: 46.2061s
Batch 3700/3907, reward: 4.047, loss: -0.0325, took: 47.3105s
Batch 3800/3907, reward: 4.047, loss: -0.0270, took: 47.7159s
Batch 3900/3907, reward: 4.042, loss: -0.0344, took: 47.7748s
Mean epoch loss/reward: -0.0305, 4.0457, 4.0349, took: 1869.0217s (46.3845s / 100 batches)
Average tour length for uniform: 4.030776364985774
Average tour length for shifted: 3.3087607350287183
Average tour length for adversary: 2.8607417319112036
